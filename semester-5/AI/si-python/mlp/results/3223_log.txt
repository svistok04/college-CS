EXPERIMENT 3223 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[64, 64, 64, 64, 64, 32, 32, 32, 32, 32, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8], activation_name=relu, targets_activation_name=linear, 
initialization_name=uniform, algo_name=sgd_momentum, learning_rate=0.0001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 25185]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.18318931763552243, loss after: 0.1831064833812976]
[epoch 1/1000, batch 11/100 -> loss before: 0.3384229871528463, loss after: 0.3381669702503084]
[epoch 1/1000, batch 21/100 -> loss before: 0.3628482873818046, loss after: 0.36315795052099786]
[epoch 1/1000, batch 31/100 -> loss before: 0.3915250411156651, loss after: 0.3911702837120219]
[epoch 1/1000, batch 41/100 -> loss before: 0.19072407877393813, loss after: 0.19063632834008937]
[epoch 1/1000, batch 51/100 -> loss before: 0.24325436136428738, loss after: 0.24341735681375556]
[epoch 1/1000, batch 61/100 -> loss before: 0.46462609975095026, loss after: 0.4644853487020565]
[epoch 1/1000, batch 71/100 -> loss before: 0.2983106699338677, loss after: 0.29754305135335457]
[epoch 1/1000, batch 81/100 -> loss before: 0.3832080941441566, loss after: 0.38312174912420993]
[epoch 1/1000, batch 91/100 -> loss before: 0.31763749309550215, loss after: 0.3175207073185326]
ENDING EPOCH 1/1000 [loss before: 0.2899884218128069, loss after: 0.2783853698994002; epoch time: 0.08918619155883789 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.200132596198564, loss after: 0.20009811254442855]
[epoch 101/1000, batch 11/100 -> loss before: 0.16535815661235193, loss after: 0.16533922406106177]
[epoch 101/1000, batch 21/100 -> loss before: 0.16085482895508327, loss after: 0.16061268448669053]
[epoch 101/1000, batch 31/100 -> loss before: 0.2745628373155643, loss after: 0.27401081014335377]
[epoch 101/1000, batch 41/100 -> loss before: 0.524912576795747, loss after: 0.524678410094689]
[epoch 101/1000, batch 51/100 -> loss before: 0.23272245519199145, loss after: 0.23256635184807856]
[epoch 101/1000, batch 61/100 -> loss before: 0.5262994274328636, loss after: 0.5262633731411106]
[epoch 101/1000, batch 71/100 -> loss before: 0.15895663014554223, loss after: 0.1586245629249111]
[epoch 101/1000, batch 81/100 -> loss before: 0.28342086537743133, loss after: 0.2834033305692884]
[epoch 101/1000, batch 91/100 -> loss before: 0.21894154156647722, loss after: 0.2188858951208422]
ENDING EPOCH 101/1000 [loss before: 0.2312429280010083, loss after: 0.23114060598695974; epoch time: 0.09308362007141113 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.08053869553422924, loss after: 0.0803095635009542]
[epoch 201/1000, batch 11/100 -> loss before: 0.20395804093523356, loss after: 0.20399658179170296]
[epoch 201/1000, batch 21/100 -> loss before: 0.23339734567350737, loss after: 0.23307052146234913]
[epoch 201/1000, batch 31/100 -> loss before: 0.37922332731526975, loss after: 0.37927842898668046]
[epoch 201/1000, batch 41/100 -> loss before: 0.2670532569145127, loss after: 0.2671026724040322]
[epoch 201/1000, batch 51/100 -> loss before: 0.20528582058625142, loss after: 0.2052866739651884]
[epoch 201/1000, batch 61/100 -> loss before: 0.5034473521451803, loss after: 0.5035254812222564]
[epoch 201/1000, batch 71/100 -> loss before: 0.4078018837413812, loss after: 0.4077816291119018]
[epoch 201/1000, batch 81/100 -> loss before: 0.1333955457837764, loss after: 0.13305869351145136]
[epoch 201/1000, batch 91/100 -> loss before: 0.060381129874222984, loss after: 0.060296375830629544]
ENDING EPOCH 201/1000 [loss before: 0.2314273479430851, loss after: 0.23109762207477594; epoch time: 0.09378242492675781 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.18779484714681108, loss after: 0.18778790412169705]
[epoch 301/1000, batch 11/100 -> loss before: 0.03863681780834599, loss after: 0.03872930645009075]
[epoch 301/1000, batch 21/100 -> loss before: 0.26105036159798, loss after: 0.2605008061429902]
[epoch 301/1000, batch 31/100 -> loss before: 0.20021793493022305, loss after: 0.20022347812182062]
[epoch 301/1000, batch 41/100 -> loss before: 0.19185485501974903, loss after: 0.1910096370511311]
[epoch 301/1000, batch 51/100 -> loss before: 0.2868193951163911, loss after: 0.28683602111996725]
[epoch 301/1000, batch 61/100 -> loss before: 0.28494892063915855, loss after: 0.2849514320954967]
[epoch 301/1000, batch 71/100 -> loss before: 0.2503535231382128, loss after: 0.2503091881504792]
[epoch 301/1000, batch 81/100 -> loss before: 0.3311947091757094, loss after: 0.3312036636321362]
[epoch 301/1000, batch 91/100 -> loss before: 0.26177148143821266, loss after: 0.26122891381508984]
ENDING EPOCH 301/1000 [loss before: 0.2307916110007947, loss after: 0.23077029507199526; epoch time: 0.09475564956665039 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.2818240217922784, loss after: 0.2818085464481356]
[epoch 401/1000, batch 11/100 -> loss before: 0.16186930338332306, loss after: 0.16184060488281302]
[epoch 401/1000, batch 21/100 -> loss before: 0.22377019250134614, loss after: 0.2237515868396875]
[epoch 401/1000, batch 31/100 -> loss before: 0.1627319438473169, loss after: 0.16288194774811424]
[epoch 401/1000, batch 41/100 -> loss before: 0.1570699611118763, loss after: 0.1568095972723968]
[epoch 401/1000, batch 51/100 -> loss before: 0.1387542931068097, loss after: 0.13801115240821799]
[epoch 401/1000, batch 61/100 -> loss before: 0.16543446332757367, loss after: 0.16539615637763733]
[epoch 401/1000, batch 71/100 -> loss before: 0.17860150617453965, loss after: 0.17860095175559554]
[epoch 401/1000, batch 81/100 -> loss before: 0.1644463758675952, loss after: 0.1645118041840155]
[epoch 401/1000, batch 91/100 -> loss before: 0.20689467653278332, loss after: 0.20686201638269724]
ENDING EPOCH 401/1000 [loss before: 0.23073808247196193, loss after: 0.2310326447095136; epoch time: 0.10671234130859375 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.1712861347630265, loss after: 0.1713858176553179]
[epoch 501/1000, batch 11/100 -> loss before: 0.4269131192968322, loss after: 0.42689969766741254]
[epoch 501/1000, batch 21/100 -> loss before: 0.2996399303253148, loss after: 0.29968315653326305]
[epoch 501/1000, batch 31/100 -> loss before: 0.2313912761989394, loss after: 0.23129204462993241]
[epoch 501/1000, batch 41/100 -> loss before: 0.318353718856211, loss after: 0.31834081301077666]
[epoch 501/1000, batch 51/100 -> loss before: 0.11584788157264922, loss after: 0.11508198960132524]
[epoch 501/1000, batch 61/100 -> loss before: 0.1070620643670964, loss after: 0.10706582235283194]
[epoch 501/1000, batch 71/100 -> loss before: 0.138124790657952, loss after: 0.1379900970660197]
[epoch 501/1000, batch 81/100 -> loss before: 0.20873069795822086, loss after: 0.2087538121044251]
[epoch 501/1000, batch 91/100 -> loss before: 0.17137144316835265, loss after: 0.1713744261088156]
ENDING EPOCH 501/1000 [loss before: 0.23070212188147532, loss after: 0.23085422279010748; epoch time: 0.11721587181091309 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.35978381588751934, loss after: 0.35978968207488915]
[epoch 601/1000, batch 11/100 -> loss before: 0.15194237163508303, loss after: 0.1516112450778205]
[epoch 601/1000, batch 21/100 -> loss before: 0.1906106073191171, loss after: 0.1906111925013442]
[epoch 601/1000, batch 31/100 -> loss before: 0.12200354655860168, loss after: 0.12097600791153977]
[epoch 601/1000, batch 41/100 -> loss before: 0.33441364585048067, loss after: 0.3342873925624171]
[epoch 601/1000, batch 51/100 -> loss before: 0.31656676716739146, loss after: 0.3165860847783035]
[epoch 601/1000, batch 61/100 -> loss before: 0.10304412744712892, loss after: 0.10294962462595207]
[epoch 601/1000, batch 71/100 -> loss before: 0.3226340309051757, loss after: 0.32239510670071664]
[epoch 601/1000, batch 81/100 -> loss before: 0.30166346828464236, loss after: 0.3015858818921288]
[epoch 601/1000, batch 91/100 -> loss before: 0.2519212534980394, loss after: 0.25180202625869186]
ENDING EPOCH 601/1000 [loss before: 0.23071136595728223, loss after: 0.23065779527963987; epoch time: 0.12241363525390625 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.11514324369086296, loss after: 0.11512809564167681]
[epoch 701/1000, batch 11/100 -> loss before: 0.23021319234418697, loss after: 0.22979364108407924]
[epoch 701/1000, batch 21/100 -> loss before: 0.20115086987195144, loss after: 0.20074766496378685]
[epoch 701/1000, batch 31/100 -> loss before: 0.2242673893056736, loss after: 0.22460272954857766]
[epoch 701/1000, batch 41/100 -> loss before: 0.13465575302616417, loss after: 0.13460379394121316]
[epoch 701/1000, batch 51/100 -> loss before: 0.31330872220199535, loss after: 0.3131969884630389]
[epoch 701/1000, batch 61/100 -> loss before: 0.1505387241956692, loss after: 0.15054851114886109]
[epoch 701/1000, batch 71/100 -> loss before: 0.19461197327598936, loss after: 0.19456409199524954]
[epoch 701/1000, batch 81/100 -> loss before: 0.21763986438823277, loss after: 0.2176099441630222]
[epoch 701/1000, batch 91/100 -> loss before: 0.3464234311119682, loss after: 0.34615134839622796]
ENDING EPOCH 701/1000 [loss before: 0.23111741976326275, loss after: 0.23117065915312807; epoch time: 0.10291051864624023 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.19921444742348374, loss after: 0.19906626771316344]
[epoch 801/1000, batch 11/100 -> loss before: 0.11678025440452522, loss after: 0.11677630678525228]
[epoch 801/1000, batch 21/100 -> loss before: 0.24197490667671193, loss after: 0.24193905797152002]
[epoch 801/1000, batch 31/100 -> loss before: 0.15858491786826004, loss after: 0.15855678244302404]
[epoch 801/1000, batch 41/100 -> loss before: 0.2953982339264634, loss after: 0.2953266873788135]
[epoch 801/1000, batch 51/100 -> loss before: 0.1428858545245412, loss after: 0.14270571744601734]
[epoch 801/1000, batch 61/100 -> loss before: 0.25454383562370086, loss after: 0.2545103151265732]
[epoch 801/1000, batch 71/100 -> loss before: 0.2860986762148476, loss after: 0.28607700024254024]
[epoch 801/1000, batch 81/100 -> loss before: 0.1723540758913505, loss after: 0.1723133127042373]
[epoch 801/1000, batch 91/100 -> loss before: 0.23764527407785935, loss after: 0.23751159382259845]
ENDING EPOCH 801/1000 [loss before: 0.23061976143970508, loss after: 0.23061354386904157; epoch time: 0.10037398338317871 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.08272174078480223, loss after: 0.08257974499294027]
[epoch 901/1000, batch 11/100 -> loss before: 0.2955853844883969, loss after: 0.2955660805473548]
[epoch 901/1000, batch 21/100 -> loss before: 0.1523101166293049, loss after: 0.1522918372368109]
[epoch 901/1000, batch 31/100 -> loss before: 0.1414878923273905, loss after: 0.14145917016942147]
[epoch 901/1000, batch 41/100 -> loss before: 0.2688013790420721, loss after: 0.26882071098569654]
[epoch 901/1000, batch 51/100 -> loss before: 0.24559238595914662, loss after: 0.24542671461100998]
[epoch 901/1000, batch 61/100 -> loss before: 0.41350945635402087, loss after: 0.41355013821496256]
[epoch 901/1000, batch 71/100 -> loss before: 0.15896105708530622, loss after: 0.1588611276198645]
[epoch 901/1000, batch 81/100 -> loss before: 0.4224058118932626, loss after: 0.4223272658007112]
[epoch 901/1000, batch 91/100 -> loss before: 0.19952007003209743, loss after: 0.19955122552239019]
ENDING EPOCH 901/1000 [loss before: 0.2307855396794431, loss after: 0.23072146315311115; epoch time: 0.09826874732971191 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.2330455982413878, loss after: 0.23309221357903823]
[epoch 1000/1000, batch 11/100 -> loss before: 0.08564840150221456, loss after: 0.08563951873903516]
[epoch 1000/1000, batch 21/100 -> loss before: 0.19626810194519856, loss after: 0.19625147185520073]
[epoch 1000/1000, batch 31/100 -> loss before: 0.24553454656195045, loss after: 0.24551907038238202]
[epoch 1000/1000, batch 41/100 -> loss before: 0.3465816952594546, loss after: 0.34642193840943514]
[epoch 1000/1000, batch 51/100 -> loss before: 0.1783416995371381, loss after: 0.17789544847872985]
[epoch 1000/1000, batch 61/100 -> loss before: 0.18468316860970904, loss after: 0.1846770134593515]
[epoch 1000/1000, batch 71/100 -> loss before: 0.36875381540493135, loss after: 0.36867297744195326]
[epoch 1000/1000, batch 81/100 -> loss before: 0.09843987963975538, loss after: 0.09847364129681403]
[epoch 1000/1000, batch 91/100 -> loss before: 0.3406546075579913, loss after: 0.34062739699670497]
ENDING EPOCH 1000/1000 [loss before: 0.23059593333840797, loss after: 0.23060834421393822; epoch time: 0.13555431365966797 s]
FIT DONE. [time: 97.7900493144989 s]
LOSS TRAIN (MSE): 0.23060834421393822
LOSS TEST (MSE): 0.2193083142859375
R^2 TRAIN: 0.18535455552766233
R^2 TEST: 0.20978623564227006
EXPERIMENT DONE
