EXPERIMENT 1142 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[128, 128, 64, 64, 32, 32], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=adam, learning_rate=0.01, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 32481]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.26392389365226554, loss after: 0.40080395441154215]
[epoch 1/1000, batch 11/100 -> loss before: 0.3245517669770458, loss after: 0.22917680690180448]
[epoch 1/1000, batch 21/100 -> loss before: 0.3296768976056148, loss after: 0.333927901486503]
[epoch 1/1000, batch 31/100 -> loss before: 0.34418912003925484, loss after: 0.3451761551246387]
[epoch 1/1000, batch 41/100 -> loss before: 0.28625170605177286, loss after: 0.28802978427775777]
[epoch 1/1000, batch 51/100 -> loss before: 0.10263725689896872, loss after: 0.10121016989457068]
[epoch 1/1000, batch 61/100 -> loss before: 0.32945549636635485, loss after: 0.3132636351737492]
[epoch 1/1000, batch 71/100 -> loss before: 0.3327055082180044, loss after: 0.3246569604947557]
[epoch 1/1000, batch 81/100 -> loss before: 0.3923750703775083, loss after: 0.38530800385089414]
[epoch 1/1000, batch 91/100 -> loss before: 0.31665974155732124, loss after: 0.31654581970749784]
ENDING EPOCH 1/1000 [loss before: 0.2838936199301506, loss after: 0.28314704627082454; epoch time: 0.09867429733276367 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.2663690905343679, loss after: 0.2660031600954774]
[epoch 101/1000, batch 11/100 -> loss before: 0.24960598255498354, loss after: 0.24955406967405058]
[epoch 101/1000, batch 21/100 -> loss before: 0.19078185522715832, loss after: 0.190964609859478]
[epoch 101/1000, batch 31/100 -> loss before: 0.1423466751037077, loss after: 0.1416985056913121]
[epoch 101/1000, batch 41/100 -> loss before: 0.26346186050162307, loss after: 0.26287777288660363]
[epoch 101/1000, batch 51/100 -> loss before: 0.1614366680806963, loss after: 0.16153462867653423]
[epoch 101/1000, batch 61/100 -> loss before: 0.12558316856403762, loss after: 0.12552460779400193]
[epoch 101/1000, batch 71/100 -> loss before: 0.3400116845760877, loss after: 0.34011459317760195]
[epoch 101/1000, batch 81/100 -> loss before: 0.2289191871853602, loss after: 0.22715683262456715]
[epoch 101/1000, batch 91/100 -> loss before: 0.25858416020525615, loss after: 0.2519050453858072]
ENDING EPOCH 101/1000 [loss before: 0.23158863969745475, loss after: 0.23144385371793674; epoch time: 0.09156680107116699 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.5229858603238727, loss after: 0.5226865021398811]
[epoch 201/1000, batch 11/100 -> loss before: 0.37205411868166827, loss after: 0.37196325193219965]
[epoch 201/1000, batch 21/100 -> loss before: 0.19827086269724478, loss after: 0.19854880317085882]
[epoch 201/1000, batch 31/100 -> loss before: 0.2146480234415086, loss after: 0.2145598087513137]
[epoch 201/1000, batch 41/100 -> loss before: 0.21549892670580612, loss after: 0.21532744298611234]
[epoch 201/1000, batch 51/100 -> loss before: 0.3876111194946017, loss after: 0.3877094666033203]
[epoch 201/1000, batch 61/100 -> loss before: 0.24431198735545712, loss after: 0.24441763764071603]
[epoch 201/1000, batch 71/100 -> loss before: 0.09203399727802516, loss after: 0.09216940465758863]
[epoch 201/1000, batch 81/100 -> loss before: 0.29744476706880263, loss after: 0.28864625120192045]
[epoch 201/1000, batch 91/100 -> loss before: 0.3325682358719635, loss after: 0.33178499731422695]
ENDING EPOCH 201/1000 [loss before: 0.2310856302787447, loss after: 0.2314000937565777; epoch time: 0.09569287300109863 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.11543668789711685, loss after: 0.11559024748842217]
[epoch 301/1000, batch 11/100 -> loss before: 0.03935272578737344, loss after: 0.03864400745746096]
[epoch 301/1000, batch 21/100 -> loss before: 0.31365642667787275, loss after: 0.3110559972792403]
[epoch 301/1000, batch 31/100 -> loss before: 0.17680136077407554, loss after: 0.17442002082709634]
[epoch 301/1000, batch 41/100 -> loss before: 0.28131140824853806, loss after: 0.2814299135744661]
[epoch 301/1000, batch 51/100 -> loss before: 0.22952665179688875, loss after: 0.22963747212479793]
[epoch 301/1000, batch 61/100 -> loss before: 0.15191465535880144, loss after: 0.15188343931609077]
[epoch 301/1000, batch 71/100 -> loss before: 0.04124354768745317, loss after: 0.04120731499689673]
[epoch 301/1000, batch 81/100 -> loss before: 0.08763447729423637, loss after: 0.08803603521926992]
[epoch 301/1000, batch 91/100 -> loss before: 0.31825845322731816, loss after: 0.31829970486041226]
ENDING EPOCH 301/1000 [loss before: 0.23207132561404337, loss after: 0.23110380830654503; epoch time: 0.09139394760131836 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.48621396432448966, loss after: 0.48479883139152224]
[epoch 401/1000, batch 11/100 -> loss before: 0.21426240096732113, loss after: 0.21358907402494703]
[epoch 401/1000, batch 21/100 -> loss before: 0.1318464174852668, loss after: 0.1290656402909902]
[epoch 401/1000, batch 31/100 -> loss before: 0.1677231614575751, loss after: 0.16543173974354575]
[epoch 401/1000, batch 41/100 -> loss before: 0.1976669939249812, loss after: 0.19766815766477322]
[epoch 401/1000, batch 51/100 -> loss before: 0.3273930311133204, loss after: 0.3253379047082985]
[epoch 401/1000, batch 61/100 -> loss before: 0.2735232668299489, loss after: 0.27338131874879396]
[epoch 401/1000, batch 71/100 -> loss before: 0.23184942646311177, loss after: 0.22946085225723717]
[epoch 401/1000, batch 81/100 -> loss before: 0.44235445655607675, loss after: 0.440520823767265]
[epoch 401/1000, batch 91/100 -> loss before: 0.08903420805467323, loss after: 0.08907881649542893]
ENDING EPOCH 401/1000 [loss before: 0.22823677777441415, loss after: 0.2310885205649135; epoch time: 0.09005045890808105 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.13072470148918786, loss after: 0.1305334007697235]
[epoch 501/1000, batch 11/100 -> loss before: 0.4159911143553476, loss after: 0.4151767898719506]
[epoch 501/1000, batch 21/100 -> loss before: 0.4848369126666726, loss after: 0.4846300208304342]
[epoch 501/1000, batch 31/100 -> loss before: 0.11186583676141144, loss after: 0.11191992485251631]
[epoch 501/1000, batch 41/100 -> loss before: 0.23696990740131202, loss after: 0.2359542937507398]
[epoch 501/1000, batch 51/100 -> loss before: 0.16052996735673292, loss after: 0.16053857866767557]
[epoch 501/1000, batch 61/100 -> loss before: 0.4601561810548171, loss after: 0.4593976148197051]
[epoch 501/1000, batch 71/100 -> loss before: 0.2404534427666151, loss after: 0.23944046728165022]
[epoch 501/1000, batch 81/100 -> loss before: 0.2951918585050379, loss after: 0.2948756225262329]
[epoch 501/1000, batch 91/100 -> loss before: 0.21499721683743664, loss after: 0.21544510181556564]
ENDING EPOCH 501/1000 [loss before: 0.23113951057652193, loss after: 0.23132440017373002; epoch time: 0.09561014175415039 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.24017739531605864, loss after: 0.23671123277748815]
[epoch 601/1000, batch 11/100 -> loss before: 0.3575632833967345, loss after: 0.3532242117130541]
[epoch 601/1000, batch 21/100 -> loss before: 0.4227273904359324, loss after: 0.41811634387537966]
[epoch 601/1000, batch 31/100 -> loss before: 0.11948573074642091, loss after: 0.11865148264324832]
[epoch 601/1000, batch 41/100 -> loss before: 0.2088926706348703, loss after: 0.20817167506770443]
[epoch 601/1000, batch 51/100 -> loss before: 0.3305619459223757, loss after: 0.3304276238327922]
[epoch 601/1000, batch 61/100 -> loss before: 0.20154171526969092, loss after: 0.2005189952359379]
[epoch 601/1000, batch 71/100 -> loss before: 0.19200100082924157, loss after: 0.1919093264861895]
[epoch 601/1000, batch 81/100 -> loss before: 0.2170118957238767, loss after: 0.21679037668677395]
[epoch 601/1000, batch 91/100 -> loss before: 0.49972337433373165, loss after: 0.49963021959179477]
ENDING EPOCH 601/1000 [loss before: 0.24171838527641304, loss after: 0.2233524103158969; epoch time: 0.09217476844787598 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.1324673073424118, loss after: 0.13310007953516095]
[epoch 701/1000, batch 11/100 -> loss before: 0.2780422529533575, loss after: 0.2759473579476896]
[epoch 701/1000, batch 21/100 -> loss before: 0.2477065695269725, loss after: 0.24630678681971058]
[epoch 701/1000, batch 31/100 -> loss before: 0.2419659046303912, loss after: 0.2413862820890218]
[epoch 701/1000, batch 41/100 -> loss before: 0.10501502113742926, loss after: 0.10398205148053571]
[epoch 701/1000, batch 51/100 -> loss before: 0.11171496895486777, loss after: 0.11166178592491842]
[epoch 701/1000, batch 61/100 -> loss before: 0.2568486722751563, loss after: 0.25663320106634735]
[epoch 701/1000, batch 71/100 -> loss before: 0.10100987970832673, loss after: 0.10003657028030392]
[epoch 701/1000, batch 81/100 -> loss before: 0.21319961491288714, loss after: 0.21341684973761957]
[epoch 701/1000, batch 91/100 -> loss before: 0.3218651826080792, loss after: 0.32241247680520524]
ENDING EPOCH 701/1000 [loss before: 0.2327328111177578, loss after: 0.2302894828179056; epoch time: 0.09436583518981934 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.1865935308809194, loss after: 0.18600594765629894]
[epoch 801/1000, batch 11/100 -> loss before: 0.2971722527247418, loss after: 0.2971657183927172]
[epoch 801/1000, batch 21/100 -> loss before: 0.1793183793582816, loss after: 0.1785068183056842]
[epoch 801/1000, batch 31/100 -> loss before: 0.2386344961153517, loss after: 0.23468683685288622]
[epoch 801/1000, batch 41/100 -> loss before: 0.23433600247589834, loss after: 0.23461715861098975]
[epoch 801/1000, batch 51/100 -> loss before: 0.19413716907216194, loss after: 0.19386399162782564]
[epoch 801/1000, batch 61/100 -> loss before: 0.21774220007853753, loss after: 0.21754237828345852]
[epoch 801/1000, batch 71/100 -> loss before: 0.227098079162402, loss after: 0.22676816492836127]
[epoch 801/1000, batch 81/100 -> loss before: 0.16307217996522155, loss after: 0.1631546687333078]
[epoch 801/1000, batch 91/100 -> loss before: 0.34093822203547886, loss after: 0.3409130068152947]
ENDING EPOCH 801/1000 [loss before: 0.23129278351980004, loss after: 0.23020252598025245; epoch time: 0.09540057182312012 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.19507929418408315, loss after: 0.19430006633308006]
[epoch 901/1000, batch 11/100 -> loss before: 0.2190745072890436, loss after: 0.21832485131119794]
[epoch 901/1000, batch 21/100 -> loss before: 0.06514381938792377, loss after: 0.06543798318614778]
[epoch 901/1000, batch 31/100 -> loss before: 0.19122192930925555, loss after: 0.1891935850016741]
[epoch 901/1000, batch 41/100 -> loss before: 0.17191024653272172, loss after: 0.20799131369680227]
[epoch 901/1000, batch 51/100 -> loss before: 0.2322521288015486, loss after: 0.23196496319440243]
[epoch 901/1000, batch 61/100 -> loss before: 0.26232459022356924, loss after: 0.2617406595355998]
[epoch 901/1000, batch 71/100 -> loss before: 0.37189066440435253, loss after: 0.3717906486565177]
[epoch 901/1000, batch 81/100 -> loss before: 0.17548818940746233, loss after: 0.17426222483254544]
[epoch 901/1000, batch 91/100 -> loss before: 0.23289519261923913, loss after: 0.23471667958403333]
ENDING EPOCH 901/1000 [loss before: 0.22446449074703959, loss after: 0.22792457877250374; epoch time: 0.0925455093383789 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.25441962663809703, loss after: 0.2541698720499086]
[epoch 1000/1000, batch 11/100 -> loss before: 0.19107398000862486, loss after: 0.1872571447889466]
[epoch 1000/1000, batch 21/100 -> loss before: 0.1880191497014606, loss after: 0.18720102464816848]
[epoch 1000/1000, batch 31/100 -> loss before: 0.22504354063641996, loss after: 0.22455634261817167]
[epoch 1000/1000, batch 41/100 -> loss before: 0.06738248463779037, loss after: 0.06653807337508909]
[epoch 1000/1000, batch 51/100 -> loss before: 0.15233052523469837, loss after: 0.15233049399838164]
[epoch 1000/1000, batch 61/100 -> loss before: 0.13262854619660386, loss after: 0.13249485513850945]
[epoch 1000/1000, batch 71/100 -> loss before: 0.1729545474856166, loss after: 0.17003329830356445]
[epoch 1000/1000, batch 81/100 -> loss before: 0.3885046201731196, loss after: 0.3854664356744651]
[epoch 1000/1000, batch 91/100 -> loss before: 0.34674495117869386, loss after: 0.3452335487620478]
ENDING EPOCH 1000/1000 [loss before: 0.19957966159398238, loss after: 0.19858143240100123; epoch time: 0.09218001365661621 s]
FIT DONE. [time: 85.99112606048584 s]
LOSS TRAIN (MSE): 0.19858143240100123
LOSS TEST (MSE): 0.19525980591592584
R^2 TRAIN: 0.29849260305954983
R^2 TEST: 0.2964380453929861
EXPERIMENT DONE
