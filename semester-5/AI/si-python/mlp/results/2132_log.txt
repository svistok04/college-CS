EXPERIMENT 2132 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[128, 128, 64, 64, 32, 32], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=rmsprop, learning_rate=0.001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 32481]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.26392389365226554, loss after: 0.2917669315888228]
[epoch 1/1000, batch 11/100 -> loss before: 0.35672296344568954, loss after: 0.2718006658136686]
[epoch 1/1000, batch 21/100 -> loss before: 0.2999677809202647, loss after: 0.28071407823839256]
[epoch 1/1000, batch 31/100 -> loss before: 0.3427196071556035, loss after: 0.34164100821769444]
[epoch 1/1000, batch 41/100 -> loss before: 0.2858066056536494, loss after: 0.28565251617355814]
[epoch 1/1000, batch 51/100 -> loss before: 0.16999537768222797, loss after: 0.13131645605787878]
[epoch 1/1000, batch 61/100 -> loss before: 0.2609159079883812, loss after: 0.23591001097000586]
[epoch 1/1000, batch 71/100 -> loss before: 0.32896223886785514, loss after: 0.3258207197321483]
[epoch 1/1000, batch 81/100 -> loss before: 0.3790968447936005, loss after: 0.3779884054225925]
[epoch 1/1000, batch 91/100 -> loss before: 0.3496769604728353, loss after: 0.3311888148483945]
ENDING EPOCH 1/1000 [loss before: 0.2838936199301506, loss after: 0.29924029947096936; epoch time: 0.07388830184936523 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.18035253536068038, loss after: 0.1672355623182824]
[epoch 101/1000, batch 11/100 -> loss before: 0.21610310050817477, loss after: 0.19163116842852973]
[epoch 101/1000, batch 21/100 -> loss before: 0.1448256064354158, loss after: 0.10680134128139354]
[epoch 101/1000, batch 31/100 -> loss before: 0.03853108802903982, loss after: 0.03611625948555713]
[epoch 101/1000, batch 41/100 -> loss before: 0.21829108389201837, loss after: 0.1979670099683874]
[epoch 101/1000, batch 51/100 -> loss before: 0.1529650004775795, loss after: 0.1386450911023635]
[epoch 101/1000, batch 61/100 -> loss before: 0.06472663612418825, loss after: 0.05774279757098372]
[epoch 101/1000, batch 71/100 -> loss before: 0.3798012041022051, loss after: 0.3379087825898176]
[epoch 101/1000, batch 81/100 -> loss before: 0.22806514350259333, loss after: 0.1992459613976934]
[epoch 101/1000, batch 91/100 -> loss before: 0.16976459546369652, loss after: 0.1669606636420251]
ENDING EPOCH 101/1000 [loss before: 0.19387565678599664, loss after: 0.19101414266623473; epoch time: 0.07245326042175293 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.36312588558672354, loss after: 0.35878011633716944]
[epoch 201/1000, batch 11/100 -> loss before: 0.291656476227636, loss after: 0.2864900689885224]
[epoch 201/1000, batch 21/100 -> loss before: 0.21127557230797006, loss after: 0.1542394683518116]
[epoch 201/1000, batch 31/100 -> loss before: 0.10264771684522837, loss after: 0.07480169222497646]
[epoch 201/1000, batch 41/100 -> loss before: 0.06729172876289025, loss after: 0.05243931308366821]
[epoch 201/1000, batch 51/100 -> loss before: 0.11832093788768024, loss after: 0.09205533241892382]
[epoch 201/1000, batch 61/100 -> loss before: 0.12372575088040963, loss after: 0.08626777715118991]
[epoch 201/1000, batch 71/100 -> loss before: 0.033584938131190636, loss after: 0.02314260498504527]
[epoch 201/1000, batch 81/100 -> loss before: 0.23915479420206723, loss after: 0.17196227685982843]
[epoch 201/1000, batch 91/100 -> loss before: 0.10639261986918787, loss after: 0.09314170921055934]
ENDING EPOCH 201/1000 [loss before: 0.1105010198675329, loss after: 0.10408722911584442; epoch time: 0.07139945030212402 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.02606843115257985, loss after: 0.007432013644184092]
[epoch 301/1000, batch 11/100 -> loss before: 0.013381722160524984, loss after: 0.00712519507659527]
[epoch 301/1000, batch 21/100 -> loss before: 0.02268302800387503, loss after: 0.024530160547317045]
[epoch 301/1000, batch 31/100 -> loss before: 0.08094712287903486, loss after: 0.02775213841335336]
[epoch 301/1000, batch 41/100 -> loss before: 0.06354376255163126, loss after: 0.017383385630047914]
[epoch 301/1000, batch 51/100 -> loss before: 0.035403185397205245, loss after: 0.021127898324265426]
[epoch 301/1000, batch 61/100 -> loss before: 0.009902539878351203, loss after: 0.006712879585240502]
[epoch 301/1000, batch 71/100 -> loss before: 0.038336761970805086, loss after: 0.009991067533033715]
[epoch 301/1000, batch 81/100 -> loss before: 0.007407989943987771, loss after: 0.0028374366136098266]
[epoch 301/1000, batch 91/100 -> loss before: 0.029418528549128777, loss after: 0.026860720418471234]
ENDING EPOCH 301/1000 [loss before: 0.054527878674666214, loss after: 0.05510591375302089; epoch time: 0.0766596794128418 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.1142640312445452, loss after: 0.10992436842046134]
[epoch 401/1000, batch 11/100 -> loss before: 0.07656097744030674, loss after: 0.044422934683272394]
[epoch 401/1000, batch 21/100 -> loss before: 0.00520073028759369, loss after: 0.001471922131824585]
[epoch 401/1000, batch 31/100 -> loss before: 0.03172811567515776, loss after: 0.011867540761617166]
[epoch 401/1000, batch 41/100 -> loss before: 0.02118050386997767, loss after: 0.01245747510003761]
[epoch 401/1000, batch 51/100 -> loss before: 0.027717274910340622, loss after: 0.008026220616873251]
[epoch 401/1000, batch 61/100 -> loss before: 0.014035730732180335, loss after: 0.007471813700639099]
[epoch 401/1000, batch 71/100 -> loss before: 0.00960721076349173, loss after: 0.00528999161223201]
[epoch 401/1000, batch 81/100 -> loss before: 0.036496710525037376, loss after: 0.019545114522020453]
[epoch 401/1000, batch 91/100 -> loss before: 0.009541659864661548, loss after: 0.004351062482882175]
ENDING EPOCH 401/1000 [loss before: 0.030165655635996408, loss after: 0.04723689700114335; epoch time: 0.0726776123046875 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.028084003961308335, loss after: 0.021817016832447193]
[epoch 501/1000, batch 11/100 -> loss before: 0.07032108567802453, loss after: 0.03814553002059056]
[epoch 501/1000, batch 21/100 -> loss before: 0.01926996403820514, loss after: 0.005791776141639597]
[epoch 501/1000, batch 31/100 -> loss before: 0.013296844169407695, loss after: 0.008791942765296597]
[epoch 501/1000, batch 41/100 -> loss before: 0.01841021552176004, loss after: 0.007771434923788759]
[epoch 501/1000, batch 51/100 -> loss before: 0.017360693057009748, loss after: 0.007637947532820215]
[epoch 501/1000, batch 61/100 -> loss before: 0.009056871841634145, loss after: 0.0038163740301082835]
[epoch 501/1000, batch 71/100 -> loss before: 0.012815090445854096, loss after: 0.012722658479180965]
[epoch 501/1000, batch 81/100 -> loss before: 0.011280922683963145, loss after: 0.006867280797367948]
[epoch 501/1000, batch 91/100 -> loss before: 0.037888112461749134, loss after: 0.01253947069721752]
ENDING EPOCH 501/1000 [loss before: 0.020537383746853523, loss after: 0.023088798094828975; epoch time: 0.07651829719543457 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.0076269291474863775, loss after: 0.004783930654637858]
[epoch 601/1000, batch 11/100 -> loss before: 0.008678054984640653, loss after: 0.003970918153756426]
[epoch 601/1000, batch 21/100 -> loss before: 0.07828091875583772, loss after: 0.062174728186732885]
[epoch 601/1000, batch 31/100 -> loss before: 0.027686914755325153, loss after: 0.0142713923834868]
[epoch 601/1000, batch 41/100 -> loss before: 0.03812996524378228, loss after: 0.02506760436362515]
[epoch 601/1000, batch 51/100 -> loss before: 0.014478392802286234, loss after: 0.004946441801512478]
[epoch 601/1000, batch 61/100 -> loss before: 0.009139075372718606, loss after: 0.013424438846834955]
[epoch 601/1000, batch 71/100 -> loss before: 0.011019587234605405, loss after: 0.005088036957300758]
[epoch 601/1000, batch 81/100 -> loss before: 0.015632045072795215, loss after: 0.003829423609493339]
[epoch 601/1000, batch 91/100 -> loss before: 0.06836734836622807, loss after: 0.04379223923260669]
ENDING EPOCH 601/1000 [loss before: 0.018980397685630444, loss after: 0.020989841306661226; epoch time: 0.07538986206054688 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.010577956197779844, loss after: 0.004882635926520983]
[epoch 701/1000, batch 11/100 -> loss before: 0.014664864348895893, loss after: 0.012951623671110205]
[epoch 701/1000, batch 21/100 -> loss before: 0.013978314098488898, loss after: 0.007194456114376628]
[epoch 701/1000, batch 31/100 -> loss before: 0.004168359109581028, loss after: 0.0010704809184748456]
[epoch 701/1000, batch 41/100 -> loss before: 0.012870414944264958, loss after: 0.0037636935761495354]
[epoch 701/1000, batch 51/100 -> loss before: 0.015938504046734886, loss after: 0.005256292626490255]
[epoch 701/1000, batch 61/100 -> loss before: 0.03089326096362428, loss after: 0.02302205953866654]
[epoch 701/1000, batch 71/100 -> loss before: 0.011774332500355736, loss after: 0.0017302712312999822]
[epoch 701/1000, batch 81/100 -> loss before: 0.008450378917358248, loss after: 0.0028490467523286343]
[epoch 701/1000, batch 91/100 -> loss before: 0.007286927851068647, loss after: 0.004452026482295045]
ENDING EPOCH 701/1000 [loss before: 0.01733477688981166, loss after: 0.01908292554828803; epoch time: 0.07255721092224121 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.005043638708425219, loss after: 0.002262234383589085]
[epoch 801/1000, batch 11/100 -> loss before: 0.004060141824994432, loss after: 0.001330577557346093]
[epoch 801/1000, batch 21/100 -> loss before: 0.042952699466149706, loss after: 0.007156659861963918]
[epoch 801/1000, batch 31/100 -> loss before: 0.01879967487105371, loss after: 0.005241602303246286]
[epoch 801/1000, batch 41/100 -> loss before: 0.0044318615821678665, loss after: 0.003779658288088239]
[epoch 801/1000, batch 51/100 -> loss before: 0.013510533338441478, loss after: 0.004445780903484829]
[epoch 801/1000, batch 61/100 -> loss before: 0.012648608113604538, loss after: 0.007631688835820817]
[epoch 801/1000, batch 71/100 -> loss before: 0.02644484152755854, loss after: 0.010711311275712992]
[epoch 801/1000, batch 81/100 -> loss before: 0.015260509358786593, loss after: 0.01877700617331899]
[epoch 801/1000, batch 91/100 -> loss before: 0.021238432402061254, loss after: 0.013907033422804933]
ENDING EPOCH 801/1000 [loss before: 0.017667465359492283, loss after: 0.020876015819274767; epoch time: 0.07635998725891113 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.006582261719713272, loss after: 0.00248796439481752]
[epoch 901/1000, batch 11/100 -> loss before: 0.011727495532372576, loss after: 0.002043001724684073]
[epoch 901/1000, batch 21/100 -> loss before: 0.00787910763081625, loss after: 0.002326478686802756]
[epoch 901/1000, batch 31/100 -> loss before: 0.010262470839902381, loss after: 0.0031727629424401042]
[epoch 901/1000, batch 41/100 -> loss before: 0.0069138333677174115, loss after: 0.0054903981847941]
[epoch 901/1000, batch 51/100 -> loss before: 0.08672691868904285, loss after: 0.05071007070623468]
[epoch 901/1000, batch 61/100 -> loss before: 0.01341136613053156, loss after: 0.004527344497382388]
[epoch 901/1000, batch 71/100 -> loss before: 0.013894051824196623, loss after: 0.011010193141834946]
[epoch 901/1000, batch 81/100 -> loss before: 0.008542051184553872, loss after: 0.00688410740626286]
[epoch 901/1000, batch 91/100 -> loss before: 0.016320080512054417, loss after: 0.010054894388857033]
ENDING EPOCH 901/1000 [loss before: 0.014696179863091855, loss after: 0.02021705571216552; epoch time: 0.08127880096435547 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.025085399354366194, loss after: 0.017535862709777333]
[epoch 1000/1000, batch 11/100 -> loss before: 0.020927453475678506, loss after: 0.009521857262583248]
[epoch 1000/1000, batch 21/100 -> loss before: 0.006942930951662814, loss after: 0.0026794090353410983]
[epoch 1000/1000, batch 31/100 -> loss before: 0.005211939361849013, loss after: 0.002215331638680467]
[epoch 1000/1000, batch 41/100 -> loss before: 0.012136304911106685, loss after: 0.007104996997247226]
[epoch 1000/1000, batch 51/100 -> loss before: 0.01869535182533708, loss after: 0.0105206890919709]
[epoch 1000/1000, batch 61/100 -> loss before: 0.008685191609994092, loss after: 0.004426976558081581]
[epoch 1000/1000, batch 71/100 -> loss before: 0.007982177829416123, loss after: 0.005911829857745731]
[epoch 1000/1000, batch 81/100 -> loss before: 0.0037890279466069876, loss after: 0.0025160984445435847]
[epoch 1000/1000, batch 91/100 -> loss before: 0.10043960900946691, loss after: 0.053186598062282577]
ENDING EPOCH 1000/1000 [loss before: 0.02137597287764347, loss after: 0.012952438899127476; epoch time: 0.07535696029663086 s]
FIT DONE. [time: 68.08992743492126 s]
LOSS TRAIN (MSE): 0.012952438899127476
LOSS TEST (MSE): 0.01688130623388105
R^2 TRAIN: 0.9542443037785675
R^2 TEST: 0.9391731198619397
EXPERIMENT DONE
