EXPERIMENT 2222 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[128, 128, 64, 64, 32, 32], activation_name=relu, targets_activation_name=linear, 
initialization_name=uniform, algo_name=sgd_momentum, learning_rate=0.001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 32481]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.87553713426397, loss after: 0.8757682416145164]
[epoch 1/1000, batch 11/100 -> loss before: 0.47751121669242486, loss after: 0.3874486517405716]
[epoch 1/1000, batch 21/100 -> loss before: 0.37327453581147657, loss after: 0.33250881050153513]
[epoch 1/1000, batch 31/100 -> loss before: 0.40471502772009604, loss after: 0.40982154942402255]
[epoch 1/1000, batch 41/100 -> loss before: 0.33778791296536553, loss after: 0.35626453122609936]
[epoch 1/1000, batch 51/100 -> loss before: 0.2006877422758358, loss after: 0.1734035840622836]
[epoch 1/1000, batch 61/100 -> loss before: 0.24448463829202544, loss after: 0.30605678816823456]
[epoch 1/1000, batch 71/100 -> loss before: 0.4724492801621249, loss after: 0.39495675363823135]
[epoch 1/1000, batch 81/100 -> loss before: 0.5140686106053542, loss after: 0.37790917360977827]
[epoch 1/1000, batch 91/100 -> loss before: 0.44986382674636555, loss after: 0.3901618153140085]
ENDING EPOCH 1/1000 [loss before: 1.102092538201909, loss after: 0.27785204738146524; epoch time: 0.050496578216552734 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.22314627777777202, loss after: 0.21095244470157937]
[epoch 101/1000, batch 11/100 -> loss before: 0.19399014498868172, loss after: 0.18060263749081723]
[epoch 101/1000, batch 21/100 -> loss before: 0.20187380430122012, loss after: 0.18369855092208115]
[epoch 101/1000, batch 31/100 -> loss before: 0.07399083650214532, loss after: 0.07511469628558029]
[epoch 101/1000, batch 41/100 -> loss before: 0.10472155213337438, loss after: 0.11453872187928471]
[epoch 101/1000, batch 51/100 -> loss before: 0.09277854883401378, loss after: 0.08197430488082671]
[epoch 101/1000, batch 61/100 -> loss before: 0.11899534073767495, loss after: 0.11914683425491943]
[epoch 101/1000, batch 71/100 -> loss before: 0.26363240148833345, loss after: 0.2620530107115156]
[epoch 101/1000, batch 81/100 -> loss before: 0.2016286194820262, loss after: 0.19904613397005091]
[epoch 101/1000, batch 91/100 -> loss before: 0.21596639279800695, loss after: 0.2126576715400908]
ENDING EPOCH 101/1000 [loss before: 0.18561121149282964, loss after: 0.17249156630175355; epoch time: 0.05125594139099121 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.384182925967473, loss after: 0.349543584641121]
[epoch 201/1000, batch 11/100 -> loss before: 0.23216457396642537, loss after: 0.21361379619019547]
[epoch 201/1000, batch 21/100 -> loss before: 0.3285008102153646, loss after: 0.26195180520947703]
[epoch 201/1000, batch 31/100 -> loss before: 0.24068779121445422, loss after: 0.22118151876958145]
[epoch 201/1000, batch 41/100 -> loss before: 0.15684755547482734, loss after: 0.136017194740048]
[epoch 201/1000, batch 51/100 -> loss before: 0.10490167292114257, loss after: 0.09620335041417677]
[epoch 201/1000, batch 61/100 -> loss before: 0.1537124187626278, loss after: 0.14752239411653098]
[epoch 201/1000, batch 71/100 -> loss before: 0.047825498653475455, loss after: 0.03946159559292477]
[epoch 201/1000, batch 81/100 -> loss before: 0.2720926631486249, loss after: 0.22945533413228203]
[epoch 201/1000, batch 91/100 -> loss before: 0.15931854918647673, loss after: 0.13737365841627322]
ENDING EPOCH 201/1000 [loss before: 0.13628711543214683, loss after: 0.1459155720302101; epoch time: 0.050452232360839844 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.0175265373848103, loss after: 0.012666830239299778]
[epoch 301/1000, batch 11/100 -> loss before: 0.051327722168519975, loss after: 0.04107987383282655]
[epoch 301/1000, batch 21/100 -> loss before: 0.08472276208840329, loss after: 0.07431690401899385]
[epoch 301/1000, batch 31/100 -> loss before: 0.08014837004237453, loss after: 0.07770315398738174]
[epoch 301/1000, batch 41/100 -> loss before: 0.32498053942924654, loss after: 0.2260243110727759]
[epoch 301/1000, batch 51/100 -> loss before: 0.039042134548496775, loss after: 0.0394612361140105]
[epoch 301/1000, batch 61/100 -> loss before: 0.09684625098283532, loss after: 0.09797076510396156]
[epoch 301/1000, batch 71/100 -> loss before: 0.05920461210043878, loss after: 0.05605060484628421]
[epoch 301/1000, batch 81/100 -> loss before: 0.02070061999664406, loss after: 0.020565487782978353]
[epoch 301/1000, batch 91/100 -> loss before: 0.0929767160762471, loss after: 0.08318762064114316]
ENDING EPOCH 301/1000 [loss before: 0.1063931369279987, loss after: 0.10704606861149125; epoch time: 0.04931139945983887 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.27560865756353986, loss after: 0.26965288746183325]
[epoch 401/1000, batch 11/100 -> loss before: 0.10513878204161944, loss after: 0.10041359714217275]
[epoch 401/1000, batch 21/100 -> loss before: 0.04865295288054062, loss after: 0.041131716292173263]
[epoch 401/1000, batch 31/100 -> loss before: 0.034132464453382326, loss after: 0.03221916738770601]
[epoch 401/1000, batch 41/100 -> loss before: 0.03530156913058011, loss after: 0.0340366797994229]
[epoch 401/1000, batch 51/100 -> loss before: 0.130846017590668, loss after: 0.11795647474117363]
[epoch 401/1000, batch 61/100 -> loss before: 0.06688355378308977, loss after: 0.06857211681907918]
[epoch 401/1000, batch 71/100 -> loss before: 0.08719730225172168, loss after: 0.07515067813511442]
[epoch 401/1000, batch 81/100 -> loss before: 0.09337096692307227, loss after: 0.06877408075294265]
[epoch 401/1000, batch 91/100 -> loss before: 0.04952033137623606, loss after: 0.045866862787744704]
ENDING EPOCH 401/1000 [loss before: 0.08380723661498932, loss after: 0.07330342957646478; epoch time: 0.05317211151123047 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.04161494845482962, loss after: 0.05102829724155254]
[epoch 501/1000, batch 11/100 -> loss before: 0.09290987676060417, loss after: 0.09576644246099644]
[epoch 501/1000, batch 21/100 -> loss before: 0.16181212666167616, loss after: 0.1517175860686915]
[epoch 501/1000, batch 31/100 -> loss before: 0.07198723687204779, loss after: 0.06703625504609725]
[epoch 501/1000, batch 41/100 -> loss before: 0.07822094468907523, loss after: 0.07005533493451552]
[epoch 501/1000, batch 51/100 -> loss before: 0.0749439741183541, loss after: 0.08583277173375246]
[epoch 501/1000, batch 61/100 -> loss before: 0.012454992508084855, loss after: 0.012098806919806538]
[epoch 501/1000, batch 71/100 -> loss before: 0.05833357597314119, loss after: 0.05022351697781069]
[epoch 501/1000, batch 81/100 -> loss before: 0.07981126245215053, loss after: 0.07075390370351667]
[epoch 501/1000, batch 91/100 -> loss before: 0.046990824933829514, loss after: 0.04308640541664361]
ENDING EPOCH 501/1000 [loss before: 0.07359042936672626, loss after: 0.07332468920804835; epoch time: 0.05372476577758789 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.05122394959240719, loss after: 0.041462326202703056]
[epoch 601/1000, batch 11/100 -> loss before: 0.03621501478492914, loss after: 0.02999201789801536]
[epoch 601/1000, batch 21/100 -> loss before: 0.26243149379153324, loss after: 0.19362437128558435]
[epoch 601/1000, batch 31/100 -> loss before: 0.06423233354781992, loss after: 0.05197428105632714]
[epoch 601/1000, batch 41/100 -> loss before: 0.09775582374030717, loss after: 0.10436797700728692]
[epoch 601/1000, batch 51/100 -> loss before: 0.020560819542508848, loss after: 0.02140067009620889]
[epoch 601/1000, batch 61/100 -> loss before: 0.08217086770846743, loss after: 0.058934959306633394]
[epoch 601/1000, batch 71/100 -> loss before: 0.04419257616078244, loss after: 0.04324266640231424]
[epoch 601/1000, batch 81/100 -> loss before: 0.007309723033768749, loss after: 0.00938169087759107]
[epoch 601/1000, batch 91/100 -> loss before: 0.045701113144416695, loss after: 0.042717258004768374]
ENDING EPOCH 601/1000 [loss before: 0.07529793794552427, loss after: 0.04594893741012834; epoch time: 0.054596900939941406 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.07044414479511399, loss after: 0.06979561494483205]
[epoch 701/1000, batch 11/100 -> loss before: 0.014832780216379904, loss after: 0.01208735702560019]
[epoch 701/1000, batch 21/100 -> loss before: 0.02849042197081491, loss after: 0.029328781899395796]
[epoch 701/1000, batch 31/100 -> loss before: 0.0224099163050078, loss after: 0.02264099157636249]
[epoch 701/1000, batch 41/100 -> loss before: 0.008004470965406716, loss after: 0.008331713443623946]
[epoch 701/1000, batch 51/100 -> loss before: 0.01417203463898859, loss after: 0.013589656833082619]
[epoch 701/1000, batch 61/100 -> loss before: 0.033671525463693015, loss after: 0.023866271584827993]
[epoch 701/1000, batch 71/100 -> loss before: 0.03972925606778744, loss after: 0.04268681277164978]
[epoch 701/1000, batch 81/100 -> loss before: 0.008816867214429206, loss after: 0.008464776652225213]
[epoch 701/1000, batch 91/100 -> loss before: 0.03317215099258897, loss after: 0.030283936867680904]
ENDING EPOCH 701/1000 [loss before: 0.06310380075963742, loss after: 0.03728639271524197; epoch time: 0.054903507232666016 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.009318184877950516, loss after: 0.00866584873272787]
[epoch 801/1000, batch 11/100 -> loss before: 0.0031538643132360384, loss after: 0.0032376372679502447]
[epoch 801/1000, batch 21/100 -> loss before: 0.015791406006854868, loss after: 0.01061907628863124]
[epoch 801/1000, batch 31/100 -> loss before: 0.007081879237942018, loss after: 0.006979811693521595]
[epoch 801/1000, batch 41/100 -> loss before: 0.0587293318638263, loss after: 0.04582106231052763]
[epoch 801/1000, batch 51/100 -> loss before: 0.015264925674466278, loss after: 0.025533268597278818]
[epoch 801/1000, batch 61/100 -> loss before: 0.022347827751943564, loss after: 0.027914535823880628]
[epoch 801/1000, batch 71/100 -> loss before: 0.06066557310211383, loss after: 0.055194903068140656]
[epoch 801/1000, batch 81/100 -> loss before: 0.025330367317912116, loss after: 0.021264845069283576]
[epoch 801/1000, batch 91/100 -> loss before: 0.04413399187680121, loss after: 0.036430643389260706]
ENDING EPOCH 801/1000 [loss before: 0.02473052982491715, loss after: 0.028963065841643914; epoch time: 0.04899930953979492 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.04332647434216545, loss after: 0.02832180510503719]
[epoch 901/1000, batch 11/100 -> loss before: 0.019825930972525167, loss after: 0.01796686127543213]
[epoch 901/1000, batch 21/100 -> loss before: 0.011813075665401535, loss after: 0.008501779690996187]
[epoch 901/1000, batch 31/100 -> loss before: 0.006298751338776501, loss after: 0.0061973497374367475]
[epoch 901/1000, batch 41/100 -> loss before: 0.023785322694617476, loss after: 0.012419290276636605]
[epoch 901/1000, batch 51/100 -> loss before: 0.10170383490958212, loss after: 0.09979263682511808]
[epoch 901/1000, batch 61/100 -> loss before: 0.013141775934645489, loss after: 0.013424357478789834]
[epoch 901/1000, batch 71/100 -> loss before: 0.01885465389012966, loss after: 0.01353929258729577]
[epoch 901/1000, batch 81/100 -> loss before: 0.011794484821437344, loss after: 0.009881804876627293]
[epoch 901/1000, batch 91/100 -> loss before: 0.027663769341575817, loss after: 0.024732297549396876]
ENDING EPOCH 901/1000 [loss before: 0.03580795117132942, loss after: 0.025033937616199636; epoch time: 0.04799365997314453 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.021244334502345696, loss after: 0.023700322541345673]
[epoch 1000/1000, batch 11/100 -> loss before: 0.010093216681166124, loss after: 0.013127160454692746]
[epoch 1000/1000, batch 21/100 -> loss before: 0.007781559887066363, loss after: 0.008522947962756947]
[epoch 1000/1000, batch 31/100 -> loss before: 0.04441279959634466, loss after: 0.030811841499666877]
[epoch 1000/1000, batch 41/100 -> loss before: 0.025834058310099085, loss after: 0.01632580035974661]
[epoch 1000/1000, batch 51/100 -> loss before: 0.01901106115084481, loss after: 0.017872588506047258]
[epoch 1000/1000, batch 61/100 -> loss before: 0.012223011499869933, loss after: 0.006953509784364318]
[epoch 1000/1000, batch 71/100 -> loss before: 0.007887521016546061, loss after: 0.006844463680045304]
[epoch 1000/1000, batch 81/100 -> loss before: 0.00912896900703252, loss after: 0.00759963464384526]
[epoch 1000/1000, batch 91/100 -> loss before: 0.0634900337462851, loss after: 0.05440678530895253]
ENDING EPOCH 1000/1000 [loss before: 0.01834779017254668, loss after: 0.015007065591486948; epoch time: 0.04819774627685547 s]
FIT DONE. [time: 47.51705622673035 s]
LOSS TRAIN (MSE): 0.015007065591486948
LOSS TEST (MSE): 0.025083726204282328
R^2 TRAIN: 0.9469861437118654
R^2 TEST: 0.9096180836894264
EXPERIMENT DONE
