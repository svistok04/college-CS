EXPERIMENT 2113 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[64, 64, 64, 64, 64, 32, 32, 32, 32, 32, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=sgd_simple, learning_rate=0.001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 25185]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.6815884323265685, loss after: 0.6135316305343468]
[epoch 1/1000, batch 11/100 -> loss before: 0.29045210375521935, loss after: 0.2897135209655782]
[epoch 1/1000, batch 21/100 -> loss before: 0.4095763057729281, loss after: 0.4019945784851001]
[epoch 1/1000, batch 31/100 -> loss before: 0.3698389622882622, loss after: 0.36982954683573804]
[epoch 1/1000, batch 41/100 -> loss before: 0.21900387313558078, loss after: 0.21642694638620216]
[epoch 1/1000, batch 51/100 -> loss before: 0.26839534566536305, loss after: 0.2632291892964308]
[epoch 1/1000, batch 61/100 -> loss before: 0.46804865541944185, loss after: 0.4679453156700501]
[epoch 1/1000, batch 71/100 -> loss before: 0.34448662030006877, loss after: 0.3435721621728717]
[epoch 1/1000, batch 81/100 -> loss before: 0.40285185115556665, loss after: 0.4020236427928376]
[epoch 1/1000, batch 91/100 -> loss before: 0.29474535099510646, loss after: 0.29418843027917363]
ENDING EPOCH 1/1000 [loss before: 0.599648552700766, loss after: 0.2832491657534558; epoch time: 0.10265517234802246 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.2906565930937817, loss after: 0.28958581366568853]
[epoch 101/1000, batch 11/100 -> loss before: 0.21975863579087168, loss after: 0.21975782954333667]
[epoch 101/1000, batch 21/100 -> loss before: 0.24147785875319538, loss after: 0.2408408764744409]
[epoch 101/1000, batch 31/100 -> loss before: 0.3922567016135607, loss after: 0.3883085587686268]
[epoch 101/1000, batch 41/100 -> loss before: 0.664921465726993, loss after: 0.6342867332000842]
[epoch 101/1000, batch 51/100 -> loss before: 0.2523471931446616, loss after: 0.24375765013468764]
[epoch 101/1000, batch 61/100 -> loss before: 0.5158289846731622, loss after: 0.5155840094102727]
[epoch 101/1000, batch 71/100 -> loss before: 0.14430594433226093, loss after: 0.1442516611983608]
[epoch 101/1000, batch 81/100 -> loss before: 0.26448009914155274, loss after: 0.26402240372178276]
[epoch 101/1000, batch 91/100 -> loss before: 0.24176190131292036, loss after: 0.2358370822951227]
ENDING EPOCH 101/1000 [loss before: 0.2838648719664602, loss after: 0.28317004805854107; epoch time: 0.08201313018798828 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.0886307272450191, loss after: 0.08855445460660825]
[epoch 201/1000, batch 11/100 -> loss before: 0.3554593658841207, loss after: 0.35405032055248276]
[epoch 201/1000, batch 21/100 -> loss before: 0.17763571203552891, loss after: 0.17376393790030858]
[epoch 201/1000, batch 31/100 -> loss before: 0.46390664850464197, loss after: 0.46147547663038424]
[epoch 201/1000, batch 41/100 -> loss before: 0.3293894402219575, loss after: 0.32362323664339737]
[epoch 201/1000, batch 51/100 -> loss before: 0.2848684522672425, loss after: 0.28443938451535944]
[epoch 201/1000, batch 61/100 -> loss before: 0.49196557689168213, loss after: 0.48936243167843163]
[epoch 201/1000, batch 71/100 -> loss before: 0.4093730301001163, loss after: 0.4090842963183777]
[epoch 201/1000, batch 81/100 -> loss before: 0.1987011815061408, loss after: 0.1987006761614009]
[epoch 201/1000, batch 91/100 -> loss before: 0.10904203473423554, loss after: 0.10903213857210634]
ENDING EPOCH 201/1000 [loss before: 0.2834088288421019, loss after: 0.2834880210191905; epoch time: 0.09389567375183105 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.19078196328062785, loss after: 0.19022764973161363]
[epoch 301/1000, batch 11/100 -> loss before: 0.20068716652622176, loss after: 0.2002294010409912]
[epoch 301/1000, batch 21/100 -> loss before: 0.31232859642840743, loss after: 0.3076354364993449]
[epoch 301/1000, batch 31/100 -> loss before: 0.33171093716592664, loss after: 0.3238851920949961]
[epoch 301/1000, batch 41/100 -> loss before: 0.2552379826351313, loss after: 0.25328504918784134]
[epoch 301/1000, batch 51/100 -> loss before: 0.2823807770663028, loss after: 0.2823773693110968]
[epoch 301/1000, batch 61/100 -> loss before: 0.3127445846466121, loss after: 0.3068854011641728]
[epoch 301/1000, batch 71/100 -> loss before: 0.30984268458019126, loss after: 0.30805865073967476]
[epoch 301/1000, batch 81/100 -> loss before: 0.32340793168291915, loss after: 0.3232466016053589]
[epoch 301/1000, batch 91/100 -> loss before: 0.3214118579199341, loss after: 0.3064285558667664]
ENDING EPOCH 301/1000 [loss before: 0.28345748289859674, loss after: 0.28387665712076493; epoch time: 0.08541178703308105 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.29023279239088035, loss after: 0.2890363231348636]
[epoch 401/1000, batch 11/100 -> loss before: 0.2747691338586745, loss after: 0.2744055631228899]
[epoch 401/1000, batch 21/100 -> loss before: 0.3340718424284415, loss after: 0.3239421674934766]
[epoch 401/1000, batch 31/100 -> loss before: 0.39695921830098957, loss after: 0.3617083015374654]
[epoch 401/1000, batch 41/100 -> loss before: 0.23686676626052075, loss after: 0.23663814805190597]
[epoch 401/1000, batch 51/100 -> loss before: 0.32537991536220884, loss after: 0.32514408156969654]
[epoch 401/1000, batch 61/100 -> loss before: 0.26378477728242655, loss after: 0.25369309951145347]
[epoch 401/1000, batch 71/100 -> loss before: 0.2684341160633806, loss after: 0.2679582132518758]
[epoch 401/1000, batch 81/100 -> loss before: 0.19886940232499384, loss after: 0.1966631168732218]
[epoch 401/1000, batch 91/100 -> loss before: 0.19426389140200048, loss after: 0.1938423085814138]
ENDING EPOCH 401/1000 [loss before: 0.28489427557720715, loss after: 0.28347874734927714; epoch time: 0.08184313774108887 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.28333180565579014, loss after: 0.281151637562745]
[epoch 501/1000, batch 11/100 -> loss before: 0.5764669990715723, loss after: 0.5752325196516263]
[epoch 501/1000, batch 21/100 -> loss before: 0.3803973236659803, loss after: 0.37980445711717054]
[epoch 501/1000, batch 31/100 -> loss before: 0.2458705444979489, loss after: 0.2444761434939739]
[epoch 501/1000, batch 41/100 -> loss before: 0.3155312976068759, loss after: 0.31543024581701895]
[epoch 501/1000, batch 51/100 -> loss before: 0.17750239529528294, loss after: 0.17685451631635787]
[epoch 501/1000, batch 61/100 -> loss before: 0.28968394711774803, loss after: 0.28968325887259905]
[epoch 501/1000, batch 71/100 -> loss before: 0.14786447391015603, loss after: 0.14781372783434252]
[epoch 501/1000, batch 81/100 -> loss before: 0.24338109527202728, loss after: 0.23659852329234976]
[epoch 501/1000, batch 91/100 -> loss before: 0.21426127760722563, loss after: 0.2140877984427368]
ENDING EPOCH 501/1000 [loss before: 0.2831077567226162, loss after: 0.2834179148138857; epoch time: 0.08382892608642578 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.3489027429043866, loss after: 0.3477282129260668]
[epoch 601/1000, batch 11/100 -> loss before: 0.3634415380621471, loss after: 0.3582744887715101]
[epoch 601/1000, batch 21/100 -> loss before: 0.2986626790152539, loss after: 0.29790146917108246]
[epoch 601/1000, batch 31/100 -> loss before: 0.14365632617654261, loss after: 0.14084808927765274]
[epoch 601/1000, batch 41/100 -> loss before: 0.35788487436876637, loss after: 0.3480673464395488]
[epoch 601/1000, batch 51/100 -> loss before: 0.2912379853268029, loss after: 0.2859917365553018]
[epoch 601/1000, batch 61/100 -> loss before: 0.136178770731635, loss after: 0.13617774320234036]
[epoch 601/1000, batch 71/100 -> loss before: 0.2626659406209117, loss after: 0.2518341841664519]
[epoch 601/1000, batch 81/100 -> loss before: 0.3441816257693179, loss after: 0.33077489528391907]
[epoch 601/1000, batch 91/100 -> loss before: 0.34803286969486136, loss after: 0.3479390228142438]
ENDING EPOCH 601/1000 [loss before: 0.2839284336265738, loss after: 0.2845090822236676; epoch time: 0.08272409439086914 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.1395314566481169, loss after: 0.13839804551232576]
[epoch 701/1000, batch 11/100 -> loss before: 0.15071278983701109, loss after: 0.14327115941819227]
[epoch 701/1000, batch 21/100 -> loss before: 0.22504792198179024, loss after: 0.2245100481631447]
[epoch 701/1000, batch 31/100 -> loss before: 0.20533469281866995, loss after: 0.20500914576006277]
[epoch 701/1000, batch 41/100 -> loss before: 0.19546440646089236, loss after: 0.19546243297078075]
[epoch 701/1000, batch 51/100 -> loss before: 0.5478190671401377, loss after: 0.5311460452790999]
[epoch 701/1000, batch 61/100 -> loss before: 0.13802906956540859, loss after: 0.13737652866635053]
[epoch 701/1000, batch 71/100 -> loss before: 0.36851047629400596, loss after: 0.36834841508891325]
[epoch 701/1000, batch 81/100 -> loss before: 0.2833623878009111, loss after: 0.28334950927532665]
[epoch 701/1000, batch 91/100 -> loss before: 0.35336171603957706, loss after: 0.35335943054987806]
ENDING EPOCH 701/1000 [loss before: 0.2830813539035966, loss after: 0.283078228416784; epoch time: 0.08525729179382324 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.2400732330834095, loss after: 0.22574765772777305]
[epoch 801/1000, batch 11/100 -> loss before: 0.11799264630932302, loss after: 0.1176994378158089]
[epoch 801/1000, batch 21/100 -> loss before: 0.2382153415337743, loss after: 0.23754213442217037]
[epoch 801/1000, batch 31/100 -> loss before: 0.21409834994100424, loss after: 0.21343246972053484]
[epoch 801/1000, batch 41/100 -> loss before: 0.4803037024922395, loss after: 0.4756770063591427]
[epoch 801/1000, batch 51/100 -> loss before: 0.2656405757689041, loss after: 0.26476310140758436]
[epoch 801/1000, batch 61/100 -> loss before: 0.2673263885045276, loss after: 0.26480092100713526]
[epoch 801/1000, batch 71/100 -> loss before: 0.3244155404586361, loss after: 0.3232140270306188]
[epoch 801/1000, batch 81/100 -> loss before: 0.29569326782637645, loss after: 0.2918880997215469]
[epoch 801/1000, batch 91/100 -> loss before: 0.24374056573878994, loss after: 0.24306390708018824]
ENDING EPOCH 801/1000 [loss before: 0.2831736274183336, loss after: 0.28307864594722093; epoch time: 0.05694580078125 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.13221782443763835, loss after: 0.1302732019526406]
[epoch 901/1000, batch 11/100 -> loss before: 0.3360022072024025, loss after: 0.3286332616513763]
[epoch 901/1000, batch 21/100 -> loss before: 0.24872759358322175, loss after: 0.24434903000862737]
[epoch 901/1000, batch 31/100 -> loss before: 0.2812326198492588, loss after: 0.2785683520811319]
[epoch 901/1000, batch 41/100 -> loss before: 0.34157752214721165, loss after: 0.33674089525520146]
[epoch 901/1000, batch 51/100 -> loss before: 0.2544883245394455, loss after: 0.24008659893880197]
[epoch 901/1000, batch 61/100 -> loss before: 0.40926566584207835, loss after: 0.40905764478092443]
[epoch 901/1000, batch 71/100 -> loss before: 0.17326287634781062, loss after: 0.17046909983843223]
[epoch 901/1000, batch 81/100 -> loss before: 0.3882212220271375, loss after: 0.3860092489553072]
[epoch 901/1000, batch 91/100 -> loss before: 0.20649739350673957, loss after: 0.20571806551338398]
ENDING EPOCH 901/1000 [loss before: 0.283411711384045, loss after: 0.2855727993006079; epoch time: 0.10511040687561035 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.2643468240090756, loss after: 0.26048298850939633]
[epoch 1000/1000, batch 11/100 -> loss before: 0.22992674563001425, loss after: 0.22925664961743988]
[epoch 1000/1000, batch 21/100 -> loss before: 0.3212802404173863, loss after: 0.31704907620141054]
[epoch 1000/1000, batch 31/100 -> loss before: 0.2378100756719846, loss after: 0.23739691756242984]
[epoch 1000/1000, batch 41/100 -> loss before: 0.29606273231748587, loss after: 0.287951899772141]
[epoch 1000/1000, batch 51/100 -> loss before: 0.3061442437539185, loss after: 0.3006104365880626]
[epoch 1000/1000, batch 61/100 -> loss before: 0.21467893376076205, loss after: 0.21141673905215277]
[epoch 1000/1000, batch 71/100 -> loss before: 0.3472724671649072, loss after: 0.3469207864728338]
[epoch 1000/1000, batch 81/100 -> loss before: 0.10233418737339355, loss after: 0.10230639594234922]
[epoch 1000/1000, batch 91/100 -> loss before: 0.351791418432606, loss after: 0.3497813249464106]
ENDING EPOCH 1000/1000 [loss before: 0.2837679214843602, loss after: 0.2831078335317115; epoch time: 0.0892794132232666 s]
FIT DONE. [time: 74.05643773078918 s]
LOSS TRAIN (MSE): 0.2831078335317115
LOSS TEST (MSE): 0.2775818533030335
R^2 TRAIN: -0.00010477793934926538
R^2 TEST: -0.00018552388303305634
EXPERIMENT DONE
