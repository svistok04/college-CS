EXPERIMENT 1223 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[64, 64, 64, 64, 64, 32, 32, 32, 32, 32, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8], activation_name=relu, targets_activation_name=linear, 
initialization_name=uniform, algo_name=sgd_momentum, learning_rate=0.01, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 25185]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.18318931763552243, loss after: 0.1759306711395067]
[epoch 1/1000, batch 11/100 -> loss before: 0.388476501624792, loss after: 0.3772876700187851]
[epoch 1/1000, batch 21/100 -> loss before: 0.35856102045566457, loss after: 0.36091074813743373]
[epoch 1/1000, batch 31/100 -> loss before: 0.36981913768190405, loss after: 0.36981112592833865]
[epoch 1/1000, batch 41/100 -> loss before: 0.2065728167739363, loss after: 0.2059021794052795]
[epoch 1/1000, batch 51/100 -> loss before: 0.25743155957705355, loss after: 0.25894286798153665]
[epoch 1/1000, batch 61/100 -> loss before: 0.46797361132281745, loss after: 0.4682276389073824]
[epoch 1/1000, batch 71/100 -> loss before: 0.34468547822712253, loss after: 0.3440375432153012]
[epoch 1/1000, batch 81/100 -> loss before: 0.4039050863105745, loss after: 0.4035825913756944]
[epoch 1/1000, batch 91/100 -> loss before: 0.29327459063118994, loss after: 0.29307673146244434]
ENDING EPOCH 1/1000 [loss before: 0.2899884218128069, loss after: 0.2868142396165869; epoch time: 0.09033632278442383 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.30476850525807525, loss after: 0.30589242141400735]
[epoch 101/1000, batch 11/100 -> loss before: 0.2230136903669487, loss after: 0.22238904764567757]
[epoch 101/1000, batch 21/100 -> loss before: 0.23791238680653626, loss after: 0.23730561825999502]
[epoch 101/1000, batch 31/100 -> loss before: 0.4166884987071412, loss after: 0.41293296079614833]
[epoch 101/1000, batch 41/100 -> loss before: 0.6483674669764837, loss after: 0.6378821254212921]
[epoch 101/1000, batch 51/100 -> loss before: 0.22660687418768655, loss after: 0.22457777033138798]
[epoch 101/1000, batch 61/100 -> loss before: 0.520146696580811, loss after: 0.5182505595276268]
[epoch 101/1000, batch 71/100 -> loss before: 0.14518378429502604, loss after: 0.14569294903672023]
[epoch 101/1000, batch 81/100 -> loss before: 0.26151178468017533, loss after: 0.2617237601944462]
[epoch 101/1000, batch 91/100 -> loss before: 0.22207854607594632, loss after: 0.22023906998074883]
ENDING EPOCH 101/1000 [loss before: 0.2907664591682795, loss after: 0.28310760041783023; epoch time: 0.09915518760681152 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.08905206243574965, loss after: 0.08922585670542589]
[epoch 201/1000, batch 11/100 -> loss before: 0.35453211596980083, loss after: 0.354251100694486]
[epoch 201/1000, batch 21/100 -> loss before: 0.16882862965334464, loss after: 0.16700849670297355]
[epoch 201/1000, batch 31/100 -> loss before: 0.4601954428579475, loss after: 0.4586750958603634]
[epoch 201/1000, batch 41/100 -> loss before: 0.3443067490372715, loss after: 0.3459797123918543]
[epoch 201/1000, batch 51/100 -> loss before: 0.2916031956113202, loss after: 0.2896574457237634]
[epoch 201/1000, batch 61/100 -> loss before: 0.4816995822515359, loss after: 0.4823571331040141]
[epoch 201/1000, batch 71/100 -> loss before: 0.41059283291253895, loss after: 0.40942166295192806]
[epoch 201/1000, batch 81/100 -> loss before: 0.19982475299498276, loss after: 0.19906011991216005]
[epoch 201/1000, batch 91/100 -> loss before: 0.11080835344699747, loss after: 0.11054794301212893]
ENDING EPOCH 201/1000 [loss before: 0.2837445446493394, loss after: 0.2839445953574097; epoch time: 0.08765220642089844 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.18699982384796943, loss after: 0.18721661393942218]
[epoch 301/1000, batch 11/100 -> loss before: 0.19898680593042495, loss after: 0.19892269479345104]
[epoch 301/1000, batch 21/100 -> loss before: 0.31653048249967575, loss after: 0.31530322895559326]
[epoch 301/1000, batch 31/100 -> loss before: 0.34055994718875426, loss after: 0.34122080863872456]
[epoch 301/1000, batch 41/100 -> loss before: 0.2593553497163307, loss after: 0.25621764800772784]
[epoch 301/1000, batch 51/100 -> loss before: 0.2859445123013461, loss after: 0.2848221024146582]
[epoch 301/1000, batch 61/100 -> loss before: 0.3057952334922738, loss after: 0.3033105321725949]
[epoch 301/1000, batch 71/100 -> loss before: 0.30785011413698804, loss after: 0.3073992720256803]
[epoch 301/1000, batch 81/100 -> loss before: 0.3258823248692143, loss after: 0.32613720532617657]
[epoch 301/1000, batch 91/100 -> loss before: 0.3267851071795026, loss after: 0.3206396638527801]
ENDING EPOCH 301/1000 [loss before: 0.2837186077795135, loss after: 0.28337587878910947; epoch time: 0.09042000770568848 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.2849106564945063, loss after: 0.2872543031835998]
[epoch 401/1000, batch 11/100 -> loss before: 0.2721098795863126, loss after: 0.27204343509476103]
[epoch 401/1000, batch 21/100 -> loss before: 0.32509039833634007, loss after: 0.32531239578639803]
[epoch 401/1000, batch 31/100 -> loss before: 0.45084612274033253, loss after: 0.44652252608131493]
[epoch 401/1000, batch 41/100 -> loss before: 0.23619000769202633, loss after: 0.2375563614367826]
[epoch 401/1000, batch 51/100 -> loss before: 0.32583736454899564, loss after: 0.32528530089304397]
[epoch 401/1000, batch 61/100 -> loss before: 0.2556776019215788, loss after: 0.2627309819555608]
[epoch 401/1000, batch 71/100 -> loss before: 0.2804824251591028, loss after: 0.281894529601963]
[epoch 401/1000, batch 81/100 -> loss before: 0.22749911662389555, loss after: 0.22426061681274265]
[epoch 401/1000, batch 91/100 -> loss before: 0.19400047600295214, loss after: 0.19264855279453258]
ENDING EPOCH 401/1000 [loss before: 0.2831558501053721, loss after: 0.28365342663758664; epoch time: 0.09255385398864746 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.28959253194945767, loss after: 0.2878533840403679]
[epoch 501/1000, batch 11/100 -> loss before: 0.5784181864810478, loss after: 0.5768878098364694]
[epoch 501/1000, batch 21/100 -> loss before: 0.37757394819672097, loss after: 0.377851823568743]
[epoch 501/1000, batch 31/100 -> loss before: 0.2401382176168239, loss after: 0.24019587398267467]
[epoch 501/1000, batch 41/100 -> loss before: 0.3148035156312625, loss after: 0.3149164539722494]
[epoch 501/1000, batch 51/100 -> loss before: 0.17453723653050263, loss after: 0.17464591552473177]
[epoch 501/1000, batch 61/100 -> loss before: 0.29000548060432396, loss after: 0.2899191938301924]
[epoch 501/1000, batch 71/100 -> loss before: 0.14902901126175844, loss after: 0.14940401225953964]
[epoch 501/1000, batch 81/100 -> loss before: 0.24637299751832695, loss after: 0.2427839264834552]
[epoch 501/1000, batch 91/100 -> loss before: 0.21557933208389807, loss after: 0.2159374049189678]
ENDING EPOCH 501/1000 [loss before: 0.2833585241204102, loss after: 0.2865619353243218; epoch time: 0.0920705795288086 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.3476604205330379, loss after: 0.3494506901966784]
[epoch 601/1000, batch 11/100 -> loss before: 0.38864900379093253, loss after: 0.38915452741069795]
[epoch 601/1000, batch 21/100 -> loss before: 0.3059857543393524, loss after: 0.30283378350237783]
[epoch 601/1000, batch 31/100 -> loss before: 0.16893882609991442, loss after: 0.17233989383775353]
[epoch 601/1000, batch 41/100 -> loss before: 0.3905522920487009, loss after: 0.37425631278112886]
[epoch 601/1000, batch 51/100 -> loss before: 0.3184797914193463, loss after: 0.31675644288255783]
[epoch 601/1000, batch 61/100 -> loss before: 0.13619451464632346, loss after: 0.13617753776438157]
[epoch 601/1000, batch 71/100 -> loss before: 0.25432534846933663, loss after: 0.24766252411000642]
[epoch 601/1000, batch 81/100 -> loss before: 0.35378713400545003, loss after: 0.34727138059495344]
[epoch 601/1000, batch 91/100 -> loss before: 0.3473207553566272, loss after: 0.34745847616041514]
ENDING EPOCH 601/1000 [loss before: 0.2835827916626348, loss after: 0.28893237787154974; epoch time: 0.09094047546386719 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.14572176426339786, loss after: 0.14470552317444751]
[epoch 701/1000, batch 11/100 -> loss before: 0.16196872322475936, loss after: 0.1528658716339391]
[epoch 701/1000, batch 21/100 -> loss before: 0.22092156000938273, loss after: 0.2209209449134088]
[epoch 701/1000, batch 31/100 -> loss before: 0.2039116078890144, loss after: 0.2045369422262072]
[epoch 701/1000, batch 41/100 -> loss before: 0.1999846133083792, loss after: 0.2018984122359638]
[epoch 701/1000, batch 51/100 -> loss before: 0.5111442510747, loss after: 0.5171293404270239]
[epoch 701/1000, batch 61/100 -> loss before: 0.13511684193441484, loss after: 0.13483564072120977]
[epoch 701/1000, batch 71/100 -> loss before: 0.3675532018780604, loss after: 0.3673898943787056]
[epoch 701/1000, batch 81/100 -> loss before: 0.2840987216153678, loss after: 0.2848526500741686]
[epoch 701/1000, batch 91/100 -> loss before: 0.3544745079347944, loss after: 0.35406960340597693]
ENDING EPOCH 701/1000 [loss before: 0.2837827011291223, loss after: 0.2836521709853269; epoch time: 0.09264922142028809 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.2677266982362914, loss after: 0.25539441846223965]
[epoch 801/1000, batch 11/100 -> loss before: 0.11585354174027451, loss after: 0.11589513336735915]
[epoch 801/1000, batch 21/100 -> loss before: 0.2353249690590738, loss after: 0.2347032967108078]
[epoch 801/1000, batch 31/100 -> loss before: 0.21454151833241047, loss after: 0.21475426958497662]
[epoch 801/1000, batch 41/100 -> loss before: 0.4900843586827922, loss after: 0.4927146912411137]
[epoch 801/1000, batch 51/100 -> loss before: 0.26333970667121215, loss after: 0.2636117136740962]
[epoch 801/1000, batch 61/100 -> loss before: 0.2696811057916074, loss after: 0.2685737561516692]
[epoch 801/1000, batch 71/100 -> loss before: 0.3251394305622817, loss after: 0.32577388307502025]
[epoch 801/1000, batch 81/100 -> loss before: 0.29902428778368434, loss after: 0.2986492427980715]
[epoch 801/1000, batch 91/100 -> loss before: 0.2480139261196678, loss after: 0.24652111920738337]
ENDING EPOCH 801/1000 [loss before: 0.2838998689969228, loss after: 0.2831529465318291; epoch time: 0.12943696975708008 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.13162540286712027, loss after: 0.12974236807259892]
[epoch 901/1000, batch 11/100 -> loss before: 0.3626115275416096, loss after: 0.3580594293779177]
[epoch 901/1000, batch 21/100 -> loss before: 0.2534315207763817, loss after: 0.2523649103380999]
[epoch 901/1000, batch 31/100 -> loss before: 0.2804941417300769, loss after: 0.27855782368803234]
[epoch 901/1000, batch 41/100 -> loss before: 0.3404530858214468, loss after: 0.34065190323517586]
[epoch 901/1000, batch 51/100 -> loss before: 0.268986761154971, loss after: 0.2695313620064096]
[epoch 901/1000, batch 61/100 -> loss before: 0.4148716882625254, loss after: 0.4154684746934113]
[epoch 901/1000, batch 71/100 -> loss before: 0.18326992527119274, loss after: 0.17666141694071943]
[epoch 901/1000, batch 81/100 -> loss before: 0.37248614033848615, loss after: 0.3725235091430033]
[epoch 901/1000, batch 91/100 -> loss before: 0.20611352917648076, loss after: 0.2037882534095194]
ENDING EPOCH 901/1000 [loss before: 0.28350373857302125, loss after: 0.29359646803810713; epoch time: 0.09291410446166992 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.2676133099472592, loss after: 0.26820747941341294]
[epoch 1000/1000, batch 11/100 -> loss before: 0.2291566066398126, loss after: 0.22912224811233659]
[epoch 1000/1000, batch 21/100 -> loss before: 0.32037085408076404, loss after: 0.32033594526105935]
[epoch 1000/1000, batch 31/100 -> loss before: 0.2362515223412009, loss after: 0.23711522278590436]
[epoch 1000/1000, batch 41/100 -> loss before: 0.3063785229525101, loss after: 0.3021493713896649]
[epoch 1000/1000, batch 51/100 -> loss before: 0.3110848788700079, loss after: 0.31160141353093374]
[epoch 1000/1000, batch 61/100 -> loss before: 0.20490136610667534, loss after: 0.20671454329365715]
[epoch 1000/1000, batch 71/100 -> loss before: 0.35258151081250205, loss after: 0.35146204922637253]
[epoch 1000/1000, batch 81/100 -> loss before: 0.10233052315095684, loss after: 0.1025790760541622]
[epoch 1000/1000, batch 91/100 -> loss before: 0.33912954917723614, loss after: 0.33938388844346373]
ENDING EPOCH 1000/1000 [loss before: 0.2843135032776458, loss after: 0.2831505232111532; epoch time: 0.09508848190307617 s]
FIT DONE. [time: 88.79919147491455 s]
LOSS TRAIN (MSE): 0.2831505232111532
LOSS TEST (MSE): 0.2775473045939466
R^2 TRAIN: -0.0002555832061825747
R^2 TEST: -6.103765208398393e-05
EXPERIMENT DONE
