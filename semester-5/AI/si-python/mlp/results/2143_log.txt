EXPERIMENT 2143 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[64, 64, 64, 64, 64, 32, 32, 32, 32, 32, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=adam, learning_rate=0.001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 25185]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.6815884323265685, loss after: 0.6719625094180314]
[epoch 1/1000, batch 11/100 -> loss before: 0.3919453306776703, loss after: 0.38658009936966914]
[epoch 1/1000, batch 21/100 -> loss before: 0.5973251325026383, loss after: 0.5901132796904045]
[epoch 1/1000, batch 31/100 -> loss before: 0.446393314981405, loss after: 0.4424898463592758]
[epoch 1/1000, batch 41/100 -> loss before: 0.3435953402493167, loss after: 0.3385348188151619]
[epoch 1/1000, batch 51/100 -> loss before: 0.3610892559065375, loss after: 0.3573610077105087]
[epoch 1/1000, batch 61/100 -> loss before: 0.48937283773457263, loss after: 0.4882440906042955]
[epoch 1/1000, batch 71/100 -> loss before: 0.3719978539800303, loss after: 0.3706365007714728]
[epoch 1/1000, batch 81/100 -> loss before: 0.41846025040614865, loss after: 0.41765795452354926]
[epoch 1/1000, batch 91/100 -> loss before: 0.2914216726301492, loss after: 0.29149384801654415]
ENDING EPOCH 1/1000 [loss before: 0.599648552700766, loss after: 0.28576762435650344; epoch time: 0.16096878051757812 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.28492453015289554, loss after: 0.2852080929738058]
[epoch 101/1000, batch 11/100 -> loss before: 0.22018168010341319, loss after: 0.22009026414612315]
[epoch 101/1000, batch 21/100 -> loss before: 0.2455020589555267, loss after: 0.2451736651291462]
[epoch 101/1000, batch 31/100 -> loss before: 0.3878737271730009, loss after: 0.3879814946610289]
[epoch 101/1000, batch 41/100 -> loss before: 0.6558525951276986, loss after: 0.6544886368597888]
[epoch 101/1000, batch 51/100 -> loss before: 0.26088085952421725, loss after: 0.2593936648453961]
[epoch 101/1000, batch 61/100 -> loss before: 0.5176449088093312, loss after: 0.5175219334577059]
[epoch 101/1000, batch 71/100 -> loss before: 0.1464670377619699, loss after: 0.1462101241633103]
[epoch 101/1000, batch 81/100 -> loss before: 0.26538316365671155, loss after: 0.2652119802839995]
[epoch 101/1000, batch 91/100 -> loss before: 0.24501435400187885, loss after: 0.2442990911634019]
ENDING EPOCH 101/1000 [loss before: 0.28329197515222154, loss after: 0.2830784509063215; epoch time: 0.1646420955657959 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.08806934054439759, loss after: 0.08806471035715616]
[epoch 201/1000, batch 11/100 -> loss before: 0.357457918604973, loss after: 0.3571470614601818]
[epoch 201/1000, batch 21/100 -> loss before: 0.18537078490437803, loss after: 0.18426078345039518]
[epoch 201/1000, batch 31/100 -> loss before: 0.4653050985004577, loss after: 0.46511460178368547]
[epoch 201/1000, batch 41/100 -> loss before: 0.31358902804157024, loss after: 0.3143131576851128]
[epoch 201/1000, batch 51/100 -> loss before: 0.28539138676945675, loss after: 0.28538430034794204]
[epoch 201/1000, batch 61/100 -> loss before: 0.48570591870922897, loss after: 0.48601199246579985]
[epoch 201/1000, batch 71/100 -> loss before: 0.41287146541817543, loss after: 0.41248773931528737]
[epoch 201/1000, batch 81/100 -> loss before: 0.2004597462069047, loss after: 0.20023739774685523]
[epoch 201/1000, batch 91/100 -> loss before: 0.10915163026579626, loss after: 0.10913360875663622]
ENDING EPOCH 201/1000 [loss before: 0.28313900110010054, loss after: 0.2830989777100323; epoch time: 0.1665487289428711 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.1888602875922694, loss after: 0.18894235839832466]
[epoch 301/1000, batch 11/100 -> loss before: 0.2000490226963199, loss after: 0.2000429674293139]
[epoch 301/1000, batch 21/100 -> loss before: 0.3099393131363634, loss after: 0.3098487017559356]
[epoch 301/1000, batch 31/100 -> loss before: 0.3218712495717516, loss after: 0.3220757754608582]
[epoch 301/1000, batch 41/100 -> loss before: 0.2613645063582476, loss after: 0.2607895933846904]
[epoch 301/1000, batch 51/100 -> loss before: 0.2830700561350282, loss after: 0.2830066322298494]
[epoch 301/1000, batch 61/100 -> loss before: 0.314981174243757, loss after: 0.31443762904276085]
[epoch 301/1000, batch 71/100 -> loss before: 0.30855603553903366, loss after: 0.30850664091346053]
[epoch 301/1000, batch 81/100 -> loss before: 0.3225360773444752, loss after: 0.32258074995135094]
[epoch 301/1000, batch 91/100 -> loss before: 0.3185880729609885, loss after: 0.3179562908945358]
ENDING EPOCH 301/1000 [loss before: 0.28308361403178617, loss after: 0.2830815317498309; epoch time: 0.17012381553649902 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.2840818096828519, loss after: 0.28432701072481376]
[epoch 401/1000, batch 11/100 -> loss before: 0.2745776507748441, loss after: 0.27454120922700975]
[epoch 401/1000, batch 21/100 -> loss before: 0.3332550645542469, loss after: 0.33254060619764997]
[epoch 401/1000, batch 31/100 -> loss before: 0.3683807900858866, loss after: 0.368176804131279]
[epoch 401/1000, batch 41/100 -> loss before: 0.23513265448010312, loss after: 0.23512013348208902]
[epoch 401/1000, batch 51/100 -> loss before: 0.3256873131594775, loss after: 0.32560359622283086]
[epoch 401/1000, batch 61/100 -> loss before: 0.2425080454358503, loss after: 0.24307715726275383]
[epoch 401/1000, batch 71/100 -> loss before: 0.2649608438974384, loss after: 0.2650464371801212]
[epoch 401/1000, batch 81/100 -> loss before: 0.19138701849207201, loss after: 0.19184514189771057]
[epoch 401/1000, batch 91/100 -> loss before: 0.19657077431318765, loss after: 0.1964613641659197]
ENDING EPOCH 401/1000 [loss before: 0.2830810664843106, loss after: 0.283810592910815; epoch time: 0.1618967056274414 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.2872437595092171, loss after: 0.28684334323795047]
[epoch 501/1000, batch 11/100 -> loss before: 0.5805595531190492, loss after: 0.580184526053781]
[epoch 501/1000, batch 21/100 -> loss before: 0.37942414345025116, loss after: 0.379426139275039]
[epoch 501/1000, batch 31/100 -> loss before: 0.24483831839503406, loss after: 0.24481317443674072]
[epoch 501/1000, batch 41/100 -> loss before: 0.31505405020282257, loss after: 0.31507070776111457]
[epoch 501/1000, batch 51/100 -> loss before: 0.17706099283267301, loss after: 0.17704214085183462]
[epoch 501/1000, batch 61/100 -> loss before: 0.2897390777467731, loss after: 0.2897307914721158]
[epoch 501/1000, batch 71/100 -> loss before: 0.14747846704537154, loss after: 0.14747259205462626]
[epoch 501/1000, batch 81/100 -> loss before: 0.24259114595966066, loss after: 0.24229691931546132]
[epoch 501/1000, batch 91/100 -> loss before: 0.21339798394760331, loss after: 0.21342959594460256]
ENDING EPOCH 501/1000 [loss before: 0.28315599884045617, loss after: 0.2830835542060551; epoch time: 0.16890239715576172 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.3428706072836185, loss after: 0.3431066330785749]
[epoch 601/1000, batch 11/100 -> loss before: 0.3479066266571944, loss after: 0.348606407368371]
[epoch 601/1000, batch 21/100 -> loss before: 0.3003403660378174, loss after: 0.3002598283249481]
[epoch 601/1000, batch 31/100 -> loss before: 0.13144977697741972, loss after: 0.13192645180404172]
[epoch 601/1000, batch 41/100 -> loss before: 0.37349445897941036, loss after: 0.3719843274674658]
[epoch 601/1000, batch 51/100 -> loss before: 0.2871581112740438, loss after: 0.2871208981323613]
[epoch 601/1000, batch 61/100 -> loss before: 0.136908299363584, loss after: 0.13683060045870085]
[epoch 601/1000, batch 71/100 -> loss before: 0.2743645995470604, loss after: 0.27297784983521833]
[epoch 601/1000, batch 81/100 -> loss before: 0.3424770962109963, loss after: 0.3418179525339949]
[epoch 601/1000, batch 91/100 -> loss before: 0.3472967868036792, loss after: 0.3472946179159801]
ENDING EPOCH 601/1000 [loss before: 0.28318489728010593, loss after: 0.28325399009864644; epoch time: 0.1643049716949463 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.13837034059650616, loss after: 0.1383648964745125]
[epoch 701/1000, batch 11/100 -> loss before: 0.17875402718422495, loss after: 0.17650695694379134]
[epoch 701/1000, batch 21/100 -> loss before: 0.2289861070155322, loss after: 0.2286317654926277]
[epoch 701/1000, batch 31/100 -> loss before: 0.20493393851588398, loss after: 0.20491015629515985]
[epoch 701/1000, batch 41/100 -> loss before: 0.19948144481569602, loss after: 0.19905444815188203]
[epoch 701/1000, batch 51/100 -> loss before: 0.5419399778399188, loss after: 0.5412063030758931]
[epoch 701/1000, batch 61/100 -> loss before: 0.13687726146784956, loss after: 0.13693152666776404]
[epoch 701/1000, batch 71/100 -> loss before: 0.36810161751842335, loss after: 0.36811762295343126]
[epoch 701/1000, batch 81/100 -> loss before: 0.2837359149130201, loss after: 0.28365480280792127]
[epoch 701/1000, batch 91/100 -> loss before: 0.3533587206659337, loss after: 0.3533573808407534]
ENDING EPOCH 701/1000 [loss before: 0.283143742560386, loss after: 0.283421019080518; epoch time: 0.16063284873962402 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.247552670132348, loss after: 0.2463905182556882]
[epoch 801/1000, batch 11/100 -> loss before: 0.11732840057145388, loss after: 0.11734749309802432]
[epoch 801/1000, batch 21/100 -> loss before: 0.24162602444468625, loss after: 0.2413535431416697]
[epoch 801/1000, batch 31/100 -> loss before: 0.21458859004336633, loss after: 0.2145151588551799]
[epoch 801/1000, batch 41/100 -> loss before: 0.4664828938695706, loss after: 0.46695287058414825]
[epoch 801/1000, batch 51/100 -> loss before: 0.266924975221348, loss after: 0.266790404640744]
[epoch 801/1000, batch 61/100 -> loss before: 0.2713427344563312, loss after: 0.27094108329982425]
[epoch 801/1000, batch 71/100 -> loss before: 0.32190538854492157, loss after: 0.32196211038955314]
[epoch 801/1000, batch 81/100 -> loss before: 0.29021834197006013, loss after: 0.2903079153909314]
[epoch 801/1000, batch 91/100 -> loss before: 0.24545722672065518, loss after: 0.24532610012229378]
ENDING EPOCH 801/1000 [loss before: 0.28307925605996254, loss after: 0.2830828545505965; epoch time: 0.17640423774719238 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.1361543987840725, loss after: 0.13580309895663573]
[epoch 901/1000, batch 11/100 -> loss before: 0.33127758285277287, loss after: 0.3312115057126794]
[epoch 901/1000, batch 21/100 -> loss before: 0.25163655387666173, loss after: 0.2512023661662834]
[epoch 901/1000, batch 31/100 -> loss before: 0.2863754256862953, loss after: 0.28588816339143264]
[epoch 901/1000, batch 41/100 -> loss before: 0.33883337342244507, loss after: 0.3386931404177478]
[epoch 901/1000, batch 51/100 -> loss before: 0.23380045393957047, loss after: 0.23415362525945235]
[epoch 901/1000, batch 61/100 -> loss before: 0.4075571664064278, loss after: 0.4075607489925742]
[epoch 901/1000, batch 71/100 -> loss before: 0.18024701863140902, loss after: 0.1797405529547927]
[epoch 901/1000, batch 81/100 -> loss before: 0.390952169608538, loss after: 0.39066302256105945]
[epoch 901/1000, batch 91/100 -> loss before: 0.21135940776479586, loss after: 0.21100348284645962]
ENDING EPOCH 901/1000 [loss before: 0.2830907348759096, loss after: 0.28317998211759904; epoch time: 0.16381406784057617 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.2551907311270133, loss after: 0.25545880730987974]
[epoch 1000/1000, batch 11/100 -> loss before: 0.2315995028457269, loss after: 0.2314597015270937]
[epoch 1000/1000, batch 21/100 -> loss before: 0.31446082590696794, loss after: 0.31461966020626425]
[epoch 1000/1000, batch 31/100 -> loss before: 0.23539325326802346, loss after: 0.2354665655764979]
[epoch 1000/1000, batch 41/100 -> loss before: 0.29512012020688894, loss after: 0.2947441392779019]
[epoch 1000/1000, batch 51/100 -> loss before: 0.2992953050919396, loss after: 0.2993789265297391]
[epoch 1000/1000, batch 61/100 -> loss before: 0.20843005780763205, loss after: 0.20856918172817746]
[epoch 1000/1000, batch 71/100 -> loss before: 0.3490629596646319, loss after: 0.3489387556073291]
[epoch 1000/1000, batch 81/100 -> loss before: 0.10224460313571124, loss after: 0.10221377083235259]
[epoch 1000/1000, batch 91/100 -> loss before: 0.35523314725796923, loss after: 0.35488946565132967]
ENDING EPOCH 1000/1000 [loss before: 0.283080097628228, loss after: 0.2832408458734587; epoch time: 0.1637263298034668 s]
FIT DONE. [time: 157.56493854522705 s]
LOSS TRAIN (MSE): 0.2832408458734587
LOSS TEST (MSE): 0.2775303823853229
R^2 TRAIN: -0.0005746564193858106
R^2 TEST: -6.339207336836239e-08
EXPERIMENT DONE
