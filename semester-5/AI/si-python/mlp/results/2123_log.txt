EXPERIMENT 2123 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[64, 64, 64, 64, 64, 32, 32, 32, 32, 32, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=sgd_momentum, learning_rate=0.001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 25185]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.6815884323265685, loss after: 0.6745513997557354]
[epoch 1/1000, batch 11/100 -> loss before: 0.3330737819220274, loss after: 0.3227578655703077]
[epoch 1/1000, batch 21/100 -> loss before: 0.41199808960031453, loss after: 0.404028339672279]
[epoch 1/1000, batch 31/100 -> loss before: 0.3712732701154322, loss after: 0.3720199468128486]
[epoch 1/1000, batch 41/100 -> loss before: 0.2039476409187173, loss after: 0.20359493809573506]
[epoch 1/1000, batch 51/100 -> loss before: 0.24370592132492871, loss after: 0.24435839047768865]
[epoch 1/1000, batch 61/100 -> loss before: 0.46775892894830723, loss after: 0.4675924556430752]
[epoch 1/1000, batch 71/100 -> loss before: 0.343518972026403, loss after: 0.3436287962794847]
[epoch 1/1000, batch 81/100 -> loss before: 0.4037903585237893, loss after: 0.4037970367218711]
[epoch 1/1000, batch 91/100 -> loss before: 0.29420916908050465, loss after: 0.29407090052395174]
ENDING EPOCH 1/1000 [loss before: 0.599648552700766, loss after: 0.28496757693093444; epoch time: 0.10355162620544434 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.2855792976604996, loss after: 0.28609045053458987]
[epoch 101/1000, batch 11/100 -> loss before: 0.21975299816245686, loss after: 0.2197577093390671]
[epoch 101/1000, batch 21/100 -> loss before: 0.24719811254204993, loss after: 0.24651117333204567]
[epoch 101/1000, batch 31/100 -> loss before: 0.38929744346238676, loss after: 0.3895436953715176]
[epoch 101/1000, batch 41/100 -> loss before: 0.6613496198987793, loss after: 0.659214486580288]
[epoch 101/1000, batch 51/100 -> loss before: 0.2584953961017359, loss after: 0.25638990701887926]
[epoch 101/1000, batch 61/100 -> loss before: 0.5185530528068392, loss after: 0.5183055133296676]
[epoch 101/1000, batch 71/100 -> loss before: 0.1459365963481036, loss after: 0.14560426809280716]
[epoch 101/1000, batch 81/100 -> loss before: 0.26415866768235485, loss after: 0.26399419396993196]
[epoch 101/1000, batch 91/100 -> loss before: 0.24494397780772426, loss after: 0.24379142068412216]
ENDING EPOCH 101/1000 [loss before: 0.2831415770334421, loss after: 0.2830784764636861; epoch time: 0.1068572998046875 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.088078240192311, loss after: 0.08810537505658718]
[epoch 201/1000, batch 11/100 -> loss before: 0.35552834089220325, loss after: 0.3551747591323917]
[epoch 201/1000, batch 21/100 -> loss before: 0.1793719657353873, loss after: 0.1780975282046602]
[epoch 201/1000, batch 31/100 -> loss before: 0.4673024783364842, loss after: 0.46677250805543247]
[epoch 201/1000, batch 41/100 -> loss before: 0.31634419112036943, loss after: 0.3175627784229321]
[epoch 201/1000, batch 51/100 -> loss before: 0.28735309012196253, loss after: 0.2872570435116007]
[epoch 201/1000, batch 61/100 -> loss before: 0.48799188757761164, loss after: 0.488323843082391]
[epoch 201/1000, batch 71/100 -> loss before: 0.4115375204278228, loss after: 0.41111274893827654]
[epoch 201/1000, batch 81/100 -> loss before: 0.20068234784398253, loss after: 0.2002835265069814]
[epoch 201/1000, batch 91/100 -> loss before: 0.10898311238165778, loss after: 0.10897696692156486]
ENDING EPOCH 201/1000 [loss before: 0.2830818233265317, loss after: 0.2833494158807549; epoch time: 0.10454916954040527 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.18903056874270877, loss after: 0.189120564915205]
[epoch 301/1000, batch 11/100 -> loss before: 0.19980943920076005, loss after: 0.1998266105394612]
[epoch 301/1000, batch 21/100 -> loss before: 0.3107742746207939, loss after: 0.3105595792008722]
[epoch 301/1000, batch 31/100 -> loss before: 0.3244562076796339, loss after: 0.32478853836373156]
[epoch 301/1000, batch 41/100 -> loss before: 0.26174333592013205, loss after: 0.26076598273886065]
[epoch 301/1000, batch 51/100 -> loss before: 0.28335945657776995, loss after: 0.28323119293358084]
[epoch 301/1000, batch 61/100 -> loss before: 0.3148704321760655, loss after: 0.3139569515301587]
[epoch 301/1000, batch 71/100 -> loss before: 0.3081876528669475, loss after: 0.3081236333704017]
[epoch 301/1000, batch 81/100 -> loss before: 0.32289768434429456, loss after: 0.3229916108971229]
[epoch 301/1000, batch 91/100 -> loss before: 0.32369847229342164, loss after: 0.32237719121822983]
ENDING EPOCH 301/1000 [loss before: 0.28309492798642133, loss after: 0.28309154309176676; epoch time: 0.10421156883239746 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.2840117227909598, loss after: 0.28441770342129435]
[epoch 401/1000, batch 11/100 -> loss before: 0.27393838640850054, loss after: 0.27390617289871805]
[epoch 401/1000, batch 21/100 -> loss before: 0.32933826048751663, loss after: 0.32842995128337804]
[epoch 401/1000, batch 31/100 -> loss before: 0.37450378008640645, loss after: 0.37432535763116903]
[epoch 401/1000, batch 41/100 -> loss before: 0.23512007755023384, loss after: 0.23514865283905864]
[epoch 401/1000, batch 51/100 -> loss before: 0.3244058651233534, loss after: 0.32435530962670284]
[epoch 401/1000, batch 61/100 -> loss before: 0.23889591247630887, loss after: 0.2401743698743753]
[epoch 401/1000, batch 71/100 -> loss before: 0.265416821532697, loss after: 0.2656907139150378]
[epoch 401/1000, batch 81/100 -> loss before: 0.19871480694213153, loss after: 0.19943040048209246]
[epoch 401/1000, batch 91/100 -> loss before: 0.20061431333457133, loss after: 0.20006181097130687]
ENDING EPOCH 401/1000 [loss before: 0.28307928970426544, loss after: 0.2844095882976841; epoch time: 0.10477519035339355 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.28516159277187864, loss after: 0.2845736340356114]
[epoch 501/1000, batch 11/100 -> loss before: 0.5786298771356125, loss after: 0.5781291792284162]
[epoch 501/1000, batch 21/100 -> loss before: 0.37847954874073764, loss after: 0.37854130730676633]
[epoch 501/1000, batch 31/100 -> loss before: 0.24460169250266106, loss after: 0.24450597887470696]
[epoch 501/1000, batch 41/100 -> loss before: 0.31494110779281986, loss after: 0.31496594990670906]
[epoch 501/1000, batch 51/100 -> loss before: 0.17693982583321502, loss after: 0.17690925376183175]
[epoch 501/1000, batch 61/100 -> loss before: 0.2897689468213728, loss after: 0.2897507829512119]
[epoch 501/1000, batch 71/100 -> loss before: 0.147505989590554, loss after: 0.14753902165148453]
[epoch 501/1000, batch 81/100 -> loss before: 0.24595277060534376, loss after: 0.24522508934726145]
[epoch 501/1000, batch 91/100 -> loss before: 0.21336119682607646, loss after: 0.21342885697576747]
ENDING EPOCH 501/1000 [loss before: 0.28308017953332726, loss after: 0.28324801020614904; epoch time: 0.10319018363952637 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.34410717367560695, loss after: 0.3445744224856065]
[epoch 601/1000, batch 11/100 -> loss before: 0.35673548422705825, loss after: 0.35781923971577706]
[epoch 601/1000, batch 21/100 -> loss before: 0.3050053942114932, loss after: 0.3045083876242544]
[epoch 601/1000, batch 31/100 -> loss before: 0.13248705673651476, loss after: 0.13354183902827577]
[epoch 601/1000, batch 41/100 -> loss before: 0.3799608213299148, loss after: 0.377072516650893]
[epoch 601/1000, batch 51/100 -> loss before: 0.2869338002467016, loss after: 0.2869450378604338]
[epoch 601/1000, batch 61/100 -> loss before: 0.136815371919289, loss after: 0.13668763427858358]
[epoch 601/1000, batch 71/100 -> loss before: 0.2735527110753884, loss after: 0.2711247505262018]
[epoch 601/1000, batch 81/100 -> loss before: 0.3440388102626937, loss after: 0.34277973151968333]
[epoch 601/1000, batch 91/100 -> loss before: 0.3473003774893265, loss after: 0.3473211718521729]
ENDING EPOCH 601/1000 [loss before: 0.28307826382396556, loss after: 0.28394006053770443; epoch time: 0.1076204776763916 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.13814513166214853, loss after: 0.13820127433583507]
[epoch 701/1000, batch 11/100 -> loss before: 0.17448273980474616, loss after: 0.17048407749042963]
[epoch 701/1000, batch 21/100 -> loss before: 0.22507973917718394, loss after: 0.22472795850049865]
[epoch 701/1000, batch 31/100 -> loss before: 0.20333895590605908, loss after: 0.20338419154548043]
[epoch 701/1000, batch 41/100 -> loss before: 0.19845447094625354, loss after: 0.1977372071930491]
[epoch 701/1000, batch 51/100 -> loss before: 0.5296419330231238, loss after: 0.528725444474201]
[epoch 701/1000, batch 61/100 -> loss before: 0.1395714049550842, loss after: 0.13953737926575993]
[epoch 701/1000, batch 71/100 -> loss before: 0.36844396687522585, loss after: 0.36842433232285543]
[epoch 701/1000, batch 81/100 -> loss before: 0.2833925381473715, loss after: 0.2833328839910257]
[epoch 701/1000, batch 91/100 -> loss before: 0.3533774830521381, loss after: 0.3533732141664748]
ENDING EPOCH 701/1000 [loss before: 0.28316606172395564, loss after: 0.28339147040516; epoch time: 0.10959315299987793 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.2518490671011159, loss after: 0.24941501820394513]
[epoch 801/1000, batch 11/100 -> loss before: 0.1174246693246358, loss after: 0.1174481908063453]
[epoch 801/1000, batch 21/100 -> loss before: 0.2395955537643868, loss after: 0.2392005181595156]
[epoch 801/1000, batch 31/100 -> loss before: 0.21362157547758595, loss after: 0.21356975260972616]
[epoch 801/1000, batch 41/100 -> loss before: 0.46862100393458805, loss after: 0.46962266463179764]
[epoch 801/1000, batch 51/100 -> loss before: 0.2653524830888464, loss after: 0.26516998551542964]
[epoch 801/1000, batch 61/100 -> loss before: 0.26788907562032893, loss after: 0.2673469838134258]
[epoch 801/1000, batch 71/100 -> loss before: 0.32087175294526793, loss after: 0.3210696118403841]
[epoch 801/1000, batch 81/100 -> loss before: 0.2907051575363703, loss after: 0.29096567786763644]
[epoch 801/1000, batch 91/100 -> loss before: 0.24652693951889035, loss after: 0.24624388700071095]
ENDING EPOCH 801/1000 [loss before: 0.28312862193806776, loss after: 0.2830848375883302; epoch time: 0.10815191268920898 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.13469177632873633, loss after: 0.13411911018060566]
[epoch 901/1000, batch 11/100 -> loss before: 0.3382245740778465, loss after: 0.3377924049403226]
[epoch 901/1000, batch 21/100 -> loss before: 0.24850666688439071, loss after: 0.24793524939306827]
[epoch 901/1000, batch 31/100 -> loss before: 0.28279695455054527, loss after: 0.2821137030081353]
[epoch 901/1000, batch 41/100 -> loss before: 0.33646586499611403, loss after: 0.33643585675345333]
[epoch 901/1000, batch 51/100 -> loss before: 0.23745993136557192, loss after: 0.23829417519716617]
[epoch 901/1000, batch 61/100 -> loss before: 0.40802308405183363, loss after: 0.40820005912095303]
[epoch 901/1000, batch 71/100 -> loss before: 0.18766235983756344, loss after: 0.18613709349397603]
[epoch 901/1000, batch 81/100 -> loss before: 0.3900971743807188, loss after: 0.3893983777233717]
[epoch 901/1000, batch 91/100 -> loss before: 0.21123454421199822, loss after: 0.21056863104127088]
ENDING EPOCH 901/1000 [loss before: 0.2831560819020667, loss after: 0.2835379904573282; epoch time: 0.10294413566589355 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.257555597896415, loss after: 0.25807949039298483]
[epoch 1000/1000, batch 11/100 -> loss before: 0.22996784427820396, loss after: 0.22979116962617022]
[epoch 1000/1000, batch 21/100 -> loss before: 0.3187708895239924, loss after: 0.3188921819840185]
[epoch 1000/1000, batch 31/100 -> loss before: 0.235039893970778, loss after: 0.2351953549121569]
[epoch 1000/1000, batch 41/100 -> loss before: 0.2958733114899461, loss after: 0.2952291982040425]
[epoch 1000/1000, batch 51/100 -> loss before: 0.3002900346269878, loss after: 0.3004218232305999]
[epoch 1000/1000, batch 61/100 -> loss before: 0.2076812421205152, loss after: 0.2080046039634554]
[epoch 1000/1000, batch 71/100 -> loss before: 0.34927403295984505, loss after: 0.349031167490137]
[epoch 1000/1000, batch 81/100 -> loss before: 0.10215473604758607, loss after: 0.10212330379393202]
[epoch 1000/1000, batch 91/100 -> loss before: 0.34980879594749387, loss after: 0.3493882195606165]
ENDING EPOCH 1000/1000 [loss before: 0.2831167946488104, loss after: 0.28384052216698197; epoch time: 0.10185718536376953 s]
FIT DONE. [time: 93.93447780609131 s]
LOSS TRAIN (MSE): 0.28384052216698197
LOSS TEST (MSE): 0.2777550345219663
R^2 TRAIN: -0.0026930687532584763
R^2 TEST: -0.0008095320670116202
EXPERIMENT DONE
