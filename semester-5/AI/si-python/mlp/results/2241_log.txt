EXPERIMENT 2241 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[128, 64, 32], activation_name=relu, targets_activation_name=linear, 
initialization_name=uniform, algo_name=adam, learning_rate=0.001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 10753]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 20.679362556386955, loss after: 16.12292213330678]
[epoch 1/1000, batch 11/100 -> loss before: 0.9178890569722913, loss after: 1.2214009142552467]
[epoch 1/1000, batch 21/100 -> loss before: 0.36305520594601315, loss after: 0.29061541408871633]
[epoch 1/1000, batch 31/100 -> loss before: 0.819357521214908, loss after: 0.7730231185286447]
[epoch 1/1000, batch 41/100 -> loss before: 0.1595688334374092, loss after: 0.15914433794029376]
[epoch 1/1000, batch 51/100 -> loss before: 0.49199704095968244, loss after: 0.46923450411532697]
[epoch 1/1000, batch 61/100 -> loss before: 0.4933217629146366, loss after: 0.4827294390160531]
[epoch 1/1000, batch 71/100 -> loss before: 0.44459457397117763, loss after: 0.42674845176360554]
[epoch 1/1000, batch 81/100 -> loss before: 0.10089381905638661, loss after: 0.0994681009477018]
[epoch 1/1000, batch 91/100 -> loss before: 0.3906635655680817, loss after: 0.3823862200957602]
ENDING EPOCH 1/1000 [loss before: 18.773492598911375, loss after: 0.27844996875502476; epoch time: 0.04605507850646973 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.14341701131960244, loss after: 0.14241177542843358]
[epoch 101/1000, batch 11/100 -> loss before: 0.04545145534566381, loss after: 0.039464555505981755]
[epoch 101/1000, batch 21/100 -> loss before: 0.1405764731486323, loss after: 0.13086684091890982]
[epoch 101/1000, batch 31/100 -> loss before: 0.056388059024816715, loss after: 0.05591399730447275]
[epoch 101/1000, batch 41/100 -> loss before: 0.37951392241783555, loss after: 0.3773767467205915]
[epoch 101/1000, batch 51/100 -> loss before: 0.06666185073156491, loss after: 0.06748387751138042]
[epoch 101/1000, batch 61/100 -> loss before: 0.11332975198477357, loss after: 0.11321530652931173]
[epoch 101/1000, batch 71/100 -> loss before: 0.2232726271382003, loss after: 0.22040073038486274]
[epoch 101/1000, batch 81/100 -> loss before: 0.2595616407061029, loss after: 0.2519977213204323]
[epoch 101/1000, batch 91/100 -> loss before: 0.07609470541617593, loss after: 0.06743017106609225]
ENDING EPOCH 101/1000 [loss before: 0.1551431699082787, loss after: 0.15118456933524652; epoch time: 0.04205155372619629 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.05157353124880606, loss after: 0.049888095361341256]
[epoch 201/1000, batch 11/100 -> loss before: 0.13407682977750351, loss after: 0.12596822214323147]
[epoch 201/1000, batch 21/100 -> loss before: 0.2637599547982105, loss after: 0.2422820972370583]
[epoch 201/1000, batch 31/100 -> loss before: 0.06919885654916894, loss after: 0.0658997254284574]
[epoch 201/1000, batch 41/100 -> loss before: 0.19735762964213693, loss after: 0.19570143118450636]
[epoch 201/1000, batch 51/100 -> loss before: 0.13543341840582776, loss after: 0.1275490176172933]
[epoch 201/1000, batch 61/100 -> loss before: 0.06965954300764664, loss after: 0.07213514526799655]
[epoch 201/1000, batch 71/100 -> loss before: 0.1395191549365377, loss after: 0.13797285760981648]
[epoch 201/1000, batch 81/100 -> loss before: 0.07070019889091646, loss after: 0.07228457230433728]
[epoch 201/1000, batch 91/100 -> loss before: 0.13246929171215246, loss after: 0.11490979109658947]
ENDING EPOCH 201/1000 [loss before: 0.09133617768562598, loss after: 0.09020187284480048; epoch time: 0.04532456398010254 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.005528262181942448, loss after: 0.004552849015735293]
[epoch 301/1000, batch 11/100 -> loss before: 0.06732279059800708, loss after: 0.06709330647109651]
[epoch 301/1000, batch 21/100 -> loss before: 0.05031532247249867, loss after: 0.047498155444026545]
[epoch 301/1000, batch 31/100 -> loss before: 0.0669720416680214, loss after: 0.06273273226948321]
[epoch 301/1000, batch 41/100 -> loss before: 0.05338673658112865, loss after: 0.04955292817774987]
[epoch 301/1000, batch 51/100 -> loss before: 0.0877636343118073, loss after: 0.07629818927722863]
[epoch 301/1000, batch 61/100 -> loss before: 0.09437055643469736, loss after: 0.09464175199470547]
[epoch 301/1000, batch 71/100 -> loss before: 0.08739321202640979, loss after: 0.08170887807262692]
[epoch 301/1000, batch 81/100 -> loss before: 0.08819557875525254, loss after: 0.08303907203703782]
[epoch 301/1000, batch 91/100 -> loss before: 0.024436348645562753, loss after: 0.02340774019159648]
ENDING EPOCH 301/1000 [loss before: 0.06934660699002589, loss after: 0.07761996524829465; epoch time: 0.05257725715637207 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.006177385001144689, loss after: 0.004142019953035732]
[epoch 401/1000, batch 11/100 -> loss before: 0.11221979021497423, loss after: 0.10791090525426814]
[epoch 401/1000, batch 21/100 -> loss before: 0.04029491872418882, loss after: 0.041180740086183075]
[epoch 401/1000, batch 31/100 -> loss before: 0.02136196396272327, loss after: 0.019999739990188482]
[epoch 401/1000, batch 41/100 -> loss before: 0.08956007080143183, loss after: 0.08680474922169071]
[epoch 401/1000, batch 51/100 -> loss before: 0.154595665052503, loss after: 0.1498863459319285]
[epoch 401/1000, batch 61/100 -> loss before: 0.023803131241180865, loss after: 0.023705622258701264]
[epoch 401/1000, batch 71/100 -> loss before: 0.08730331629517077, loss after: 0.08017217353813438]
[epoch 401/1000, batch 81/100 -> loss before: 0.07353033058202338, loss after: 0.07238094544771491]
[epoch 401/1000, batch 91/100 -> loss before: 0.11426450217122805, loss after: 0.10065497325257336]
ENDING EPOCH 401/1000 [loss before: 0.05958082820487862, loss after: 0.06165549473587352; epoch time: 0.057106733322143555 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.03937623318620731, loss after: 0.02787397810665688]
[epoch 501/1000, batch 11/100 -> loss before: 0.10236212236078461, loss after: 0.09042333668701821]
[epoch 501/1000, batch 21/100 -> loss before: 0.09339174496654687, loss after: 0.09509919706335454]
[epoch 501/1000, batch 31/100 -> loss before: 0.02273267466011982, loss after: 0.021633340365360423]
[epoch 501/1000, batch 41/100 -> loss before: 0.03357694183942629, loss after: 0.03159627025279656]
[epoch 501/1000, batch 51/100 -> loss before: 0.03283011131474005, loss after: 0.032624606358416665]
[epoch 501/1000, batch 61/100 -> loss before: 0.021128192259113894, loss after: 0.019177688187614006]
[epoch 501/1000, batch 71/100 -> loss before: 0.06414782185873269, loss after: 0.041978304817815885]
[epoch 501/1000, batch 81/100 -> loss before: 0.07468381703923226, loss after: 0.06250977237832722]
[epoch 501/1000, batch 91/100 -> loss before: 0.11555396171660925, loss after: 0.1100189002985262]
ENDING EPOCH 501/1000 [loss before: 0.05630118731285748, loss after: 0.052193304545518404; epoch time: 0.05142521858215332 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.06155588348214055, loss after: 0.05052965045161828]
[epoch 601/1000, batch 11/100 -> loss before: 0.029711149344857413, loss after: 0.027047679860403306]
[epoch 601/1000, batch 21/100 -> loss before: 0.016992889408060614, loss after: 0.015422800347624083]
[epoch 601/1000, batch 31/100 -> loss before: 0.032982114411355484, loss after: 0.031904202974503526]
[epoch 601/1000, batch 41/100 -> loss before: 0.003832441248891077, loss after: 0.0044726827029915785]
[epoch 601/1000, batch 51/100 -> loss before: 0.014244677723525931, loss after: 0.011158501381836028]
[epoch 601/1000, batch 61/100 -> loss before: 0.02153648602502072, loss after: 0.020692420863439264]
[epoch 601/1000, batch 71/100 -> loss before: 0.030170242759256206, loss after: 0.02570551715018719]
[epoch 601/1000, batch 81/100 -> loss before: 0.036373101211007376, loss after: 0.02622429186679376]
[epoch 601/1000, batch 91/100 -> loss before: 0.04283368641058098, loss after: 0.042404087839306825]
ENDING EPOCH 601/1000 [loss before: 0.03163443309362458, loss after: 0.03363727435332551; epoch time: 0.04229092597961426 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.016342751676856424, loss after: 0.015751726386415844]
[epoch 701/1000, batch 11/100 -> loss before: 0.008324121289134212, loss after: 0.007182072656893272]
[epoch 701/1000, batch 21/100 -> loss before: 0.022999039876777788, loss after: 0.0187366832005671]
[epoch 701/1000, batch 31/100 -> loss before: 0.0071664154531828434, loss after: 0.006112048029966217]
[epoch 701/1000, batch 41/100 -> loss before: 0.020143252843309412, loss after: 0.019302595527905662]
[epoch 701/1000, batch 51/100 -> loss before: 0.0018259195470239462, loss after: 0.0018531516402274766]
[epoch 701/1000, batch 61/100 -> loss before: 0.01708236789355991, loss after: 0.017134519940186922]
[epoch 701/1000, batch 71/100 -> loss before: 0.04814896307642414, loss after: 0.04085985342194008]
[epoch 701/1000, batch 81/100 -> loss before: 0.016651190914029515, loss after: 0.012749545930861686]
[epoch 701/1000, batch 91/100 -> loss before: 0.044852526484378104, loss after: 0.04287327998373554]
ENDING EPOCH 701/1000 [loss before: 0.027991943009841074, loss after: 0.030350926285340415; epoch time: 0.03840947151184082 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.0593530218293468, loss after: 0.054679059070365854]
[epoch 801/1000, batch 11/100 -> loss before: 0.042684276637986865, loss after: 0.03740321250536424]
[epoch 801/1000, batch 21/100 -> loss before: 0.014581575857494002, loss after: 0.013414052837020652]
[epoch 801/1000, batch 31/100 -> loss before: 0.01661424095506298, loss after: 0.012468550305187238]
[epoch 801/1000, batch 41/100 -> loss before: 0.03510921762621064, loss after: 0.032071193666680084]
[epoch 801/1000, batch 51/100 -> loss before: 0.03032057364973555, loss after: 0.023424437273832137]
[epoch 801/1000, batch 61/100 -> loss before: 0.006621432507666278, loss after: 0.005694329364940184]
[epoch 801/1000, batch 71/100 -> loss before: 0.004304327333304018, loss after: 0.003504639487557991]
[epoch 801/1000, batch 81/100 -> loss before: 0.012353487110871126, loss after: 0.011371355545034004]
[epoch 801/1000, batch 91/100 -> loss before: 0.015591662998403177, loss after: 0.013517940810747725]
ENDING EPOCH 801/1000 [loss before: 0.02716950413398143, loss after: 0.02976694226886608; epoch time: 0.055649518966674805 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.006691162994957996, loss after: 0.006598726972810475]
[epoch 901/1000, batch 11/100 -> loss before: 0.0414334053764772, loss after: 0.04087103673996857]
[epoch 901/1000, batch 21/100 -> loss before: 0.022894987725226457, loss after: 0.020217781091397373]
[epoch 901/1000, batch 31/100 -> loss before: 0.1479736784106637, loss after: 0.14091657272218094]
[epoch 901/1000, batch 41/100 -> loss before: 0.02604243780209693, loss after: 0.02364250448636876]
[epoch 901/1000, batch 51/100 -> loss before: 0.04990558160617452, loss after: 0.048417876401506944]
[epoch 901/1000, batch 61/100 -> loss before: 0.06653199590174923, loss after: 0.05894158343675433]
[epoch 901/1000, batch 71/100 -> loss before: 0.010870112049273831, loss after: 0.010183714925467055]
[epoch 901/1000, batch 81/100 -> loss before: 0.04978707732150074, loss after: 0.03827071668824418]
[epoch 901/1000, batch 91/100 -> loss before: 0.0068952100558872355, loss after: 0.006337805744251789]
ENDING EPOCH 901/1000 [loss before: 0.023314186634671566, loss after: 0.02591028138932379; epoch time: 0.048838138580322266 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.06482320912227343, loss after: 0.05027097121271901]
[epoch 1000/1000, batch 11/100 -> loss before: 0.01625977332221433, loss after: 0.014090670944930647]
[epoch 1000/1000, batch 21/100 -> loss before: 0.023620688316200966, loss after: 0.019192419298860015]
[epoch 1000/1000, batch 31/100 -> loss before: 0.022429358338032498, loss after: 0.017970646013947163]
[epoch 1000/1000, batch 41/100 -> loss before: 0.01994833538379773, loss after: 0.0177593024589501]
[epoch 1000/1000, batch 51/100 -> loss before: 0.07757577876580829, loss after: 0.07176812732942991]
[epoch 1000/1000, batch 61/100 -> loss before: 0.01710676911821018, loss after: 0.015012798580997633]
[epoch 1000/1000, batch 71/100 -> loss before: 0.007941389975617618, loss after: 0.007262833895932043]
[epoch 1000/1000, batch 81/100 -> loss before: 0.022439305683841032, loss after: 0.01929109620659649]
[epoch 1000/1000, batch 91/100 -> loss before: 0.1014527993498849, loss after: 0.09076318421871188]
ENDING EPOCH 1000/1000 [loss before: 0.04017095617333124, loss after: 0.025259662442454717; epoch time: 0.048169612884521484 s]
FIT DONE. [time: 46.6377739906311 s]
LOSS TRAIN (MSE): 0.025259662442454717
LOSS TEST (MSE): 0.036213577112034226
R^2 TRAIN: 0.9107678908679706
R^2 TEST: 0.8695149010481704
EXPERIMENT DONE
