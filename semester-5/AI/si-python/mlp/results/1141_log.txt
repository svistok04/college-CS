EXPERIMENT 1141 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[128, 64, 32], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=adam, learning_rate=0.01, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 10753]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 1.6197437009973044, loss after: 0.464708624294655]
[epoch 1/1000, batch 11/100 -> loss before: 0.35077794314371197, loss after: 0.3299955474084843]
[epoch 1/1000, batch 21/100 -> loss before: 0.18487927660768344, loss after: 0.19960226271363796]
[epoch 1/1000, batch 31/100 -> loss before: 0.4610305233409912, loss after: 0.4704474225993026]
[epoch 1/1000, batch 41/100 -> loss before: 0.20245907552153047, loss after: 0.20621127170567627]
[epoch 1/1000, batch 51/100 -> loss before: 0.4737273590338546, loss after: 0.4261948869808377]
[epoch 1/1000, batch 61/100 -> loss before: 0.54731943391525, loss after: 0.5260566067545666]
[epoch 1/1000, batch 71/100 -> loss before: 0.42426200693952765, loss after: 0.4229915414241777]
[epoch 1/1000, batch 81/100 -> loss before: 0.13627074300468553, loss after: 0.13592887353198205]
[epoch 1/1000, batch 91/100 -> loss before: 0.4213995507339118, loss after: 0.4104626655004884]
ENDING EPOCH 1/1000 [loss before: 0.7227142221308361, loss after: 0.2997306033194072; epoch time: 0.04507613182067871 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.09451562649499576, loss after: 0.0906626755238809]
[epoch 101/1000, batch 11/100 -> loss before: 0.04097016666528022, loss after: 0.04114855143315517]
[epoch 101/1000, batch 21/100 -> loss before: 0.20023334015925148, loss after: 0.1839189436140709]
[epoch 101/1000, batch 31/100 -> loss before: 0.05789492845671544, loss after: 0.0569648563861202]
[epoch 101/1000, batch 41/100 -> loss before: 0.21981248044914015, loss after: 0.21744753254650379]
[epoch 101/1000, batch 51/100 -> loss before: 0.032169529754714, loss after: 0.029394009730909897]
[epoch 101/1000, batch 61/100 -> loss before: 0.04871607476612644, loss after: 0.04804473727180096]
[epoch 101/1000, batch 71/100 -> loss before: 0.13887237549055614, loss after: 0.14000088955372453]
[epoch 101/1000, batch 81/100 -> loss before: 0.08488672525999341, loss after: 0.0822426474333812]
[epoch 101/1000, batch 91/100 -> loss before: 0.03668764067005217, loss after: 0.032581359660057804]
ENDING EPOCH 101/1000 [loss before: 0.1137923417841614, loss after: 0.09175067954803796; epoch time: 0.09171104431152344 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.061766510875976545, loss after: 0.05614399081262446]
[epoch 201/1000, batch 11/100 -> loss before: 0.01741600315384091, loss after: 0.014754876156248648]
[epoch 201/1000, batch 21/100 -> loss before: 0.070642323276579, loss after: 0.0652195978634148]
[epoch 201/1000, batch 31/100 -> loss before: 0.010368596655504212, loss after: 0.0063110014593184575]
[epoch 201/1000, batch 41/100 -> loss before: 0.0442121370501612, loss after: 0.03990369112702108]
[epoch 201/1000, batch 51/100 -> loss before: 0.05080562313511716, loss after: 0.045105354379659936]
[epoch 201/1000, batch 61/100 -> loss before: 0.043121957049869236, loss after: 0.04150335610988973]
[epoch 201/1000, batch 71/100 -> loss before: 0.07020376972396898, loss after: 0.06961110542316043]
[epoch 201/1000, batch 81/100 -> loss before: 0.08365473907978258, loss after: 0.08139451624577075]
[epoch 201/1000, batch 91/100 -> loss before: 0.1350130115133757, loss after: 0.10196113821322858]
ENDING EPOCH 201/1000 [loss before: 0.04669679972295459, loss after: 0.04648076828160572; epoch time: 0.042669057846069336 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.025370518107735395, loss after: 0.025314962210247694]
[epoch 301/1000, batch 11/100 -> loss before: 0.050525681367046514, loss after: 0.0458690767413959]
[epoch 301/1000, batch 21/100 -> loss before: 0.020722577148923234, loss after: 0.015434836334072508]
[epoch 301/1000, batch 31/100 -> loss before: 0.025235948429529177, loss after: 0.01899250662511886]
[epoch 301/1000, batch 41/100 -> loss before: 0.008848237577862277, loss after: 0.007477028902495228]
[epoch 301/1000, batch 51/100 -> loss before: 0.06442964162810742, loss after: 0.0612039583866563]
[epoch 301/1000, batch 61/100 -> loss before: 0.044748750778314694, loss after: 0.04122347554348563]
[epoch 301/1000, batch 71/100 -> loss before: 0.03175966392726718, loss after: 0.031635538125783416]
[epoch 301/1000, batch 81/100 -> loss before: 0.03102760115557417, loss after: 0.02763496992852888]
[epoch 301/1000, batch 91/100 -> loss before: 0.016918950258332694, loss after: 0.016859171824683612]
ENDING EPOCH 301/1000 [loss before: 0.03296294260093983, loss after: 0.03497393481023118; epoch time: 0.04837489128112793 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.01571545545950171, loss after: 0.012079753382498065]
[epoch 401/1000, batch 11/100 -> loss before: 0.06379754744715173, loss after: 0.0628148373021586]
[epoch 401/1000, batch 21/100 -> loss before: 0.01022730637171596, loss after: 0.012095412844963809]
[epoch 401/1000, batch 31/100 -> loss before: 0.014742823032510876, loss after: 0.01387238676507092]
[epoch 401/1000, batch 41/100 -> loss before: 0.01285192968231758, loss after: 0.010575779186736673]
[epoch 401/1000, batch 51/100 -> loss before: 0.06669479516283079, loss after: 0.06117911871418007]
[epoch 401/1000, batch 61/100 -> loss before: 0.02671536548788227, loss after: 0.024895510504779478]
[epoch 401/1000, batch 71/100 -> loss before: 0.014501186048339661, loss after: 0.012534108949455858]
[epoch 401/1000, batch 81/100 -> loss before: 0.01793234847596159, loss after: 0.0170750442211632]
[epoch 401/1000, batch 91/100 -> loss before: 0.013939139106472717, loss after: 0.011734192436879887]
ENDING EPOCH 401/1000 [loss before: 0.03160409770337387, loss after: 0.02659622946596877; epoch time: 0.041229963302612305 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.018847507215805026, loss after: 0.017947849563071053]
[epoch 501/1000, batch 11/100 -> loss before: 0.020142770543596096, loss after: 0.018868235019175266]
[epoch 501/1000, batch 21/100 -> loss before: 0.01685099890044877, loss after: 0.015213907551964907]
[epoch 501/1000, batch 31/100 -> loss before: 0.018799085192528302, loss after: 0.014435234345841699]
[epoch 501/1000, batch 41/100 -> loss before: 0.012010570000831566, loss after: 0.011739621642704806]
[epoch 501/1000, batch 51/100 -> loss before: 0.01203476719967481, loss after: 0.010968812381390669]
[epoch 501/1000, batch 61/100 -> loss before: 0.029181375354197618, loss after: 0.026048146832527423]
[epoch 501/1000, batch 71/100 -> loss before: 0.025688401146964975, loss after: 0.02254100493925467]
[epoch 501/1000, batch 81/100 -> loss before: 0.04666159566336873, loss after: 0.04163253718787896]
[epoch 501/1000, batch 91/100 -> loss before: 0.023845689021710526, loss after: 0.0229552310092318]
ENDING EPOCH 501/1000 [loss before: 0.021773074601693122, loss after: 0.02491290412908638; epoch time: 0.0434267520904541 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.04286217972772734, loss after: 0.03720317552136772]
[epoch 601/1000, batch 11/100 -> loss before: 0.02900419188136192, loss after: 0.025652782330806236]
[epoch 601/1000, batch 21/100 -> loss before: 0.007434526863384789, loss after: 0.006694735286345142]
[epoch 601/1000, batch 31/100 -> loss before: 0.0170360458882913, loss after: 0.014562006345921794]
[epoch 601/1000, batch 41/100 -> loss before: 0.012280237114926627, loss after: 0.0112827155917186]
[epoch 601/1000, batch 51/100 -> loss before: 0.02471131694820149, loss after: 0.019748234109021]
[epoch 601/1000, batch 61/100 -> loss before: 0.006095346202145385, loss after: 0.005865984501512667]
[epoch 601/1000, batch 71/100 -> loss before: 0.014790155787538787, loss after: 0.012901994070181545]
[epoch 601/1000, batch 81/100 -> loss before: 0.015293885122387251, loss after: 0.014064177553651826]
[epoch 601/1000, batch 91/100 -> loss before: 0.027312276280066095, loss after: 0.016619629764325385]
ENDING EPOCH 601/1000 [loss before: 0.02769807380248545, loss after: 0.033584537990660225; epoch time: 0.04443168640136719 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.007790449805780896, loss after: 0.006542060201651853]
[epoch 701/1000, batch 11/100 -> loss before: 0.013430324824650097, loss after: 0.010014276689963908]
[epoch 701/1000, batch 21/100 -> loss before: 0.008840316706172698, loss after: 0.008058407977324841]
[epoch 701/1000, batch 31/100 -> loss before: 0.0054385193083467745, loss after: 0.0049924334716873135]
[epoch 701/1000, batch 41/100 -> loss before: 0.04299890453634308, loss after: 0.04137731827112059]
[epoch 701/1000, batch 51/100 -> loss before: 0.006350160110481566, loss after: 0.005329349462481507]
[epoch 701/1000, batch 61/100 -> loss before: 0.014301883305572968, loss after: 0.013578728168984472]
[epoch 701/1000, batch 71/100 -> loss before: 0.0439199314135784, loss after: 0.03243097150947381]
[epoch 701/1000, batch 81/100 -> loss before: 0.020160269043103158, loss after: 0.013454804910811372]
[epoch 701/1000, batch 91/100 -> loss before: 0.018215702272205304, loss after: 0.015494637594558106]
ENDING EPOCH 701/1000 [loss before: 0.02061964330610932, loss after: 0.02005688339493152; epoch time: 0.042107582092285156 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.02134112362426959, loss after: 0.016821544962270816]
[epoch 801/1000, batch 11/100 -> loss before: 0.00859726277609131, loss after: 0.0070870329458064445]
[epoch 801/1000, batch 21/100 -> loss before: 0.012713614676062237, loss after: 0.01002630421825942]
[epoch 801/1000, batch 31/100 -> loss before: 0.008775702710299916, loss after: 0.00653869547679716]
[epoch 801/1000, batch 41/100 -> loss before: 0.016124981655910885, loss after: 0.01584033254239944]
[epoch 801/1000, batch 51/100 -> loss before: 0.008611060722773917, loss after: 0.007296426426536188]
[epoch 801/1000, batch 61/100 -> loss before: 0.007168021085590869, loss after: 0.006140811822073218]
[epoch 801/1000, batch 71/100 -> loss before: 0.008963360372035482, loss after: 0.008301447415846212]
[epoch 801/1000, batch 81/100 -> loss before: 0.016004340978008975, loss after: 0.013683197989549275]
[epoch 801/1000, batch 91/100 -> loss before: 0.00898295408436756, loss after: 0.008110760325777419]
ENDING EPOCH 801/1000 [loss before: 0.02084351426377116, loss after: 0.017488649581097308; epoch time: 0.042856454849243164 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.004997998705363936, loss after: 0.0038457597568382178]
[epoch 901/1000, batch 11/100 -> loss before: 0.022850949850165142, loss after: 0.0221051416586547]
[epoch 901/1000, batch 21/100 -> loss before: 0.006795101090899232, loss after: 0.006276568718553884]
[epoch 901/1000, batch 31/100 -> loss before: 0.12289256535173095, loss after: 0.12063891803850346]
[epoch 901/1000, batch 41/100 -> loss before: 0.004584648543244531, loss after: 0.005380334274158189]
[epoch 901/1000, batch 51/100 -> loss before: 0.08056404012150302, loss after: 0.07822439981337412]
[epoch 901/1000, batch 61/100 -> loss before: 0.05708774187017709, loss after: 0.05245915389514312]
[epoch 901/1000, batch 71/100 -> loss before: 0.010188053042685231, loss after: 0.0076486956098158684]
[epoch 901/1000, batch 81/100 -> loss before: 0.0215138718244668, loss after: 0.02060426855716235]
[epoch 901/1000, batch 91/100 -> loss before: 0.01831365415704652, loss after: 0.014510957351844708]
ENDING EPOCH 901/1000 [loss before: 0.02207691852141977, loss after: 0.025012249377997606; epoch time: 0.04493856430053711 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.0072114663367904815, loss after: 0.005967289575629795]
[epoch 1000/1000, batch 11/100 -> loss before: 0.01609742925209935, loss after: 0.014183829366859557]
[epoch 1000/1000, batch 21/100 -> loss before: 0.019099071643571823, loss after: 0.012866889321611405]
[epoch 1000/1000, batch 31/100 -> loss before: 0.009919638505752068, loss after: 0.009783493588525432]
[epoch 1000/1000, batch 41/100 -> loss before: 0.017574300153045487, loss after: 0.017584427506712047]
[epoch 1000/1000, batch 51/100 -> loss before: 0.08282482310370218, loss after: 0.07429037447966773]
[epoch 1000/1000, batch 61/100 -> loss before: 0.012426365029854556, loss after: 0.010436932262139275]
[epoch 1000/1000, batch 71/100 -> loss before: 0.010644006149240456, loss after: 0.010266710681152202]
[epoch 1000/1000, batch 81/100 -> loss before: 0.023432876637245782, loss after: 0.02259202360279185]
[epoch 1000/1000, batch 91/100 -> loss before: 0.09686785890505502, loss after: 0.08847292417086451]
ENDING EPOCH 1000/1000 [loss before: 0.022189916792598227, loss after: 0.02932149072328251; epoch time: 0.055257558822631836 s]
FIT DONE. [time: 41.30760431289673 s]
LOSS TRAIN (MSE): 0.02932149072328251
LOSS TEST (MSE): 0.04046516143415002
R^2 TRAIN: 0.8964191043291126
R^2 TEST: 0.8541955527480278
EXPERIMENT DONE
