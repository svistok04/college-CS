EXPERIMENT 3243 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[64, 64, 64, 64, 64, 32, 32, 32, 32, 32, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8], activation_name=relu, targets_activation_name=linear, 
initialization_name=uniform, algo_name=adam, learning_rate=0.0001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 25185]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.18318931763552243, loss after: 0.18101212449992873]
[epoch 1/1000, batch 11/100 -> loss before: 0.32160342534123315, loss after: 0.31199130768097155]
[epoch 1/1000, batch 21/100 -> loss before: 0.3666116082545452, loss after: 0.36659348437345673]
[epoch 1/1000, batch 31/100 -> loss before: 0.2555617468879038, loss after: 0.2535483906564517]
[epoch 1/1000, batch 41/100 -> loss before: 0.20104513184499714, loss after: 0.19657730427994857]
[epoch 1/1000, batch 51/100 -> loss before: 0.23612846025831705, loss after: 0.23608018547451187]
[epoch 1/1000, batch 61/100 -> loss before: 0.4292224677459112, loss after: 0.42783732014260467]
[epoch 1/1000, batch 71/100 -> loss before: 0.3023725962529377, loss after: 0.30692780413954884]
[epoch 1/1000, batch 81/100 -> loss before: 0.3387401862891478, loss after: 0.3365867235812103]
[epoch 1/1000, batch 91/100 -> loss before: 0.3662278994370053, loss after: 0.361239384025138]
ENDING EPOCH 1/1000 [loss before: 0.2899884218128069, loss after: 0.2634981296646459; epoch time: 0.17458367347717285 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.20039937669416572, loss after: 0.20042465454828617]
[epoch 101/1000, batch 11/100 -> loss before: 0.1437077596007491, loss after: 0.1434271902763742]
[epoch 101/1000, batch 21/100 -> loss before: 0.14776140610090546, loss after: 0.14678324710326643]
[epoch 101/1000, batch 31/100 -> loss before: 0.26396734369782104, loss after: 0.2632606913344903]
[epoch 101/1000, batch 41/100 -> loss before: 0.4984731445142902, loss after: 0.4984404979677561]
[epoch 101/1000, batch 51/100 -> loss before: 0.2223714610757826, loss after: 0.22234899230819954]
[epoch 101/1000, batch 61/100 -> loss before: 0.4918825028020895, loss after: 0.48751765990440354]
[epoch 101/1000, batch 71/100 -> loss before: 0.21155330730142405, loss after: 0.20933902575489646]
[epoch 101/1000, batch 81/100 -> loss before: 0.29854197934123655, loss after: 0.2982794875104835]
[epoch 101/1000, batch 91/100 -> loss before: 0.27963633604503235, loss after: 0.27335080268604567]
ENDING EPOCH 101/1000 [loss before: 0.22335374999971486, loss after: 0.22459419941045058; epoch time: 0.15149855613708496 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.017539041612188166, loss after: 0.01649545280752534]
[epoch 201/1000, batch 11/100 -> loss before: 0.15100755843709177, loss after: 0.15086345623218275]
[epoch 201/1000, batch 21/100 -> loss before: 0.2543728991945916, loss after: 0.2551699640680228]
[epoch 201/1000, batch 31/100 -> loss before: 0.16812949846893957, loss after: 0.1672847521642896]
[epoch 201/1000, batch 41/100 -> loss before: 0.22134766596019356, loss after: 0.21783457286928415]
[epoch 201/1000, batch 51/100 -> loss before: 0.11091029595938977, loss after: 0.10778339080274295]
[epoch 201/1000, batch 61/100 -> loss before: 0.2131860796049223, loss after: 0.2125164723637794]
[epoch 201/1000, batch 71/100 -> loss before: 0.3000891108030913, loss after: 0.29666345116887305]
[epoch 201/1000, batch 81/100 -> loss before: 0.0899285407974314, loss after: 0.08592206939423265]
[epoch 201/1000, batch 91/100 -> loss before: 0.07652946570673391, loss after: 0.07667987704136256]
ENDING EPOCH 201/1000 [loss before: 0.15525630951506356, loss after: 0.15572583389581582; epoch time: 0.15149855613708496 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.1421741794095041, loss after: 0.1397519703065652]
[epoch 301/1000, batch 11/100 -> loss before: 0.058719169486431945, loss after: 0.05776844476833948]
[epoch 301/1000, batch 21/100 -> loss before: 0.11693708862790198, loss after: 0.1149761973610232]
[epoch 301/1000, batch 31/100 -> loss before: 0.15177523697743095, loss after: 0.14890117697725821]
[epoch 301/1000, batch 41/100 -> loss before: 0.04558592446496636, loss after: 0.04611792612259526]
[epoch 301/1000, batch 51/100 -> loss before: 0.10123098606085694, loss after: 0.09883002656813562]
[epoch 301/1000, batch 61/100 -> loss before: 0.13141472399755125, loss after: 0.1302165779993511]
[epoch 301/1000, batch 71/100 -> loss before: 0.19136396847875253, loss after: 0.18897671817551057]
[epoch 301/1000, batch 81/100 -> loss before: 0.11308736742457808, loss after: 0.10358272398602593]
[epoch 301/1000, batch 91/100 -> loss before: 0.17401601273636172, loss after: 0.16941955390319094]
ENDING EPOCH 301/1000 [loss before: 0.12481685154392766, loss after: 0.11724425696686429; epoch time: 0.15897583961486816 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.06611252540573401, loss after: 0.061559393341634704]
[epoch 401/1000, batch 11/100 -> loss before: 0.10567208841345747, loss after: 0.10469275232045941]
[epoch 401/1000, batch 21/100 -> loss before: 0.1451580951064882, loss after: 0.1353535676799157]
[epoch 401/1000, batch 31/100 -> loss before: 0.07955224197413632, loss after: 0.07374162448098706]
[epoch 401/1000, batch 41/100 -> loss before: 0.08842017071831959, loss after: 0.08608677448254622]
[epoch 401/1000, batch 51/100 -> loss before: 0.07124420845526618, loss after: 0.06558057002601263]
[epoch 401/1000, batch 61/100 -> loss before: 0.2528165282348501, loss after: 0.24501610787641148]
[epoch 401/1000, batch 71/100 -> loss before: 0.04178820697128511, loss after: 0.043338219525117946]
[epoch 401/1000, batch 81/100 -> loss before: 0.045437249879815976, loss after: 0.04830328347913614]
[epoch 401/1000, batch 91/100 -> loss before: 0.0605932934616946, loss after: 0.06039868639558632]
ENDING EPOCH 401/1000 [loss before: 0.0986677399043082, loss after: 0.11068016593084547; epoch time: 0.15194416046142578 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.10060438010757256, loss after: 0.09265586103746198]
[epoch 501/1000, batch 11/100 -> loss before: 0.038951610363839914, loss after: 0.038788437378375513]
[epoch 501/1000, batch 21/100 -> loss before: 0.0455015588822321, loss after: 0.04271027072949527]
[epoch 501/1000, batch 31/100 -> loss before: 0.06053950903415632, loss after: 0.05898226817032742]
[epoch 501/1000, batch 41/100 -> loss before: 0.06855055039092302, loss after: 0.06204839422344375]
[epoch 501/1000, batch 51/100 -> loss before: 0.024860027735074824, loss after: 0.021966300159816852]
[epoch 501/1000, batch 61/100 -> loss before: 0.05217670181296745, loss after: 0.04840011636966]
[epoch 501/1000, batch 71/100 -> loss before: 0.0888433741547128, loss after: 0.08582763009211199]
[epoch 501/1000, batch 81/100 -> loss before: 0.02276641687669174, loss after: 0.03019963858832775]
[epoch 501/1000, batch 91/100 -> loss before: 0.015873991342739705, loss after: 0.015111216652923845]
ENDING EPOCH 501/1000 [loss before: 0.06102009358293309, loss after: 0.07322741357773813; epoch time: 0.15166020393371582 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.08069983144017367, loss after: 0.08127889520755054]
[epoch 601/1000, batch 11/100 -> loss before: 0.08391638787778623, loss after: 0.08218820461374043]
[epoch 601/1000, batch 21/100 -> loss before: 0.06912154226066826, loss after: 0.06237766210039007]
[epoch 601/1000, batch 31/100 -> loss before: 0.009037817172920865, loss after: 0.00839963796537953]
[epoch 601/1000, batch 41/100 -> loss before: 0.018778391676703605, loss after: 0.01864725560269542]
[epoch 601/1000, batch 51/100 -> loss before: 0.14586786793020706, loss after: 0.1182503644202247]
[epoch 601/1000, batch 61/100 -> loss before: 0.05639168880404908, loss after: 0.05202289560107183]
[epoch 601/1000, batch 71/100 -> loss before: 0.05922338196686318, loss after: 0.056360259553634065]
[epoch 601/1000, batch 81/100 -> loss before: 0.045496365949713596, loss after: 0.04350577268569947]
[epoch 601/1000, batch 91/100 -> loss before: 0.04054167261301556, loss after: 0.036764518971065675]
ENDING EPOCH 601/1000 [loss before: 0.0571221705552989, loss after: 0.07295341551847132; epoch time: 0.15442395210266113 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.02859515526596233, loss after: 0.027171087906023628]
[epoch 701/1000, batch 11/100 -> loss before: 0.02756167236793221, loss after: 0.024672800833851975]
[epoch 701/1000, batch 21/100 -> loss before: 0.020867015807366406, loss after: 0.02285088099896562]
[epoch 701/1000, batch 31/100 -> loss before: 0.12477954286740922, loss after: 0.12156572113098967]
[epoch 701/1000, batch 41/100 -> loss before: 0.03709131977338272, loss after: 0.029442199817686265]
[epoch 701/1000, batch 51/100 -> loss before: 0.07851419458090612, loss after: 0.07058099119374174]
[epoch 701/1000, batch 61/100 -> loss before: 0.015395110447017446, loss after: 0.014350621524450477]
[epoch 701/1000, batch 71/100 -> loss before: 0.0210742530717422, loss after: 0.02074709094429245]
[epoch 701/1000, batch 81/100 -> loss before: 0.029583273504335234, loss after: 0.02215636639970105]
[epoch 701/1000, batch 91/100 -> loss before: 0.027826392826031478, loss after: 0.02350187718063384]
ENDING EPOCH 701/1000 [loss before: 0.02919818463493409, loss after: 0.02571137774267634; epoch time: 0.1586308479309082 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.02083692342725649, loss after: 0.02120983628131233]
[epoch 801/1000, batch 11/100 -> loss before: 0.01502116109220881, loss after: 0.017934400582762293]
[epoch 801/1000, batch 21/100 -> loss before: 0.020019217616408208, loss after: 0.01730419996364626]
[epoch 801/1000, batch 31/100 -> loss before: 0.008856822621919967, loss after: 0.008883038957118698]
[epoch 801/1000, batch 41/100 -> loss before: 0.006847676667021794, loss after: 0.007684431195423126]
[epoch 801/1000, batch 51/100 -> loss before: 0.020269667528673097, loss after: 0.01164801816370334]
[epoch 801/1000, batch 61/100 -> loss before: 0.04602458281133366, loss after: 0.0448030412170616]
[epoch 801/1000, batch 71/100 -> loss before: 0.019986123671282573, loss after: 0.018909144240106396]
[epoch 801/1000, batch 81/100 -> loss before: 0.011858090067027791, loss after: 0.01081677191625844]
[epoch 801/1000, batch 91/100 -> loss before: 0.029207867950491007, loss after: 0.0247281352764016]
ENDING EPOCH 801/1000 [loss before: 0.03279182447406906, loss after: 0.024596791451853298; epoch time: 0.16162991523742676 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.03214132990454217, loss after: 0.03287702704455754]
[epoch 901/1000, batch 11/100 -> loss before: 0.07806122019177948, loss after: 0.048665391683065905]
[epoch 901/1000, batch 21/100 -> loss before: 0.007036408626634109, loss after: 0.006629775516233468]
[epoch 901/1000, batch 31/100 -> loss before: 0.0055932429276077215, loss after: 0.005569136397039013]
[epoch 901/1000, batch 41/100 -> loss before: 0.013506372383255078, loss after: 0.012527087033423414]
[epoch 901/1000, batch 51/100 -> loss before: 0.018258061438773846, loss after: 0.017217419258631687]
[epoch 901/1000, batch 61/100 -> loss before: 0.01171440470654786, loss after: 0.012134949354391042]
[epoch 901/1000, batch 71/100 -> loss before: 0.024002611529699118, loss after: 0.022371455802433916]
[epoch 901/1000, batch 81/100 -> loss before: 0.029284894123377803, loss after: 0.01806402023311087]
[epoch 901/1000, batch 91/100 -> loss before: 0.09589867037790722, loss after: 0.06516690342374001]
ENDING EPOCH 901/1000 [loss before: 0.025402819234750153, loss after: 0.06894971689084034; epoch time: 0.19789409637451172 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.028849579872446074, loss after: 0.025318245888059882]
[epoch 1000/1000, batch 11/100 -> loss before: 0.01172058854981135, loss after: 0.012790259438298604]
[epoch 1000/1000, batch 21/100 -> loss before: 0.00782158834008043, loss after: 0.007650439865800923]
[epoch 1000/1000, batch 31/100 -> loss before: 0.0327188028402563, loss after: 0.029816347537670727]
[epoch 1000/1000, batch 41/100 -> loss before: 0.03602643395399429, loss after: 0.034273053195015175]
[epoch 1000/1000, batch 51/100 -> loss before: 0.009078223666287494, loss after: 0.00813388672683354]
[epoch 1000/1000, batch 61/100 -> loss before: 0.054732741359892666, loss after: 0.031182838587210816]
[epoch 1000/1000, batch 71/100 -> loss before: 0.0405704320153774, loss after: 0.03705529518703306]
[epoch 1000/1000, batch 81/100 -> loss before: 0.02289601558930803, loss after: 0.021063948643388876]
[epoch 1000/1000, batch 91/100 -> loss before: 0.04449422499271751, loss after: 0.04301741870877239]
ENDING EPOCH 1000/1000 [loss before: 0.024290898149380027, loss after: 0.03523227947165791; epoch time: 0.1543869972229004 s]
FIT DONE. [time: 153.84113192558289 s]
LOSS TRAIN (MSE): 0.03523227947165791
LOSS TEST (MSE): 0.05532796830775985
R^2 TRAIN: 0.8755386928092448
R^2 TEST: 0.8006417483391164
EXPERIMENT DONE
