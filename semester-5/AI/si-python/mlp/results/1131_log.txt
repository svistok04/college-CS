EXPERIMENT 1131 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[128, 64, 32], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=rmsprop, learning_rate=0.01, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 10753]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 1.6197437009973044, loss after: 1.7458929136728698]
[epoch 1/1000, batch 11/100 -> loss before: 0.22518382816591836, loss after: 0.1857705172055208]
[epoch 1/1000, batch 21/100 -> loss before: 0.07277224345110021, loss after: 0.07261800921285141]
[epoch 1/1000, batch 31/100 -> loss before: 0.47590110406197716, loss after: 0.45306334852645397]
[epoch 1/1000, batch 41/100 -> loss before: 0.17941855119225064, loss after: 0.1686193544005467]
[epoch 1/1000, batch 51/100 -> loss before: 0.32227429172503674, loss after: 0.24822602336141952]
[epoch 1/1000, batch 61/100 -> loss before: 0.5243359259233968, loss after: 0.48881163380710524]
[epoch 1/1000, batch 71/100 -> loss before: 0.4644074664515155, loss after: 0.3946950980217802]
[epoch 1/1000, batch 81/100 -> loss before: 0.15230456930137656, loss after: 0.1396556851344546]
[epoch 1/1000, batch 91/100 -> loss before: 0.3978651398455881, loss after: 0.3893787353283346]
ENDING EPOCH 1/1000 [loss before: 0.7227142221308361, loss after: 0.2793736299375512; epoch time: 0.03949236869812012 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.07798226976896602, loss after: 0.06844730332688691]
[epoch 101/1000, batch 11/100 -> loss before: 0.09299136112647724, loss after: 0.020957618585885178]
[epoch 101/1000, batch 21/100 -> loss before: 0.06523793834027136, loss after: 0.05813512007925874]
[epoch 101/1000, batch 31/100 -> loss before: 0.04445936223892051, loss after: 0.04230625668064364]
[epoch 101/1000, batch 41/100 -> loss before: 0.21787557079559022, loss after: 0.18534917892619923]
[epoch 101/1000, batch 51/100 -> loss before: 0.12605485829071997, loss after: 0.05111001974022168]
[epoch 101/1000, batch 61/100 -> loss before: 0.02184356507359895, loss after: 0.009883246310257315]
[epoch 101/1000, batch 71/100 -> loss before: 0.1543028822924295, loss after: 0.14033235149205106]
[epoch 101/1000, batch 81/100 -> loss before: 0.07882956798097618, loss after: 0.054538092596896404]
[epoch 101/1000, batch 91/100 -> loss before: 0.020974244460387873, loss after: 0.022089050626633733]
ENDING EPOCH 101/1000 [loss before: 0.07146320961891345, loss after: 0.09034891682807975; epoch time: 0.03507709503173828 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.045690556710492956, loss after: 0.026865277346663923]
[epoch 201/1000, batch 11/100 -> loss before: 0.032374615698375214, loss after: 0.006646103080365189]
[epoch 201/1000, batch 21/100 -> loss before: 0.09851536372713304, loss after: 0.04905121362677763]
[epoch 201/1000, batch 31/100 -> loss before: 0.023725623184754362, loss after: 0.07496949591186486]
[epoch 201/1000, batch 41/100 -> loss before: 0.039455229724598195, loss after: 0.08675077801777771]
[epoch 201/1000, batch 51/100 -> loss before: 0.043057402198658966, loss after: 0.03474233082933858]
[epoch 201/1000, batch 61/100 -> loss before: 0.04656726913960836, loss after: 0.022099636706621913]
[epoch 201/1000, batch 71/100 -> loss before: 0.06706934367729159, loss after: 0.040694587256870585]
[epoch 201/1000, batch 81/100 -> loss before: 0.07899055723485159, loss after: 0.05600227614577774]
[epoch 201/1000, batch 91/100 -> loss before: 0.026953273223057128, loss after: 0.007961890361717941]
ENDING EPOCH 201/1000 [loss before: 0.045831730576914594, loss after: 0.052167942975281434; epoch time: 0.03740811347961426 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.06690382271086083, loss after: 0.060731936626621186]
[epoch 301/1000, batch 11/100 -> loss before: 0.021018882335031602, loss after: 0.007821141288110012]
[epoch 301/1000, batch 21/100 -> loss before: 0.0439606846122189, loss after: 0.024153856006008775]
[epoch 301/1000, batch 31/100 -> loss before: 0.026824505984344067, loss after: 0.01600341344952528]
[epoch 301/1000, batch 41/100 -> loss before: 0.016468945100313905, loss after: 0.005268730398750323]
[epoch 301/1000, batch 51/100 -> loss before: 0.022758989118486683, loss after: 0.0133353761049908]
[epoch 301/1000, batch 61/100 -> loss before: 0.0351584134643548, loss after: 0.023891994877245543]
[epoch 301/1000, batch 71/100 -> loss before: 0.02274715060803784, loss after: 0.029273590564507224]
[epoch 301/1000, batch 81/100 -> loss before: 0.028693307118170315, loss after: 0.019173255104042452]
[epoch 301/1000, batch 91/100 -> loss before: 0.013460009974573398, loss after: 0.010831346709214306]
ENDING EPOCH 301/1000 [loss before: 0.046017472323314566, loss after: 0.046959684433865036; epoch time: 0.038942575454711914 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.014680762803928337, loss after: 0.010579865756295468]
[epoch 401/1000, batch 11/100 -> loss before: 0.09272030366097006, loss after: 0.05514668827035202]
[epoch 401/1000, batch 21/100 -> loss before: 0.004466725194713207, loss after: 0.0050032385285442415]
[epoch 401/1000, batch 31/100 -> loss before: 0.03256599615882071, loss after: 0.016888376714460494]
[epoch 401/1000, batch 41/100 -> loss before: 0.019596815907835304, loss after: 0.00967545475999977]
[epoch 401/1000, batch 51/100 -> loss before: 0.16607400223866498, loss after: 0.06960661426089562]
[epoch 401/1000, batch 61/100 -> loss before: 0.031220556802024863, loss after: 0.00947853657703364]
[epoch 401/1000, batch 71/100 -> loss before: 0.014480553886926133, loss after: 0.0069395596769927775]
[epoch 401/1000, batch 81/100 -> loss before: 0.014662969146500313, loss after: 0.009347180318683904]
[epoch 401/1000, batch 91/100 -> loss before: 0.017352144021087323, loss after: 0.012994887908433912]
ENDING EPOCH 401/1000 [loss before: 0.027371192025341693, loss after: 0.033018193592356124; epoch time: 0.030795574188232422 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.01596675848534292, loss after: 0.00648915210340859]
[epoch 501/1000, batch 11/100 -> loss before: 0.04014768264609049, loss after: 0.020883270057610506]
[epoch 501/1000, batch 21/100 -> loss before: 0.021647812030583306, loss after: 0.005176833319278642]
[epoch 501/1000, batch 31/100 -> loss before: 0.0249793086851504, loss after: 0.005757387141309187]
[epoch 501/1000, batch 41/100 -> loss before: 0.016997218159416617, loss after: 0.007894529651508015]
[epoch 501/1000, batch 51/100 -> loss before: 0.034585738745211134, loss after: 0.018299451441633474]
[epoch 501/1000, batch 61/100 -> loss before: 0.03659732297091515, loss after: 0.008040773979222959]
[epoch 501/1000, batch 71/100 -> loss before: 0.03161266369601186, loss after: 0.01947843583102689]
[epoch 501/1000, batch 81/100 -> loss before: 0.03300252978956621, loss after: 0.013972680420761408]
[epoch 501/1000, batch 91/100 -> loss before: 0.013300270624178132, loss after: 0.007248113071072215]
ENDING EPOCH 501/1000 [loss before: 0.02975362541145554, loss after: 0.0333290404938334; epoch time: 0.03427624702453613 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.012187965192563377, loss after: 0.007854812649577457]
[epoch 601/1000, batch 11/100 -> loss before: 0.02251907356730982, loss after: 0.014935445423173413]
[epoch 601/1000, batch 21/100 -> loss before: 0.01615284575499224, loss after: 0.004710525934394546]
[epoch 601/1000, batch 31/100 -> loss before: 0.015321373100981622, loss after: 0.0054790114911590065]
[epoch 601/1000, batch 41/100 -> loss before: 0.018825673959637586, loss after: 0.008740252359227438]
[epoch 601/1000, batch 51/100 -> loss before: 0.02016628351747591, loss after: 0.023160967193526037]
[epoch 601/1000, batch 61/100 -> loss before: 0.0257303635029357, loss after: 0.011393765708593516]
[epoch 601/1000, batch 71/100 -> loss before: 0.022480668932794608, loss after: 0.04271119786250931]
[epoch 601/1000, batch 81/100 -> loss before: 0.029354769585054712, loss after: 0.0015990770704071532]
[epoch 601/1000, batch 91/100 -> loss before: 0.018172525604020086, loss after: 0.011670442318648581]
ENDING EPOCH 601/1000 [loss before: 0.022552563352840203, loss after: 0.02575660550773987; epoch time: 0.034117698669433594 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.0100648923159608, loss after: 0.004537657037392195]
[epoch 701/1000, batch 11/100 -> loss before: 0.013189973311028039, loss after: 0.004308791336477301]
[epoch 701/1000, batch 21/100 -> loss before: 0.014643834800010391, loss after: 0.00788980477930591]
[epoch 701/1000, batch 31/100 -> loss before: 0.010551476995803494, loss after: 0.006256879023541652]
[epoch 701/1000, batch 41/100 -> loss before: 0.03757895084486156, loss after: 0.006746201796790703]
[epoch 701/1000, batch 51/100 -> loss before: 0.053617031669074514, loss after: 0.010570747703425712]
[epoch 701/1000, batch 61/100 -> loss before: 0.017191576901480377, loss after: 0.006978047648158871]
[epoch 701/1000, batch 71/100 -> loss before: 0.03526001942378635, loss after: 0.02480106424743008]
[epoch 701/1000, batch 81/100 -> loss before: 0.032527929142881704, loss after: 0.019818978464997434]
[epoch 701/1000, batch 91/100 -> loss before: 0.026448275369289908, loss after: 0.0131680144732234]
ENDING EPOCH 701/1000 [loss before: 0.019517326075383538, loss after: 0.03391742056241109; epoch time: 0.03273773193359375 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.025924361149367515, loss after: 0.0068654753662700544]
[epoch 801/1000, batch 11/100 -> loss before: 0.01824397069209007, loss after: 0.005668341706158968]
[epoch 801/1000, batch 21/100 -> loss before: 0.03032999999272154, loss after: 0.0038897077584758013]
[epoch 801/1000, batch 31/100 -> loss before: 0.008456528501878705, loss after: 0.008576042073562915]
[epoch 801/1000, batch 41/100 -> loss before: 0.014525567605951972, loss after: 0.0038476585752333856]
[epoch 801/1000, batch 51/100 -> loss before: 0.026630514797343956, loss after: 0.03168244948943461]
[epoch 801/1000, batch 61/100 -> loss before: 0.034264744552230955, loss after: 0.016961975526097843]
[epoch 801/1000, batch 71/100 -> loss before: 0.012599224149444931, loss after: 0.004521177491303873]
[epoch 801/1000, batch 81/100 -> loss before: 0.19179988183884134, loss after: 0.024228236778370184]
[epoch 801/1000, batch 91/100 -> loss before: 0.010483934097176042, loss after: 0.003151911217815357]
ENDING EPOCH 801/1000 [loss before: 0.02398564401234528, loss after: 0.01885956105329042; epoch time: 0.03400850296020508 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.014371063714085988, loss after: 0.014421845884136694]
[epoch 901/1000, batch 11/100 -> loss before: 0.027220214816160753, loss after: 0.014220793439145979]
[epoch 901/1000, batch 21/100 -> loss before: 0.011139675559947283, loss after: 0.00801058872546068]
[epoch 901/1000, batch 31/100 -> loss before: 0.06679802570297515, loss after: 0.007100733163604715]
[epoch 901/1000, batch 41/100 -> loss before: 0.012772279683623933, loss after: 0.0030793101650915663]
[epoch 901/1000, batch 51/100 -> loss before: 0.030925220743531663, loss after: 0.012650815368437835]
[epoch 901/1000, batch 61/100 -> loss before: 0.06332468546050787, loss after: 0.04054144694101285]
[epoch 901/1000, batch 71/100 -> loss before: 0.003522618962711845, loss after: 0.00834445595724153]
[epoch 901/1000, batch 81/100 -> loss before: 0.031279061182350354, loss after: 0.015188339861785093]
[epoch 901/1000, batch 91/100 -> loss before: 0.010444243403378978, loss after: 0.003273629703722482]
ENDING EPOCH 901/1000 [loss before: 0.03125255904360534, loss after: 0.023646363278952365; epoch time: 0.03468132019042969 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.009142209164350195, loss after: 0.022630274366927182]
[epoch 1000/1000, batch 11/100 -> loss before: 0.022078114245181697, loss after: 0.004643520187914614]
[epoch 1000/1000, batch 21/100 -> loss before: 0.02512450545165206, loss after: 0.01724627832491573]
[epoch 1000/1000, batch 31/100 -> loss before: 0.0207219176925933, loss after: 0.004411184527175138]
[epoch 1000/1000, batch 41/100 -> loss before: 0.014695620351345612, loss after: 0.005380935306965567]
[epoch 1000/1000, batch 51/100 -> loss before: 0.14131603132517256, loss after: 0.03843583296048945]
[epoch 1000/1000, batch 61/100 -> loss before: 0.0365701643883584, loss after: 0.06352239142578509]
[epoch 1000/1000, batch 71/100 -> loss before: 0.019500829298376033, loss after: 0.005774327527743683]
[epoch 1000/1000, batch 81/100 -> loss before: 0.019557479845809158, loss after: 0.008920387535516184]
[epoch 1000/1000, batch 91/100 -> loss before: 0.043177889869983416, loss after: 0.018870642590970037]
ENDING EPOCH 1000/1000 [loss before: 0.01855539981109811, loss after: 0.02068488189145847; epoch time: 0.03714585304260254 s]
FIT DONE. [time: 32.65353298187256 s]
LOSS TRAIN (MSE): 0.02068488189145847
LOSS TEST (MSE): 0.03196315637478062
R^2 TRAIN: 0.9269287290546075
R^2 TEST: 0.8848300567084805
EXPERIMENT DONE
