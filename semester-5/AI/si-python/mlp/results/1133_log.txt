EXPERIMENT 1133 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[64, 64, 64, 64, 64, 32, 32, 32, 32, 32, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=rmsprop, learning_rate=0.01, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 25185]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.6815884323265685, loss after: 0.3335970310110145]
[epoch 1/1000, batch 11/100 -> loss before: 0.3001535491125481, loss after: 0.2892577295077414]
[epoch 1/1000, batch 21/100 -> loss before: 0.40147029744898743, loss after: 0.3730262568435164]
[epoch 1/1000, batch 31/100 -> loss before: 0.3799276005547421, loss after: 0.37161465154121304]
[epoch 1/1000, batch 41/100 -> loss before: 0.24643360861808694, loss after: 0.20829201325930105]
[epoch 1/1000, batch 51/100 -> loss before: 0.2960612778867905, loss after: 0.24589507777047262]
[epoch 1/1000, batch 61/100 -> loss before: 0.48879634044161574, loss after: 0.4706029978181089]
[epoch 1/1000, batch 71/100 -> loss before: 0.3568601035562367, loss after: 0.34086742820299004]
[epoch 1/1000, batch 81/100 -> loss before: 0.39845232944420017, loss after: 0.3973545620441104]
[epoch 1/1000, batch 91/100 -> loss before: 0.30120456713488364, loss after: 0.29267853723939663]
ENDING EPOCH 1/1000 [loss before: 0.599648552700766, loss after: 0.30061596624137815; epoch time: 0.16997623443603516 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.2906340175813418, loss after: 0.28941414465853005]
[epoch 101/1000, batch 11/100 -> loss before: 0.21976120299711216, loss after: 0.21976014615824901]
[epoch 101/1000, batch 21/100 -> loss before: 0.24255788931308028, loss after: 0.24183739629199813]
[epoch 101/1000, batch 31/100 -> loss before: 0.39116930949760914, loss after: 0.387257509380272]
[epoch 101/1000, batch 41/100 -> loss before: 0.6629243018640011, loss after: 0.6392005270072222]
[epoch 101/1000, batch 51/100 -> loss before: 0.25519838077677204, loss after: 0.2476991118329987]
[epoch 101/1000, batch 61/100 -> loss before: 0.5163253988907441, loss after: 0.5160557972116531]
[epoch 101/1000, batch 71/100 -> loss before: 0.1444850255938559, loss after: 0.14440299605921114]
[epoch 101/1000, batch 81/100 -> loss before: 0.2647599386143087, loss after: 0.26413347638595036]
[epoch 101/1000, batch 91/100 -> loss before: 0.24242200389792576, loss after: 0.23782887134864797]
ENDING EPOCH 101/1000 [loss before: 0.2838577284570559, loss after: 0.28310318333384316; epoch time: 0.13049054145812988 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.08839228448742792, loss after: 0.08835277689029541]
[epoch 201/1000, batch 11/100 -> loss before: 0.35592380614292635, loss after: 0.354629317484916]
[epoch 201/1000, batch 21/100 -> loss before: 0.18019122449512087, loss after: 0.17671751074435166]
[epoch 201/1000, batch 31/100 -> loss before: 0.46422940431208304, loss after: 0.4622058980635759]
[epoch 201/1000, batch 41/100 -> loss before: 0.3276714085705753, loss after: 0.3225314925414092]
[epoch 201/1000, batch 51/100 -> loss before: 0.28448244166870296, loss after: 0.2840315068370892]
[epoch 201/1000, batch 61/100 -> loss before: 0.48881208553067623, loss after: 0.4869639707341788]
[epoch 201/1000, batch 71/100 -> loss before: 0.41060037543513683, loss after: 0.41020394540563776]
[epoch 201/1000, batch 81/100 -> loss before: 0.19876365725966216, loss after: 0.19875764781540017]
[epoch 201/1000, batch 91/100 -> loss before: 0.10905790644964578, loss after: 0.10904698277617215]
ENDING EPOCH 201/1000 [loss before: 0.2832342456747971, loss after: 0.2834013704612231; epoch time: 0.1287078857421875 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.19005280173439884, loss after: 0.18972113215096212]
[epoch 301/1000, batch 11/100 -> loss before: 0.2009053453366109, loss after: 0.20042398106415021]
[epoch 301/1000, batch 21/100 -> loss before: 0.3124515253282326, loss after: 0.3071948840828431]
[epoch 301/1000, batch 31/100 -> loss before: 0.33110005054351777, loss after: 0.32431578765538777]
[epoch 301/1000, batch 41/100 -> loss before: 0.2560959535098807, loss after: 0.2540345986257598]
[epoch 301/1000, batch 51/100 -> loss before: 0.2823707722334422, loss after: 0.28236882558835724]
[epoch 301/1000, batch 61/100 -> loss before: 0.3124321325150899, loss after: 0.3075753648883062]
[epoch 301/1000, batch 71/100 -> loss before: 0.30927994269231174, loss after: 0.30785425674626]
[epoch 301/1000, batch 81/100 -> loss before: 0.3229918177714436, loss after: 0.32290434728410167]
[epoch 301/1000, batch 91/100 -> loss before: 0.32024636955946884, loss after: 0.30757186917394724]
ENDING EPOCH 301/1000 [loss before: 0.2832621385522894, loss after: 0.283373067400126; epoch time: 0.12881946563720703 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.2885103203150879, loss after: 0.287715631885097]
[epoch 401/1000, batch 11/100 -> loss before: 0.2749995110484395, loss after: 0.2746041879872573]
[epoch 401/1000, batch 21/100 -> loss before: 0.3333620292412238, loss after: 0.3246681334658076]
[epoch 401/1000, batch 31/100 -> loss before: 0.3851225542398786, loss after: 0.3653503484869697]
[epoch 401/1000, batch 41/100 -> loss before: 0.23566942567488658, loss after: 0.2356135159079816]
[epoch 401/1000, batch 51/100 -> loss before: 0.32606376029223244, loss after: 0.3258135361168121]
[epoch 401/1000, batch 61/100 -> loss before: 0.2591767258967638, loss after: 0.25238273134636396]
[epoch 401/1000, batch 71/100 -> loss before: 0.26760535045462297, loss after: 0.26725839159476417]
[epoch 401/1000, batch 81/100 -> loss before: 0.19747368421744405, loss after: 0.19547578442585314]
[epoch 401/1000, batch 91/100 -> loss before: 0.1938472951826506, loss after: 0.19341979054704653]
ENDING EPOCH 401/1000 [loss before: 0.28417642791597375, loss after: 0.28337407884076254; epoch time: 0.12451052665710449 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.28368219198293787, loss after: 0.28151327475290727]
[epoch 501/1000, batch 11/100 -> loss before: 0.5770731120666112, loss after: 0.5758408844069514]
[epoch 501/1000, batch 21/100 -> loss before: 0.38056774303792285, loss after: 0.37990723150411665]
[epoch 501/1000, batch 31/100 -> loss before: 0.2457150633261796, loss after: 0.24452222832455467]
[epoch 501/1000, batch 41/100 -> loss before: 0.31546357110391154, loss after: 0.31537961734573283]
[epoch 501/1000, batch 51/100 -> loss before: 0.1771607540276292, loss after: 0.1766710032532075]
[epoch 501/1000, batch 61/100 -> loss before: 0.289679744527657, loss after: 0.2896796135941644]
[epoch 501/1000, batch 71/100 -> loss before: 0.1477109336388423, loss after: 0.14768186628102337]
[epoch 501/1000, batch 81/100 -> loss before: 0.24173297170374242, loss after: 0.23613757179646044]
[epoch 501/1000, batch 91/100 -> loss before: 0.21433588783578106, loss after: 0.2141358521391899]
ENDING EPOCH 501/1000 [loss before: 0.28309497178807735, loss after: 0.2834341519309896; epoch time: 0.12805938720703125 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.346850924951596, loss after: 0.3460539579664201]
[epoch 601/1000, batch 11/100 -> loss before: 0.361387194306005, loss after: 0.3559786612992767]
[epoch 601/1000, batch 21/100 -> loss before: 0.29885818852119694, loss after: 0.29807602907156583]
[epoch 601/1000, batch 31/100 -> loss before: 0.14081270311975863, loss after: 0.13875869669933877]
[epoch 601/1000, batch 41/100 -> loss before: 0.3629989993251064, loss after: 0.35521978932056486]
[epoch 601/1000, batch 51/100 -> loss before: 0.28957321009714126, loss after: 0.28503999883465514]
[epoch 601/1000, batch 61/100 -> loss before: 0.13622202072473405, loss after: 0.13621578665662398]
[epoch 601/1000, batch 71/100 -> loss before: 0.2643597361111448, loss after: 0.25494990458612343]
[epoch 601/1000, batch 81/100 -> loss before: 0.34683783414645064, loss after: 0.3370402709274567]
[epoch 601/1000, batch 91/100 -> loss before: 0.3479120840318169, loss after: 0.3478388944629562]
ENDING EPOCH 601/1000 [loss before: 0.2833956139741011, loss after: 0.28432229158131916; epoch time: 0.1321568489074707 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.13962299243672577, loss after: 0.1384577084085173]
[epoch 701/1000, batch 11/100 -> loss before: 0.1545533834873551, loss after: 0.14828610367133482]
[epoch 701/1000, batch 21/100 -> loss before: 0.225711074813507, loss after: 0.22511522319975047]
[epoch 701/1000, batch 31/100 -> loss before: 0.20556680699887528, loss after: 0.2051572492465993]
[epoch 701/1000, batch 41/100 -> loss before: 0.19551396990664424, loss after: 0.19550670206737364]
[epoch 701/1000, batch 51/100 -> loss before: 0.5450987035482041, loss after: 0.5323586457915666]
[epoch 701/1000, batch 61/100 -> loss before: 0.1377579694840974, loss after: 0.13708178338107282]
[epoch 701/1000, batch 71/100 -> loss before: 0.3682814079704399, loss after: 0.36813694565224403]
[epoch 701/1000, batch 81/100 -> loss before: 0.28340384071529623, loss after: 0.28338024756987273]
[epoch 701/1000, batch 91/100 -> loss before: 0.35334519620357124, loss after: 0.35334493570035064]
ENDING EPOCH 701/1000 [loss before: 0.2830798721123215, loss after: 0.28307944289071874; epoch time: 0.12879705429077148 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.2406892234857741, loss after: 0.22778380155142203]
[epoch 801/1000, batch 11/100 -> loss before: 0.11760844987573538, loss after: 0.11737871030982402]
[epoch 801/1000, batch 21/100 -> loss before: 0.23883052627842988, loss after: 0.23796507108625384]
[epoch 801/1000, batch 31/100 -> loss before: 0.21363929156107822, loss after: 0.2131238836931834]
[epoch 801/1000, batch 41/100 -> loss before: 0.47769842685341485, loss after: 0.4736635504111401]
[epoch 801/1000, batch 51/100 -> loss before: 0.26568204298108344, loss after: 0.2648380749480642]
[epoch 801/1000, batch 61/100 -> loss before: 0.26711977557929606, loss after: 0.26463526925347347]
[epoch 801/1000, batch 71/100 -> loss before: 0.32467670407300336, loss after: 0.32323195837845753]
[epoch 801/1000, batch 81/100 -> loss before: 0.2969979666226813, loss after: 0.2922125128831264]
[epoch 801/1000, batch 91/100 -> loss before: 0.24416671123440764, loss after: 0.2434105392437227]
ENDING EPOCH 801/1000 [loss before: 0.28315679312658387, loss after: 0.28307819065694273; epoch time: 0.129624605178833 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.13197457217077277, loss after: 0.12971838246363926]
[epoch 901/1000, batch 11/100 -> loss before: 0.3367077465444299, loss after: 0.329825128584308]
[epoch 901/1000, batch 21/100 -> loss before: 0.24778534877766348, loss after: 0.24318479292123607]
[epoch 901/1000, batch 31/100 -> loss before: 0.2801618432937566, loss after: 0.277197274897514]
[epoch 901/1000, batch 41/100 -> loss before: 0.34114466631647267, loss after: 0.33652693020628754]
[epoch 901/1000, batch 51/100 -> loss before: 0.2544983589828025, loss after: 0.24128583551352775]
[epoch 901/1000, batch 61/100 -> loss before: 0.40920382571008973, loss after: 0.4089749019215951]
[epoch 901/1000, batch 71/100 -> loss before: 0.17221428523217291, loss after: 0.16930840943030254]
[epoch 901/1000, batch 81/100 -> loss before: 0.3866967363759102, loss after: 0.3846334471724273]
[epoch 901/1000, batch 91/100 -> loss before: 0.20810289248062638, loss after: 0.20726870740952666]
ENDING EPOCH 901/1000 [loss before: 0.2834479355602866, loss after: 0.2846663424311277; epoch time: 0.13065457344055176 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.26285397540003563, loss after: 0.25948338075116323]
[epoch 1000/1000, batch 11/100 -> loss before: 0.23019662522349588, loss after: 0.22943103600901954]
[epoch 1000/1000, batch 21/100 -> loss before: 0.32141984485583636, loss after: 0.3165244940503454]
[epoch 1000/1000, batch 31/100 -> loss before: 0.23696069923353794, loss after: 0.23666579704630034]
[epoch 1000/1000, batch 41/100 -> loss before: 0.2945698103018281, loss after: 0.2859640061969857]
[epoch 1000/1000, batch 51/100 -> loss before: 0.3060637886355045, loss after: 0.3002836974128321]
[epoch 1000/1000, batch 61/100 -> loss before: 0.21506176465749732, loss after: 0.2115428163223962]
[epoch 1000/1000, batch 71/100 -> loss before: 0.34756323946258927, loss after: 0.3471945191548659]
[epoch 1000/1000, batch 81/100 -> loss before: 0.10230387884005017, loss after: 0.10227987873376854]
[epoch 1000/1000, batch 91/100 -> loss before: 0.3507114168428621, loss after: 0.34907651777962073]
ENDING EPOCH 1000/1000 [loss before: 0.283564501827511, loss after: 0.28314874219440767; epoch time: 0.12819290161132812 s]
FIT DONE. [time: 121.50117516517639 s]
LOSS TRAIN (MSE): 0.28314874219440767
LOSS TEST (MSE): 0.2775481828542704
R^2 TRAIN: -0.0002492915987246658
R^2 TEST: -6.420220787761721e-05
EXPERIMENT DONE
