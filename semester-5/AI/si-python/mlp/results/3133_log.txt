EXPERIMENT 3133 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[64, 64, 64, 64, 64, 32, 32, 32, 32, 32, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=rmsprop, learning_rate=0.0001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 25185]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.6815884323265685, loss after: 0.6770333699173927]
[epoch 1/1000, batch 11/100 -> loss before: 0.4412160954531329, loss after: 0.44058362299246534]
[epoch 1/1000, batch 21/100 -> loss before: 0.7528390519828022, loss after: 0.7513179479413147]
[epoch 1/1000, batch 31/100 -> loss before: 0.5983693758274079, loss after: 0.5974945699769943]
[epoch 1/1000, batch 41/100 -> loss before: 0.6142353631201861, loss after: 0.6127341698669546]
[epoch 1/1000, batch 51/100 -> loss before: 0.6844944650736556, loss after: 0.6827420454449641]
[epoch 1/1000, batch 61/100 -> loss before: 0.717005515818768, loss after: 0.7159748192474612]
[epoch 1/1000, batch 71/100 -> loss before: 0.6577677280471534, loss after: 0.6564976915191101]
[epoch 1/1000, batch 81/100 -> loss before: 0.6955496350170501, loss after: 0.6943573956965137]
[epoch 1/1000, batch 91/100 -> loss before: 0.43709489102920795, loss after: 0.4364488004123408]
ENDING EPOCH 1/1000 [loss before: 0.599648552700766, loss after: 0.4931774015730349; epoch time: 0.16641640663146973 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.2865166754558324, loss after: 0.28646028131989876]
[epoch 101/1000, batch 11/100 -> loss before: 0.22106063375470336, loss after: 0.2210450323801883]
[epoch 101/1000, batch 21/100 -> loss before: 0.2434098507452827, loss after: 0.24333243561577564]
[epoch 101/1000, batch 31/100 -> loss before: 0.387131395102425, loss after: 0.3868134876551188]
[epoch 101/1000, batch 41/100 -> loss before: 0.6516636425374266, loss after: 0.6495349983092655]
[epoch 101/1000, batch 51/100 -> loss before: 0.2671234085440391, loss after: 0.26628628894373546]
[epoch 101/1000, batch 61/100 -> loss before: 0.5155134680139699, loss after: 0.5154968071950397]
[epoch 101/1000, batch 71/100 -> loss before: 0.14609912469268516, loss after: 0.14607069147704196]
[epoch 101/1000, batch 81/100 -> loss before: 0.26674163873327295, loss after: 0.26665051924222805]
[epoch 101/1000, batch 91/100 -> loss before: 0.24571606350914146, loss after: 0.24524293417970439]
ENDING EPOCH 101/1000 [loss before: 0.2830782716148651, loss after: 0.2830781768431628; epoch time: 0.13373422622680664 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.08808755635656504, loss after: 0.08808726329635222]
[epoch 201/1000, batch 11/100 -> loss before: 0.3581373644265765, loss after: 0.3579735546204585]
[epoch 201/1000, batch 21/100 -> loss before: 0.1946030583802759, loss after: 0.1940976945897001]
[epoch 201/1000, batch 31/100 -> loss before: 0.45966788217581456, loss after: 0.45950667451501614]
[epoch 201/1000, batch 41/100 -> loss before: 0.31657463551343135, loss after: 0.3161786092268556]
[epoch 201/1000, batch 51/100 -> loss before: 0.28441829215311204, loss after: 0.2843707852616869]
[epoch 201/1000, batch 61/100 -> loss before: 0.4836447749192816, loss after: 0.48351244249344605]
[epoch 201/1000, batch 71/100 -> loss before: 0.4161288169332396, loss after: 0.4160235433698797]
[epoch 201/1000, batch 81/100 -> loss before: 0.19907132991671467, loss after: 0.19906777429740977]
[epoch 201/1000, batch 91/100 -> loss before: 0.10907143333730829, loss after: 0.10907010040931253]
ENDING EPOCH 201/1000 [loss before: 0.2830788358789269, loss after: 0.28307818048221794; epoch time: 0.1427159309387207 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.18853494127381623, loss after: 0.18851556338138392]
[epoch 301/1000, batch 11/100 -> loss before: 0.20129718310105998, loss after: 0.2012408148315024]
[epoch 301/1000, batch 21/100 -> loss before: 0.30586933823754964, loss after: 0.3054108229691582]
[epoch 301/1000, batch 31/100 -> loss before: 0.3238999931122185, loss after: 0.3232676957030172]
[epoch 301/1000, batch 41/100 -> loss before: 0.2627469794920597, loss after: 0.2624454225551884]
[epoch 301/1000, batch 51/100 -> loss before: 0.28246470788133954, loss after: 0.2824632594703087]
[epoch 301/1000, batch 61/100 -> loss before: 0.31112051400400337, loss after: 0.31062278392098075]
[epoch 301/1000, batch 71/100 -> loss before: 0.31133757926374045, loss after: 0.3111632657831032]
[epoch 301/1000, batch 81/100 -> loss before: 0.3224883887505618, loss after: 0.322484979275373]
[epoch 301/1000, batch 91/100 -> loss before: 0.31475967337009436, loss after: 0.3134875366606261]
ENDING EPOCH 301/1000 [loss before: 0.2830796795837403, loss after: 0.28307823197166687; epoch time: 0.1523292064666748 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.28401357419388207, loss after: 0.2839812828471141]
[epoch 401/1000, batch 11/100 -> loss before: 0.27622573662180566, loss after: 0.27616687393032296]
[epoch 401/1000, batch 21/100 -> loss before: 0.34344357703355827, loss after: 0.3424315857426957]
[epoch 401/1000, batch 31/100 -> loss before: 0.37419794270306383, loss after: 0.3722225815880678]
[epoch 401/1000, batch 41/100 -> loss before: 0.2354882492729858, loss after: 0.23548449491099918]
[epoch 401/1000, batch 51/100 -> loss before: 0.33064763165907235, loss after: 0.3305746354992346]
[epoch 401/1000, batch 61/100 -> loss before: 0.2616613727618413, loss after: 0.2609216705529373]
[epoch 401/1000, batch 71/100 -> loss before: 0.26574862596636556, loss after: 0.2657362707042374]
[epoch 401/1000, batch 81/100 -> loss before: 0.1886739469901606, loss after: 0.18858552993026778]
[epoch 401/1000, batch 91/100 -> loss before: 0.1933465004576283, loss after: 0.1933085541366693]
ENDING EPOCH 401/1000 [loss before: 0.28307932617469045, loss after: 0.2830834630646486; epoch time: 0.12914299964904785 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.2843100135493991, loss after: 0.2840728999840857]
[epoch 501/1000, batch 11/100 -> loss before: 0.5794059671310748, loss after: 0.57924698721274]
[epoch 501/1000, batch 21/100 -> loss before: 0.37981965278028157, loss after: 0.3797602239306277]
[epoch 501/1000, batch 31/100 -> loss before: 0.24559055881603853, loss after: 0.24546621486689996]
[epoch 501/1000, batch 41/100 -> loss before: 0.31527701918889245, loss after: 0.31527045767059236]
[epoch 501/1000, batch 51/100 -> loss before: 0.1770738810994269, loss after: 0.17702320327486004]
[epoch 501/1000, batch 61/100 -> loss before: 0.2896946344501059, loss after: 0.28969443906868286]
[epoch 501/1000, batch 71/100 -> loss before: 0.1475695943638599, loss after: 0.14756835338820656]
[epoch 501/1000, batch 81/100 -> loss before: 0.2376907590736658, loss after: 0.23714632516818038]
[epoch 501/1000, batch 91/100 -> loss before: 0.21390245499348848, loss after: 0.2138876615988396]
ENDING EPOCH 501/1000 [loss before: 0.2830811673588978, loss after: 0.2830781876816494; epoch time: 0.1374666690826416 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.3436829650055794, loss after: 0.3436363061451625]
[epoch 601/1000, batch 11/100 -> loss before: 0.3434244134892743, loss after: 0.34313399942280587]
[epoch 601/1000, batch 21/100 -> loss before: 0.2961086585153281, loss after: 0.29606190590865167]
[epoch 601/1000, batch 31/100 -> loss before: 0.13425944296099643, loss after: 0.13411740178144488]
[epoch 601/1000, batch 41/100 -> loss before: 0.3760774011919253, loss after: 0.37515995832420745]
[epoch 601/1000, batch 51/100 -> loss before: 0.2823286290874042, loss after: 0.281937643339143]
[epoch 601/1000, batch 61/100 -> loss before: 0.13771062788450059, loss after: 0.1376915290016487]
[epoch 601/1000, batch 71/100 -> loss before: 0.2673125832877099, loss after: 0.26629619670434196]
[epoch 601/1000, batch 81/100 -> loss before: 0.3478739623792562, loss after: 0.34684411712173835]
[epoch 601/1000, batch 91/100 -> loss before: 0.3473280868891037, loss after: 0.34732767248240826]
ENDING EPOCH 601/1000 [loss before: 0.2830908662454828, loss after: 0.2830959930122135; epoch time: 0.1529247760772705 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.14001716363466218, loss after: 0.13988824200451797]
[epoch 701/1000, batch 11/100 -> loss before: 0.1753592675267481, loss after: 0.17453596559274706]
[epoch 701/1000, batch 21/100 -> loss before: 0.2324962929706112, loss after: 0.23235412665702843]
[epoch 701/1000, batch 31/100 -> loss before: 0.20834082082234398, loss after: 0.20825562638758582]
[epoch 701/1000, batch 41/100 -> loss before: 0.19804033694464993, loss after: 0.1980113255607913]
[epoch 701/1000, batch 51/100 -> loss before: 0.5431208641686771, loss after: 0.541803977706701]
[epoch 701/1000, batch 61/100 -> loss before: 0.13582697857943668, loss after: 0.13578340126028968]
[epoch 701/1000, batch 71/100 -> loss before: 0.3678037446640283, loss after: 0.3677952468885683]
[epoch 701/1000, batch 81/100 -> loss before: 0.28447123922772916, loss after: 0.28445198708802766]
[epoch 701/1000, batch 91/100 -> loss before: 0.3539375037325023, loss after: 0.3539279237068813]
ENDING EPOCH 701/1000 [loss before: 0.2830787159144814, loss after: 0.2830809539232979; epoch time: 0.13416171073913574 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.2469311596694579, loss after: 0.24552513863851191]
[epoch 801/1000, batch 11/100 -> loss before: 0.1175487920193091, loss after: 0.11752521103598516]
[epoch 801/1000, batch 21/100 -> loss before: 0.24301260063869798, loss after: 0.24286195594827364]
[epoch 801/1000, batch 31/100 -> loss before: 0.21582013593677404, loss after: 0.21574200430379414]
[epoch 801/1000, batch 41/100 -> loss before: 0.46737322048756313, loss after: 0.46707602237441065]
[epoch 801/1000, batch 51/100 -> loss before: 0.26734085373084227, loss after: 0.2672315036865884]
[epoch 801/1000, batch 61/100 -> loss before: 0.2737119022643929, loss after: 0.27337307632045665]
[epoch 801/1000, batch 71/100 -> loss before: 0.32383942390287856, loss after: 0.32369785083540503]
[epoch 801/1000, batch 81/100 -> loss before: 0.29283169072994947, loss after: 0.29238399839294915]
[epoch 801/1000, batch 91/100 -> loss before: 0.24581731846317895, loss after: 0.24571487391002983]
ENDING EPOCH 801/1000 [loss before: 0.28307819720739025, loss after: 0.28307828101522853; epoch time: 0.13674712181091309 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.13698441497895905, loss after: 0.1366770907933248]
[epoch 901/1000, batch 11/100 -> loss before: 0.3248999955350099, loss after: 0.3243090809940582]
[epoch 901/1000, batch 21/100 -> loss before: 0.25701859342575517, loss after: 0.256423618875167]
[epoch 901/1000, batch 31/100 -> loss before: 0.2929122681976359, loss after: 0.2924412457729768]
[epoch 901/1000, batch 41/100 -> loss before: 0.34738536836559797, loss after: 0.346826611567496]
[epoch 901/1000, batch 51/100 -> loss before: 0.24415929760502103, loss after: 0.24287896170642248]
[epoch 901/1000, batch 61/100 -> loss before: 0.40755861131870486, loss after: 0.40755858950357415]
[epoch 901/1000, batch 71/100 -> loss before: 0.17657187128529744, loss after: 0.17620684961341354]
[epoch 901/1000, batch 81/100 -> loss before: 0.39139834364600523, loss after: 0.3911194790768514]
[epoch 901/1000, batch 91/100 -> loss before: 0.2116265568350355, loss after: 0.21150108157520814]
ENDING EPOCH 901/1000 [loss before: 0.28307857356362537, loss after: 0.28307857216329757; epoch time: 0.13007712364196777 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.25575537870812814, loss after: 0.25548340881585774]
[epoch 1000/1000, batch 11/100 -> loss before: 0.23229978570812962, loss after: 0.2321905575255096]
[epoch 1000/1000, batch 21/100 -> loss before: 0.31168248746667615, loss after: 0.311307692703637]
[epoch 1000/1000, batch 31/100 -> loss before: 0.2362641432003997, loss after: 0.23624119312726077]
[epoch 1000/1000, batch 41/100 -> loss before: 0.2982892312246518, loss after: 0.297338828295653]
[epoch 1000/1000, batch 51/100 -> loss before: 0.29613202488435575, loss after: 0.29565219517689173]
[epoch 1000/1000, batch 61/100 -> loss before: 0.21213958670057628, loss after: 0.21180383202620537]
[epoch 1000/1000, batch 71/100 -> loss before: 0.34996769600980515, loss after: 0.34990048869662105]
[epoch 1000/1000, batch 81/100 -> loss before: 0.10212521314792944, loss after: 0.10212493160418046]
[epoch 1000/1000, batch 91/100 -> loss before: 0.3578945032323353, loss after: 0.3576471809111982]
ENDING EPOCH 1000/1000 [loss before: 0.2830783866384619, loss after: 0.28307817398609014; epoch time: 0.13065505027770996 s]
FIT DONE. [time: 129.10213088989258 s]
LOSS TRAIN (MSE): 0.28307817398609014
LOSS TEST (MSE): 0.2776889575109352
R^2 TRAIN: -2.8332796109253877e-09
R^2 TEST: -0.0005714427643126463
EXPERIMENT DONE
