EXPERIMENT 2243 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[64, 64, 64, 64, 64, 32, 32, 32, 32, 32, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8], activation_name=relu, targets_activation_name=linear, 
initialization_name=uniform, algo_name=adam, learning_rate=0.001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 25185]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.18318931763552243, loss after: 0.1901147708717404]
[epoch 1/1000, batch 11/100 -> loss before: 0.2595433864447139, loss after: 0.26898214983120716]
[epoch 1/1000, batch 21/100 -> loss before: 0.35493004407860995, loss after: 0.35492517450372]
[epoch 1/1000, batch 31/100 -> loss before: 0.3293441094107332, loss after: 0.32246767473036897]
[epoch 1/1000, batch 41/100 -> loss before: 0.23270189204292607, loss after: 0.22851297772408788]
[epoch 1/1000, batch 51/100 -> loss before: 0.24270364812182638, loss after: 0.24309142369537814]
[epoch 1/1000, batch 61/100 -> loss before: 0.46857442134026106, loss after: 0.4683427832972584]
[epoch 1/1000, batch 71/100 -> loss before: 0.3292092556031879, loss after: 0.32553684294479923]
[epoch 1/1000, batch 81/100 -> loss before: 0.40308025643439593, loss after: 0.40201130362703275]
[epoch 1/1000, batch 91/100 -> loss before: 0.32487752704548617, loss after: 0.3221712469886782]
ENDING EPOCH 1/1000 [loss before: 0.2899884218128069, loss after: 0.2826715520355955; epoch time: 0.18710088729858398 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.1998930226117653, loss after: 0.19968244470243438]
[epoch 101/1000, batch 11/100 -> loss before: 0.16511260997893754, loss after: 0.1650137756234488]
[epoch 101/1000, batch 21/100 -> loss before: 0.16131030429777368, loss after: 0.16086253367803788]
[epoch 101/1000, batch 31/100 -> loss before: 0.27604958780590827, loss after: 0.275334789660041]
[epoch 101/1000, batch 41/100 -> loss before: 0.5285856744433246, loss after: 0.5280762612746066]
[epoch 101/1000, batch 51/100 -> loss before: 0.23322476735062062, loss after: 0.2329495272035524]
[epoch 101/1000, batch 61/100 -> loss before: 0.5263547934159034, loss after: 0.526312721432933]
[epoch 101/1000, batch 71/100 -> loss before: 0.1602391662468032, loss after: 0.1592446795272184]
[epoch 101/1000, batch 81/100 -> loss before: 0.28322949165209305, loss after: 0.2832109356741248]
[epoch 101/1000, batch 91/100 -> loss before: 0.21919123647559663, loss after: 0.2190090301259675]
ENDING EPOCH 101/1000 [loss before: 0.23153068500858093, loss after: 0.23156045939342257; epoch time: 0.15917611122131348 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.09365102296999438, loss after: 0.08996893544444742]
[epoch 201/1000, batch 11/100 -> loss before: 0.21926217191129255, loss after: 0.21862260135251615]
[epoch 201/1000, batch 21/100 -> loss before: 0.23422615329673296, loss after: 0.2339720968655566]
[epoch 201/1000, batch 31/100 -> loss before: 0.3815423884905159, loss after: 0.3822919344717103]
[epoch 201/1000, batch 41/100 -> loss before: 0.2630019373113914, loss after: 0.26267563561300067]
[epoch 201/1000, batch 51/100 -> loss before: 0.20530222037371826, loss after: 0.20531869002919537]
[epoch 201/1000, batch 61/100 -> loss before: 0.5035625733190029, loss after: 0.5036138143738252]
[epoch 201/1000, batch 71/100 -> loss before: 0.4078184560921798, loss after: 0.40780373134172276]
[epoch 201/1000, batch 81/100 -> loss before: 0.13554245193126424, loss after: 0.1351353276385742]
[epoch 201/1000, batch 91/100 -> loss before: 0.05993367508725248, loss after: 0.05937344054104323]
ENDING EPOCH 201/1000 [loss before: 0.23916112166544412, loss after: 0.23236703279049098; epoch time: 0.1609792709350586 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.18777155791585667, loss after: 0.1877644665442131]
[epoch 301/1000, batch 11/100 -> loss before: 0.03894261280425163, loss after: 0.0386710702472362]
[epoch 301/1000, batch 21/100 -> loss before: 0.2599315046360814, loss after: 0.25982237289566773]
[epoch 301/1000, batch 31/100 -> loss before: 0.20100672153708737, loss after: 0.20070454828138146]
[epoch 301/1000, batch 41/100 -> loss before: 0.19055035940812987, loss after: 0.1901488409560858]
[epoch 301/1000, batch 51/100 -> loss before: 0.2868143629907751, loss after: 0.2868345496190342]
[epoch 301/1000, batch 61/100 -> loss before: 0.2849321826504846, loss after: 0.28493086548220814]
[epoch 301/1000, batch 71/100 -> loss before: 0.25279617808380056, loss after: 0.2527498968366365]
[epoch 301/1000, batch 81/100 -> loss before: 0.3311826203348908, loss after: 0.3311888324354152]
[epoch 301/1000, batch 91/100 -> loss before: 0.2606516222283413, loss after: 0.2600162821142688]
ENDING EPOCH 301/1000 [loss before: 0.23080495859051453, loss after: 0.23086122635341483; epoch time: 0.1600940227508545 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.2818277255585141, loss after: 0.2818102628653431]
[epoch 401/1000, batch 11/100 -> loss before: 0.16307927580901496, loss after: 0.16277003581676747]
[epoch 401/1000, batch 21/100 -> loss before: 0.22521575534910016, loss after: 0.2251228209586642]
[epoch 401/1000, batch 31/100 -> loss before: 0.16305560153217602, loss after: 0.16313126664954417]
[epoch 401/1000, batch 41/100 -> loss before: 0.15635706811660277, loss after: 0.1562865704179821]
[epoch 401/1000, batch 51/100 -> loss before: 0.13441197029258042, loss after: 0.13328415890387119]
[epoch 401/1000, batch 61/100 -> loss before: 0.1659528699660065, loss after: 0.16586767206712488]
[epoch 401/1000, batch 71/100 -> loss before: 0.17896103790819512, loss after: 0.17884504422728698]
[epoch 401/1000, batch 81/100 -> loss before: 0.1635358860868355, loss after: 0.16358006887580503]
[epoch 401/1000, batch 91/100 -> loss before: 0.2067870979970076, loss after: 0.20675362877507864]
ENDING EPOCH 401/1000 [loss before: 0.23045265297615275, loss after: 0.23061362700461221; epoch time: 0.1604905128479004 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.1703498167467533, loss after: 0.1703156153737463]
[epoch 501/1000, batch 11/100 -> loss before: 0.42716377566220604, loss after: 0.4270896172055808]
[epoch 501/1000, batch 21/100 -> loss before: 0.2992509169324517, loss after: 0.2992372909424076]
[epoch 501/1000, batch 31/100 -> loss before: 0.23046875867859287, loss after: 0.23048109354467003]
[epoch 501/1000, batch 41/100 -> loss before: 0.3183597521067047, loss after: 0.31834656762613883]
[epoch 501/1000, batch 51/100 -> loss before: 0.11078742326345208, loss after: 0.11073012421453443]
[epoch 501/1000, batch 61/100 -> loss before: 0.10699430586491139, loss after: 0.10701391921719675]
[epoch 501/1000, batch 71/100 -> loss before: 0.13681550068775228, loss after: 0.13692774701582391]
[epoch 501/1000, batch 81/100 -> loss before: 0.20870591950709402, loss after: 0.20872658421928708]
[epoch 501/1000, batch 91/100 -> loss before: 0.17164062375417496, loss after: 0.17155205443698338]
ENDING EPOCH 501/1000 [loss before: 0.2304088761386672, loss after: 0.2311668500004472; epoch time: 0.15932321548461914 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.36187015593547456, loss after: 0.36186734535540055]
[epoch 601/1000, batch 11/100 -> loss before: 0.14847318649477384, loss after: 0.1480475386197994]
[epoch 601/1000, batch 21/100 -> loss before: 0.19041878299389886, loss after: 0.19032761892157335]
[epoch 601/1000, batch 31/100 -> loss before: 0.1170864677646141, loss after: 0.1170340549238645]
[epoch 601/1000, batch 41/100 -> loss before: 0.33173994427577547, loss after: 0.3316281842487486]
[epoch 601/1000, batch 51/100 -> loss before: 0.3195178329365108, loss after: 0.3195263248701859]
[epoch 601/1000, batch 61/100 -> loss before: 0.10195494872525024, loss after: 0.10185885231552953]
[epoch 601/1000, batch 71/100 -> loss before: 0.3263575112760926, loss after: 0.3261302304767293]
[epoch 601/1000, batch 81/100 -> loss before: 0.2991651344741363, loss after: 0.2991031976654668]
[epoch 601/1000, batch 91/100 -> loss before: 0.2516183366189445, loss after: 0.25163084854601914]
ENDING EPOCH 601/1000 [loss before: 0.23032865262251964, loss after: 0.23043363054487775; epoch time: 0.19587492942810059 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.10744349892563188, loss after: 0.10744383595965187]
[epoch 701/1000, batch 11/100 -> loss before: 0.22994793478836123, loss after: 0.22947645680376189]
[epoch 701/1000, batch 21/100 -> loss before: 0.1991220023126164, loss after: 0.1989602499192445]
[epoch 701/1000, batch 31/100 -> loss before: 0.22333308323201653, loss after: 0.2233196828027723]
[epoch 701/1000, batch 41/100 -> loss before: 0.1348817408248262, loss after: 0.13478333826127273]
[epoch 701/1000, batch 51/100 -> loss before: 0.3132132651494901, loss after: 0.3129602800538766]
[epoch 701/1000, batch 61/100 -> loss before: 0.15041831843345918, loss after: 0.15043349365958608]
[epoch 701/1000, batch 71/100 -> loss before: 0.19343016711039715, loss after: 0.19329292182141514]
[epoch 701/1000, batch 81/100 -> loss before: 0.21769915105956997, loss after: 0.2176262460736309]
[epoch 701/1000, batch 91/100 -> loss before: 0.3467060978198738, loss after: 0.34639937467886084]
ENDING EPOCH 701/1000 [loss before: 0.23071382450881783, loss after: 0.230636187674794; epoch time: 0.1566159725189209 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.18732820321058702, loss after: 0.18715075218166047]
[epoch 801/1000, batch 11/100 -> loss before: 0.12457073389194669, loss after: 0.12436643027203154]
[epoch 801/1000, batch 21/100 -> loss before: 0.1844570966648664, loss after: 0.18221904774580916]
[epoch 801/1000, batch 31/100 -> loss before: 0.17451015743428652, loss after: 0.1727087473351876]
[epoch 801/1000, batch 41/100 -> loss before: 0.2920146928174063, loss after: 0.29209415806771877]
[epoch 801/1000, batch 51/100 -> loss before: 0.1324112918652794, loss after: 0.13147714177733327]
[epoch 801/1000, batch 61/100 -> loss before: 0.2498722565893233, loss after: 0.2501182680460657]
[epoch 801/1000, batch 71/100 -> loss before: 0.23346593536805216, loss after: 0.23235977016759596]
[epoch 801/1000, batch 81/100 -> loss before: 0.17123357483056548, loss after: 0.16997526806644792]
[epoch 801/1000, batch 91/100 -> loss before: 0.25156360435990455, loss after: 0.251414094046175]
ENDING EPOCH 801/1000 [loss before: 0.21831173272022725, loss after: 0.2118779506884229; epoch time: 0.1616804599761963 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.09493361831668112, loss after: 0.08943701171923016]
[epoch 901/1000, batch 11/100 -> loss before: 0.19410241892177502, loss after: 0.1923314874642162]
[epoch 901/1000, batch 21/100 -> loss before: 0.03239871424093251, loss after: 0.03235044912230166]
[epoch 901/1000, batch 31/100 -> loss before: 0.03759426986189259, loss after: 0.03674670838559172]
[epoch 901/1000, batch 41/100 -> loss before: 0.2164695065986204, loss after: 0.2173742922514507]
[epoch 901/1000, batch 51/100 -> loss before: 0.19277023283680608, loss after: 0.18846892004504057]
[epoch 901/1000, batch 61/100 -> loss before: 0.3403440518602809, loss after: 0.34039074349787785]
[epoch 901/1000, batch 71/100 -> loss before: 0.1962129546671346, loss after: 0.19499539522478423]
[epoch 901/1000, batch 81/100 -> loss before: 0.3498976915609593, loss after: 0.3495271693458499]
[epoch 901/1000, batch 91/100 -> loss before: 0.14076722018262394, loss after: 0.14087129886103883]
ENDING EPOCH 901/1000 [loss before: 0.17534118638108626, loss after: 0.174277377789784; epoch time: 0.15791749954223633 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.211489488288995, loss after: 0.21141377069982464]
[epoch 1000/1000, batch 11/100 -> loss before: 0.09889433797117614, loss after: 0.09879165411059473]
[epoch 1000/1000, batch 21/100 -> loss before: 0.1385637468234072, loss after: 0.13831630712317733]
[epoch 1000/1000, batch 31/100 -> loss before: 0.2842209845105793, loss after: 0.28393083032927585]
[epoch 1000/1000, batch 41/100 -> loss before: 0.2949578336156577, loss after: 0.29442219150424914]
[epoch 1000/1000, batch 51/100 -> loss before: 0.1491640430125183, loss after: 0.14613684654597928]
[epoch 1000/1000, batch 61/100 -> loss before: 0.1639416307898084, loss after: 0.16509696975461263]
[epoch 1000/1000, batch 71/100 -> loss before: 0.40709021680738317, loss after: 0.4069353767706591]
[epoch 1000/1000, batch 81/100 -> loss before: 0.041424985498614605, loss after: 0.04139764943931543]
[epoch 1000/1000, batch 91/100 -> loss before: 0.31383985031030126, loss after: 0.31404613373905865]
ENDING EPOCH 1000/1000 [loss before: 0.20965311959384683, loss after: 0.19902219137995963; epoch time: 0.15717792510986328 s]
FIT DONE. [time: 169.0488679409027 s]
LOSS TRAIN (MSE): 0.19902219137995963
LOSS TEST (MSE): 0.1916052634798444
R^2 TRAIN: 0.2969355809338212
R^2 TEST: 0.30960612679848953
EXPERIMENT DONE
