EXPERIMENT 1123 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[64, 64, 64, 64, 64, 32, 32, 32, 32, 32, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=sgd_momentum, learning_rate=0.01, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 25185]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.6815884323265685, loss after: 0.6135316305343468]
[epoch 1/1000, batch 11/100 -> loss before: 0.6631920090536385, loss after: 0.6294581963318743]
[epoch 1/1000, batch 21/100 -> loss before: 0.4837629468880058, loss after: 0.5017569379094036]
[epoch 1/1000, batch 31/100 -> loss before: 0.3893754916634785, loss after: 0.4000794882249009]
[epoch 1/1000, batch 41/100 -> loss before: 0.20097923281725558, loss after: 0.2063371495546496]
[epoch 1/1000, batch 51/100 -> loss before: 0.3344689071998177, loss after: 0.31082598561875374]
[epoch 1/1000, batch 61/100 -> loss before: 0.4738465316510883, loss after: 0.470161715106246]
[epoch 1/1000, batch 71/100 -> loss before: 0.34592529636824565, loss after: 0.34524419063848033]
[epoch 1/1000, batch 81/100 -> loss before: 0.41218746071901763, loss after: 0.40756452291387407]
[epoch 1/1000, batch 91/100 -> loss before: 0.2941967160670189, loss after: 0.2937399520854339]
ENDING EPOCH 1/1000 [loss before: 0.599648552700766, loss after: 0.2851777402249914; epoch time: 0.09973621368408203 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.3081274096355782, loss after: 0.3080322479110746]
[epoch 101/1000, batch 11/100 -> loss before: 0.2212093458785819, loss after: 0.22065418991085956]
[epoch 101/1000, batch 21/100 -> loss before: 0.23688382922303938, loss after: 0.23708257434296365]
[epoch 101/1000, batch 31/100 -> loss before: 0.40961272130219906, loss after: 0.40327879763132624]
[epoch 101/1000, batch 41/100 -> loss before: 0.6160056040622663, loss after: 0.6074509300901985]
[epoch 101/1000, batch 51/100 -> loss before: 0.22514013850441153, loss after: 0.22450434324702506]
[epoch 101/1000, batch 61/100 -> loss before: 0.5156734601001338, loss after: 0.5146324644886179]
[epoch 101/1000, batch 71/100 -> loss before: 0.14815398472074268, loss after: 0.14830808843703058]
[epoch 101/1000, batch 81/100 -> loss before: 0.2638713472883812, loss after: 0.26464419551345786]
[epoch 101/1000, batch 91/100 -> loss before: 0.21231102733184945, loss after: 0.21222251126677577]
ENDING EPOCH 101/1000 [loss before: 0.29280339425570157, loss after: 0.28449093321130065; epoch time: 0.10613679885864258 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.08879227777335805, loss after: 0.08892800358999786]
[epoch 201/1000, batch 11/100 -> loss before: 0.3555771373332326, loss after: 0.35511175215161933]
[epoch 201/1000, batch 21/100 -> loss before: 0.16680584596101006, loss after: 0.1649169642571004]
[epoch 201/1000, batch 31/100 -> loss before: 0.458668938845629, loss after: 0.4570544605478258]
[epoch 201/1000, batch 41/100 -> loss before: 0.34890865064984844, loss after: 0.3501845445479032]
[epoch 201/1000, batch 51/100 -> loss before: 0.29047722703046436, loss after: 0.2883381284485156]
[epoch 201/1000, batch 61/100 -> loss before: 0.4801821638507085, loss after: 0.48111475653661956]
[epoch 201/1000, batch 71/100 -> loss before: 0.40964803212657275, loss after: 0.4085499349024495]
[epoch 201/1000, batch 81/100 -> loss before: 0.19973317989774897, loss after: 0.19891873458241116]
[epoch 201/1000, batch 91/100 -> loss before: 0.11162466609756061, loss after: 0.11115806590183179]
ENDING EPOCH 201/1000 [loss before: 0.2835346744685989, loss after: 0.28375149750235196; epoch time: 0.1031186580657959 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.18687731200769175, loss after: 0.18708288943062196]
[epoch 301/1000, batch 11/100 -> loss before: 0.19882999906664345, loss after: 0.1987629518517586]
[epoch 301/1000, batch 21/100 -> loss before: 0.3170341539992325, loss after: 0.3156062367931334]
[epoch 301/1000, batch 31/100 -> loss before: 0.34321658886139683, loss after: 0.34380637875855474]
[epoch 301/1000, batch 41/100 -> loss before: 0.25859180872757487, loss after: 0.2552441560901638]
[epoch 301/1000, batch 51/100 -> loss before: 0.28626678492878993, loss after: 0.2849400107575343]
[epoch 301/1000, batch 61/100 -> loss before: 0.303849124642074, loss after: 0.3013395165510994]
[epoch 301/1000, batch 71/100 -> loss before: 0.30753614374194044, loss after: 0.3070007429582628]
[epoch 301/1000, batch 81/100 -> loss before: 0.32615342216596, loss after: 0.3264176322587137]
[epoch 301/1000, batch 91/100 -> loss before: 0.32567214606884776, loss after: 0.31905143133511554]
ENDING EPOCH 301/1000 [loss before: 0.28390083166425273, loss after: 0.2834387842139691; epoch time: 0.10413432121276855 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.2855425377000896, loss after: 0.28823807017981407]
[epoch 401/1000, batch 11/100 -> loss before: 0.27223712735689554, loss after: 0.2721122807471894]
[epoch 401/1000, batch 21/100 -> loss before: 0.3270220881221703, loss after: 0.3273538070655159]
[epoch 401/1000, batch 31/100 -> loss before: 0.45866411507638577, loss after: 0.4531907707416735]
[epoch 401/1000, batch 41/100 -> loss before: 0.23665086228033397, loss after: 0.23833639720564864]
[epoch 401/1000, batch 51/100 -> loss before: 0.3263506037623954, loss after: 0.3256128855629978]
[epoch 401/1000, batch 61/100 -> loss before: 0.25984664900065446, loss after: 0.2674411660337228]
[epoch 401/1000, batch 71/100 -> loss before: 0.28213929121875414, loss after: 0.28338765060985477]
[epoch 401/1000, batch 81/100 -> loss before: 0.22696290715201656, loss after: 0.22333150259055284]
[epoch 401/1000, batch 91/100 -> loss before: 0.1931722380489397, loss after: 0.19205212563983687]
ENDING EPOCH 401/1000 [loss before: 0.28326664373053817, loss after: 0.2836815420066258; epoch time: 0.1022481918334961 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.2900803938808331, loss after: 0.2881443005457242]
[epoch 501/1000, batch 11/100 -> loss before: 0.5780528866946748, loss after: 0.5764888296934247]
[epoch 501/1000, batch 21/100 -> loss before: 0.37749247442362294, loss after: 0.37779492215434585]
[epoch 501/1000, batch 31/100 -> loss before: 0.23985878246146602, loss after: 0.239932508255524]
[epoch 501/1000, batch 41/100 -> loss before: 0.31480855808067665, loss after: 0.31493449365395293]
[epoch 501/1000, batch 51/100 -> loss before: 0.17440947104151167, loss after: 0.17453732651357892]
[epoch 501/1000, batch 61/100 -> loss before: 0.2900545929362649, loss after: 0.2899575700583546]
[epoch 501/1000, batch 71/100 -> loss before: 0.14915363148688582, loss after: 0.14955228778012966]
[epoch 501/1000, batch 81/100 -> loss before: 0.2460030162706865, loss after: 0.24226513864342203]
[epoch 501/1000, batch 91/100 -> loss before: 0.2157669603000635, loss after: 0.21613111106129343]
ENDING EPOCH 501/1000 [loss before: 0.2834144534715812, loss after: 0.28667762042026407; epoch time: 0.11218047142028809 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.34797051983411187, loss after: 0.34985494757554836]
[epoch 601/1000, batch 11/100 -> loss before: 0.3900670801019457, loss after: 0.39040348593626517]
[epoch 601/1000, batch 21/100 -> loss before: 0.3055012654408325, loss after: 0.30230410626475357]
[epoch 601/1000, batch 31/100 -> loss before: 0.17110788591089748, loss after: 0.1744257547826795]
[epoch 601/1000, batch 41/100 -> loss before: 0.3884547910943323, loss after: 0.3717183173897582]
[epoch 601/1000, batch 51/100 -> loss before: 0.32062357042448825, loss after: 0.31850631358182835]
[epoch 601/1000, batch 61/100 -> loss before: 0.1362187881337333, loss after: 0.13619343595872452]
[epoch 601/1000, batch 71/100 -> loss before: 0.2527033322276991, loss after: 0.24609856617440906]
[epoch 601/1000, batch 81/100 -> loss before: 0.3530877992498542, loss after: 0.34626458675851024]
[epoch 601/1000, batch 91/100 -> loss before: 0.34731067484607325, loss after: 0.3474412263930188]
ENDING EPOCH 601/1000 [loss before: 0.2836628975703398, loss after: 0.2892480672654843; epoch time: 0.11135339736938477 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.14584251645320417, loss after: 0.14472560283336938]
[epoch 701/1000, batch 11/100 -> loss before: 0.16241567494890713, loss after: 0.15305065841742727]
[epoch 701/1000, batch 21/100 -> loss before: 0.22090010598382204, loss after: 0.2209016300901349]
[epoch 701/1000, batch 31/100 -> loss before: 0.20399836322428228, loss after: 0.20466047773102938]
[epoch 701/1000, batch 41/100 -> loss before: 0.20048632876578268, loss after: 0.20248206183916265]
[epoch 701/1000, batch 51/100 -> loss before: 0.5125305187356723, loss after: 0.5188314511467013]
[epoch 701/1000, batch 61/100 -> loss before: 0.1348800014981935, loss after: 0.13463616938592232]
[epoch 701/1000, batch 71/100 -> loss before: 0.3675563926154645, loss after: 0.3673820593587645]
[epoch 701/1000, batch 81/100 -> loss before: 0.28430263577477877, loss after: 0.2851341486033966]
[epoch 701/1000, batch 91/100 -> loss before: 0.35446415050959545, loss after: 0.3540368831411433]
ENDING EPOCH 701/1000 [loss before: 0.28380885516279764, loss after: 0.28380262088019526; epoch time: 0.10844206809997559 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.2674787417733315, loss after: 0.25475794296216747]
[epoch 801/1000, batch 11/100 -> loss before: 0.11580333764620665, loss after: 0.11584612558256666]
[epoch 801/1000, batch 21/100 -> loss before: 0.23518295834242675, loss after: 0.23456724605077234]
[epoch 801/1000, batch 31/100 -> loss before: 0.21458796226968038, loss after: 0.2148194438800978]
[epoch 801/1000, batch 41/100 -> loss before: 0.4908466361614739, loss after: 0.4934924373263209]
[epoch 801/1000, batch 51/100 -> loss before: 0.2634217629961345, loss after: 0.26371412657639104]
[epoch 801/1000, batch 61/100 -> loss before: 0.26992517623608137, loss after: 0.2687667047291198]
[epoch 801/1000, batch 71/100 -> loss before: 0.3252288956137893, loss after: 0.3258626472642614]
[epoch 801/1000, batch 81/100 -> loss before: 0.2990158569180065, loss after: 0.2986167273358821]
[epoch 801/1000, batch 91/100 -> loss before: 0.24795503100914612, loss after: 0.2464409111327201]
ENDING EPOCH 801/1000 [loss before: 0.2838812120347781, loss after: 0.28315815986488097; epoch time: 0.10018301010131836 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.131337442621176, loss after: 0.12944189760472372]
[epoch 901/1000, batch 11/100 -> loss before: 0.3632249968727791, loss after: 0.3584648124923159]
[epoch 901/1000, batch 21/100 -> loss before: 0.25392358732085024, loss after: 0.2528217449876395]
[epoch 901/1000, batch 31/100 -> loss before: 0.28045454495375355, loss after: 0.2784661963325817]
[epoch 901/1000, batch 41/100 -> loss before: 0.34049115026346427, loss after: 0.34069885292222324]
[epoch 901/1000, batch 51/100 -> loss before: 0.26968924938689537, loss after: 0.27019523123946143]
[epoch 901/1000, batch 61/100 -> loss before: 0.4150143721234431, loss after: 0.41560277184938865]
[epoch 901/1000, batch 71/100 -> loss before: 0.18274326080791078, loss after: 0.17608254092143977]
[epoch 901/1000, batch 81/100 -> loss before: 0.3723070415492895, loss after: 0.3723736761813012]
[epoch 901/1000, batch 91/100 -> loss before: 0.20570070829832687, loss after: 0.2034336678473893]
ENDING EPOCH 901/1000 [loss before: 0.28355323217804074, loss after: 0.29398796239603675; epoch time: 0.10405468940734863 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.26775565729881234, loss after: 0.26832426953667377]
[epoch 1000/1000, batch 11/100 -> loss before: 0.2292297921622036, loss after: 0.2291958483115329]
[epoch 1000/1000, batch 21/100 -> loss before: 0.3203461183126025, loss after: 0.3203289092541102]
[epoch 1000/1000, batch 31/100 -> loss before: 0.23627022551735882, loss after: 0.2371509650786879]
[epoch 1000/1000, batch 41/100 -> loss before: 0.3063801984839841, loss after: 0.30206071883613816]
[epoch 1000/1000, batch 51/100 -> loss before: 0.3114621793935077, loss after: 0.311959880062313]
[epoch 1000/1000, batch 61/100 -> loss before: 0.20496048204281841, loss after: 0.20682453760475714]
[epoch 1000/1000, batch 71/100 -> loss before: 0.3527229502649387, loss after: 0.3515628748882663]
[epoch 1000/1000, batch 81/100 -> loss before: 0.10233670976276836, loss after: 0.10259052415525916]
[epoch 1000/1000, batch 91/100 -> loss before: 0.3390521922264477, loss after: 0.3393219247143421]
ENDING EPOCH 1000/1000 [loss before: 0.28434019219627876, loss after: 0.28313128224486184; epoch time: 0.12018728256225586 s]
FIT DONE. [time: 93.81771779060364 s]
LOSS TRAIN (MSE): 0.28313128224486184
LOSS TEST (MSE): 0.27755881725060005
R^2 TRAIN: -0.00018761270151612663
R^2 TEST: -0.0001025201639601736
EXPERIMENT DONE
