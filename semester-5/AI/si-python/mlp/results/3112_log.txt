EXPERIMENT 3112 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[128, 128, 64, 64, 32, 32], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=sgd_simple, learning_rate=0.0001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 32481]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.26392389365226554, loss after: 0.2639129510856675]
[epoch 1/1000, batch 11/100 -> loss before: 0.2836682525721442, loss after: 0.2786019929489866]
[epoch 1/1000, batch 21/100 -> loss before: 0.29022844246603097, loss after: 0.2891240946074247]
[epoch 1/1000, batch 31/100 -> loss before: 0.3453569247788747, loss after: 0.3451927998963867]
[epoch 1/1000, batch 41/100 -> loss before: 0.30566463853241266, loss after: 0.3048993768278331]
[epoch 1/1000, batch 51/100 -> loss before: 0.17915459874536244, loss after: 0.17605440557320545]
[epoch 1/1000, batch 61/100 -> loss before: 0.2468840084102042, loss after: 0.24572706658919966]
[epoch 1/1000, batch 71/100 -> loss before: 0.3262115222887739, loss after: 0.3261160353777218]
[epoch 1/1000, batch 81/100 -> loss before: 0.3778856006213297, loss after: 0.37786895353352046]
[epoch 1/1000, batch 91/100 -> loss before: 0.3319704605248194, loss after: 0.3313782363651542]
ENDING EPOCH 1/1000 [loss before: 0.2838936199301506, loss after: 0.28315462021494525; epoch time: 0.06226062774658203 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.30882553225771103, loss after: 0.30551184353916355]
[epoch 101/1000, batch 11/100 -> loss before: 0.24258169390099904, loss after: 0.24258142800310095]
[epoch 101/1000, batch 21/100 -> loss before: 0.278380440353473, loss after: 0.2783164466888525]
[epoch 101/1000, batch 31/100 -> loss before: 0.15037944386718993, loss after: 0.15000889297376035]
[epoch 101/1000, batch 41/100 -> loss before: 0.36842378768984435, loss after: 0.36842095545755393]
[epoch 101/1000, batch 51/100 -> loss before: 0.16072671384028447, loss after: 0.160722523811714]
[epoch 101/1000, batch 61/100 -> loss before: 0.16278069765712438, loss after: 0.16278011103852955]
[epoch 101/1000, batch 71/100 -> loss before: 0.36824325112767037, loss after: 0.36666225783226813]
[epoch 101/1000, batch 81/100 -> loss before: 0.3203776924246919, loss after: 0.31328220334654555]
[epoch 101/1000, batch 91/100 -> loss before: 0.3750472384945601, loss after: 0.3743743024529221]
ENDING EPOCH 101/1000 [loss before: 0.28308118239397756, loss after: 0.28323539096577055; epoch time: 0.0657496452331543 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.5707088477118492, loss after: 0.5694291060301302]
[epoch 201/1000, batch 11/100 -> loss before: 0.45855041323859885, loss after: 0.4585167948226994]
[epoch 201/1000, batch 21/100 -> loss before: 0.3238076684830009, loss after: 0.31860064296314744]
[epoch 201/1000, batch 31/100 -> loss before: 0.2742426327462421, loss after: 0.2741334607362593]
[epoch 201/1000, batch 41/100 -> loss before: 0.2433490869972707, loss after: 0.2412693125775495]
[epoch 201/1000, batch 51/100 -> loss before: 0.38913756508527986, loss after: 0.38904678809373294]
[epoch 201/1000, batch 61/100 -> loss before: 0.30377544771921977, loss after: 0.30305744327590756]
[epoch 201/1000, batch 71/100 -> loss before: 0.17307772379738962, loss after: 0.1728965358782046]
[epoch 201/1000, batch 81/100 -> loss before: 0.29654403027124776, loss after: 0.29399029515264274]
[epoch 201/1000, batch 91/100 -> loss before: 0.46804487178647386, loss after: 0.46669105843839986]
ENDING EPOCH 201/1000 [loss before: 0.2831375902953485, loss after: 0.2831538195905795; epoch time: 0.05311083793640137 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.1503780571762651, loss after: 0.15018860121591457]
[epoch 301/1000, batch 11/100 -> loss before: 0.10641278181164257, loss after: 0.10606342221440179]
[epoch 301/1000, batch 21/100 -> loss before: 0.39735376238179265, loss after: 0.3942783413123951]
[epoch 301/1000, batch 31/100 -> loss before: 0.28141620305581405, loss after: 0.2811522180312661]
[epoch 301/1000, batch 41/100 -> loss before: 0.3267366925692157, loss after: 0.32670287832147826]
[epoch 301/1000, batch 51/100 -> loss before: 0.315271594894412, loss after: 0.31471924753974073]
[epoch 301/1000, batch 61/100 -> loss before: 0.17598465547889375, loss after: 0.17594955313803984]
[epoch 301/1000, batch 71/100 -> loss before: 0.19108347554890565, loss after: 0.19060099211313017]
[epoch 301/1000, batch 81/100 -> loss before: 0.24435408095627734, loss after: 0.2436567588544974]
[epoch 301/1000, batch 91/100 -> loss before: 0.3510308190309639, loss after: 0.34865137744191327]
ENDING EPOCH 301/1000 [loss before: 0.28310941433590014, loss after: 0.28308482303684396; epoch time: 0.06560182571411133 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.5520894140665156, loss after: 0.5486838872434735]
[epoch 401/1000, batch 11/100 -> loss before: 0.2447838317168205, loss after: 0.24452301052044909]
[epoch 401/1000, batch 21/100 -> loss before: 0.21389900796146116, loss after: 0.21030019411312026]
[epoch 401/1000, batch 31/100 -> loss before: 0.28622859388418365, loss after: 0.28622821318013625]
[epoch 401/1000, batch 41/100 -> loss before: 0.2053053301264795, loss after: 0.20502214830498128]
[epoch 401/1000, batch 51/100 -> loss before: 0.3133206269110675, loss after: 0.31249558678650274]
[epoch 401/1000, batch 61/100 -> loss before: 0.2852547210466573, loss after: 0.28512680895360853]
[epoch 401/1000, batch 71/100 -> loss before: 0.29301135314905025, loss after: 0.2888250504488765]
[epoch 401/1000, batch 81/100 -> loss before: 0.47813284033801934, loss after: 0.4742898181148682]
[epoch 401/1000, batch 91/100 -> loss before: 0.15476878883139486, loss after: 0.15470112653043175]
ENDING EPOCH 401/1000 [loss before: 0.28310886417276815, loss after: 0.28307705144656914; epoch time: 0.055177927017211914 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.2873588641692647, loss after: 0.2863077265186417]
[epoch 501/1000, batch 11/100 -> loss before: 0.40648289388431114, loss after: 0.40589989336554205]
[epoch 501/1000, batch 21/100 -> loss before: 0.63582116476789, loss after: 0.6341560516970757]
[epoch 501/1000, batch 31/100 -> loss before: 0.11618572885455518, loss after: 0.11602245484411948]
[epoch 501/1000, batch 41/100 -> loss before: 0.29200305622575684, loss after: 0.2864875607378984]
[epoch 501/1000, batch 51/100 -> loss before: 0.2093226804005504, loss after: 0.2091097557573928]
[epoch 501/1000, batch 61/100 -> loss before: 0.5060363910091419, loss after: 0.5029648694247342]
[epoch 501/1000, batch 71/100 -> loss before: 0.2679800417789966, loss after: 0.267592294122119]
[epoch 501/1000, batch 81/100 -> loss before: 0.5047242014124886, loss after: 0.504706393832946]
[epoch 501/1000, batch 91/100 -> loss before: 0.2641608791322776, loss after: 0.263993596761665]
ENDING EPOCH 501/1000 [loss before: 0.28307586861090717, loss after: 0.283145883484616; epoch time: 0.05658411979675293 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.2671691948198005, loss after: 0.2671568547221256]
[epoch 601/1000, batch 11/100 -> loss before: 0.3678726453443403, loss after: 0.3638503553762944]
[epoch 601/1000, batch 21/100 -> loss before: 0.625038152969544, loss after: 0.6240878274856974]
[epoch 601/1000, batch 31/100 -> loss before: 0.15496898914535134, loss after: 0.15389886769853614]
[epoch 601/1000, batch 41/100 -> loss before: 0.3518881734919839, loss after: 0.3517191985539139]
[epoch 601/1000, batch 51/100 -> loss before: 0.30657196644736906, loss after: 0.30651107068573513]
[epoch 601/1000, batch 61/100 -> loss before: 0.1697042751698738, loss after: 0.16930583280709485]
[epoch 601/1000, batch 71/100 -> loss before: 0.2051447271872679, loss after: 0.2048444863487826]
[epoch 601/1000, batch 81/100 -> loss before: 0.32243414291126965, loss after: 0.3215271562105192]
[epoch 601/1000, batch 91/100 -> loss before: 0.5206278764988416, loss after: 0.5202176618125058]
ENDING EPOCH 601/1000 [loss before: 0.283079492959725, loss after: 0.28318179221712847; epoch time: 0.04883861541748047 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.23572097494655075, loss after: 0.23406361248616422]
[epoch 701/1000, batch 11/100 -> loss before: 0.4001527507040456, loss after: 0.3990218954276227]
[epoch 701/1000, batch 21/100 -> loss before: 0.3425084406120708, loss after: 0.33904762902501595]
[epoch 701/1000, batch 31/100 -> loss before: 0.2529462762750364, loss after: 0.25293819462323414]
[epoch 701/1000, batch 41/100 -> loss before: 0.16372150294734275, loss after: 0.16175277593639736]
[epoch 701/1000, batch 51/100 -> loss before: 0.29216472699317775, loss after: 0.29183999619655926]
[epoch 701/1000, batch 61/100 -> loss before: 0.3145956821291652, loss after: 0.3145908318852711]
[epoch 701/1000, batch 71/100 -> loss before: 0.11934405048633992, loss after: 0.11821749950764848]
[epoch 701/1000, batch 81/100 -> loss before: 0.3115924276415877, loss after: 0.31074799592321095]
[epoch 701/1000, batch 91/100 -> loss before: 0.2987649914120116, loss after: 0.2977157786799777]
ENDING EPOCH 701/1000 [loss before: 0.28363050814289603, loss after: 0.2831202511659418; epoch time: 0.048742055892944336 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.2668133730944031, loss after: 0.26599481865330044]
[epoch 801/1000, batch 11/100 -> loss before: 0.30738333406204577, loss after: 0.30699575201231977]
[epoch 801/1000, batch 21/100 -> loss before: 0.2440791491669317, loss after: 0.239501751287223]
[epoch 801/1000, batch 31/100 -> loss before: 0.19268464267523427, loss after: 0.19145781767242948]
[epoch 801/1000, batch 41/100 -> loss before: 0.2609007904656087, loss after: 0.26064807260306727]
[epoch 801/1000, batch 51/100 -> loss before: 0.20752373472712934, loss after: 0.20696784246495864]
[epoch 801/1000, batch 61/100 -> loss before: 0.36599108303533406, loss after: 0.3659488636205904]
[epoch 801/1000, batch 71/100 -> loss before: 0.2873609702170164, loss after: 0.28643212012180747]
[epoch 801/1000, batch 81/100 -> loss before: 0.18059165173577935, loss after: 0.1791166536317074]
[epoch 801/1000, batch 91/100 -> loss before: 0.32981111669286267, loss after: 0.32979579506642265]
ENDING EPOCH 801/1000 [loss before: 0.2831543315493205, loss after: 0.2830805437311017; epoch time: 0.05235719680786133 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.20876705348940622, loss after: 0.20792698372684626]
[epoch 901/1000, batch 11/100 -> loss before: 0.2885297669099184, loss after: 0.28849483755644395]
[epoch 901/1000, batch 21/100 -> loss before: 0.10294336837511224, loss after: 0.10235854959962942]
[epoch 901/1000, batch 31/100 -> loss before: 0.22070206362711975, loss after: 0.21856364630823433]
[epoch 901/1000, batch 41/100 -> loss before: 0.3023146463131963, loss after: 0.301981174422774]
[epoch 901/1000, batch 51/100 -> loss before: 0.2941458106028674, loss after: 0.29394879646257904]
[epoch 901/1000, batch 61/100 -> loss before: 0.31533102606299696, loss after: 0.3152145375962348]
[epoch 901/1000, batch 71/100 -> loss before: 0.392369438821121, loss after: 0.39033194178761765]
[epoch 901/1000, batch 81/100 -> loss before: 0.20192824861246184, loss after: 0.19900251346704023]
[epoch 901/1000, batch 91/100 -> loss before: 0.3996430590696022, loss after: 0.3953750209734558]
ENDING EPOCH 901/1000 [loss before: 0.28326227271437643, loss after: 0.2830741351641977; epoch time: 0.049799203872680664 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.3583050923260279, loss after: 0.3582494008914555]
[epoch 1000/1000, batch 11/100 -> loss before: 0.2855225372190163, loss after: 0.28185006981691396]
[epoch 1000/1000, batch 21/100 -> loss before: 0.23953781602061874, loss after: 0.23571077005821892]
[epoch 1000/1000, batch 31/100 -> loss before: 0.35125191423363716, loss after: 0.3508565614062623]
[epoch 1000/1000, batch 41/100 -> loss before: 0.2961358806464057, loss after: 0.2940972522809687]
[epoch 1000/1000, batch 51/100 -> loss before: 0.19832421948350026, loss after: 0.19832059536690988]
[epoch 1000/1000, batch 61/100 -> loss before: 0.21348523233180144, loss after: 0.21279018607944628]
[epoch 1000/1000, batch 71/100 -> loss before: 0.2410751375451432, loss after: 0.24084265832546764]
[epoch 1000/1000, batch 81/100 -> loss before: 0.33834605689367925, loss after: 0.33611508141139035]
[epoch 1000/1000, batch 91/100 -> loss before: 0.40852792013357836, loss after: 0.4074974999234908]
ENDING EPOCH 1000/1000 [loss before: 0.2832371505695901, loss after: 0.2830770997376976; epoch time: 0.05207490921020508 s]
FIT DONE. [time: 45.62117123603821 s]
LOSS TRAIN (MSE): 0.2830770997376976
LOSS TEST (MSE): 0.27763799212684576
R^2 TRAIN: 3.7920491743337337e-06
R^2 TEST: -0.00038780381681369214
EXPERIMENT DONE
