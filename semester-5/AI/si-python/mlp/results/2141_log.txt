EXPERIMENT 2141 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[128, 64, 32], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=adam, learning_rate=0.001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 10753]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 1.6197437009973044, loss after: 1.4648680983925577]
[epoch 1/1000, batch 11/100 -> loss before: 0.18893200323776907, loss after: 0.18524392048627691]
[epoch 1/1000, batch 21/100 -> loss before: 0.12611113833615908, loss after: 0.12579099949028397]
[epoch 1/1000, batch 31/100 -> loss before: 0.4551993656216579, loss after: 0.45161547044625705]
[epoch 1/1000, batch 41/100 -> loss before: 0.16935170489264179, loss after: 0.16873275473387955]
[epoch 1/1000, batch 51/100 -> loss before: 0.4046863996670373, loss after: 0.38534527625947895]
[epoch 1/1000, batch 61/100 -> loss before: 0.5578711155507967, loss after: 0.5505627669476725]
[epoch 1/1000, batch 71/100 -> loss before: 0.4454981739544287, loss after: 0.43674534679372073]
[epoch 1/1000, batch 81/100 -> loss before: 0.1446288659193375, loss after: 0.14360828790856753]
[epoch 1/1000, batch 91/100 -> loss before: 0.4361680221175481, loss after: 0.43338266535315134]
ENDING EPOCH 1/1000 [loss before: 0.7227142221308361, loss after: 0.2874727092362356; epoch time: 0.04643964767456055 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.14097060632310063, loss after: 0.14119945083082802]
[epoch 101/1000, batch 11/100 -> loss before: 0.07940138763988826, loss after: 0.07945268901713291]
[epoch 101/1000, batch 21/100 -> loss before: 0.0933028152076184, loss after: 0.0933526795256115]
[epoch 101/1000, batch 31/100 -> loss before: 0.11957718676980814, loss after: 0.11923942349489518]
[epoch 101/1000, batch 41/100 -> loss before: 0.4109274102333941, loss after: 0.4098965743459019]
[epoch 101/1000, batch 51/100 -> loss before: 0.12518986592385312, loss after: 0.12450571989246714]
[epoch 101/1000, batch 61/100 -> loss before: 0.16244081153962925, loss after: 0.16137626281485]
[epoch 101/1000, batch 71/100 -> loss before: 0.22835459726909607, loss after: 0.22801201511864547]
[epoch 101/1000, batch 81/100 -> loss before: 0.2690080730050221, loss after: 0.2695904506347103]
[epoch 101/1000, batch 91/100 -> loss before: 0.13754281967338322, loss after: 0.13701035016508373]
ENDING EPOCH 101/1000 [loss before: 0.1989816109698333, loss after: 0.20035522079134777; epoch time: 0.04399251937866211 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.07594192967744766, loss after: 0.07263496759399415]
[epoch 201/1000, batch 11/100 -> loss before: 0.1403425842393365, loss after: 0.13478390412918123]
[epoch 201/1000, batch 21/100 -> loss before: 0.19639465978185836, loss after: 0.1943483900843035]
[epoch 201/1000, batch 31/100 -> loss before: 0.06057012954831912, loss after: 0.05931548910773422]
[epoch 201/1000, batch 41/100 -> loss before: 0.25600772304961494, loss after: 0.25437900622958043]
[epoch 201/1000, batch 51/100 -> loss before: 0.11141925423154261, loss after: 0.10940651798998106]
[epoch 201/1000, batch 61/100 -> loss before: 0.14218063909282128, loss after: 0.14011437619982467]
[epoch 201/1000, batch 71/100 -> loss before: 0.11714000429223977, loss after: 0.11284312208575557]
[epoch 201/1000, batch 81/100 -> loss before: 0.07972063538278147, loss after: 0.07674219515027725]
[epoch 201/1000, batch 91/100 -> loss before: 0.09522381367780304, loss after: 0.09026383347496966]
ENDING EPOCH 201/1000 [loss before: 0.10415512931987143, loss after: 0.10424923005045031; epoch time: 0.04402732849121094 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.025001550609350102, loss after: 0.02381637121726082]
[epoch 301/1000, batch 11/100 -> loss before: 0.0638375653371104, loss after: 0.0637612950347818]
[epoch 301/1000, batch 21/100 -> loss before: 0.03023581181674051, loss after: 0.029565031114069724]
[epoch 301/1000, batch 31/100 -> loss before: 0.07018714255848818, loss after: 0.06515144127217287]
[epoch 301/1000, batch 41/100 -> loss before: 0.06918694645954344, loss after: 0.06967405631773924]
[epoch 301/1000, batch 51/100 -> loss before: 0.09001686407560011, loss after: 0.08358213297451297]
[epoch 301/1000, batch 61/100 -> loss before: 0.10536465795808773, loss after: 0.10394089293099933]
[epoch 301/1000, batch 71/100 -> loss before: 0.13798077093025304, loss after: 0.13591677603251573]
[epoch 301/1000, batch 81/100 -> loss before: 0.0592472420985977, loss after: 0.05748478677791752]
[epoch 301/1000, batch 91/100 -> loss before: 0.029003975335808785, loss after: 0.026833022169043697]
ENDING EPOCH 301/1000 [loss before: 0.06455230472180472, loss after: 0.0646938058261326; epoch time: 0.047129154205322266 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.011327330476779852, loss after: 0.00998790504186598]
[epoch 401/1000, batch 11/100 -> loss before: 0.06484688391007089, loss after: 0.06431662449806505]
[epoch 401/1000, batch 21/100 -> loss before: 0.03533398277526677, loss after: 0.0319210919999791]
[epoch 401/1000, batch 31/100 -> loss before: 0.0180556417492903, loss after: 0.017697349595400428]
[epoch 401/1000, batch 41/100 -> loss before: 0.040723855363322084, loss after: 0.04018995234609581]
[epoch 401/1000, batch 51/100 -> loss before: 0.08826739376098512, loss after: 0.0875216441945321]
[epoch 401/1000, batch 61/100 -> loss before: 0.024625457371398637, loss after: 0.024442565442395158]
[epoch 401/1000, batch 71/100 -> loss before: 0.03925009768253305, loss after: 0.03744480866300696]
[epoch 401/1000, batch 81/100 -> loss before: 0.06025367433456896, loss after: 0.05983441346985907]
[epoch 401/1000, batch 91/100 -> loss before: 0.03206072127159877, loss after: 0.030053673497862386]
ENDING EPOCH 401/1000 [loss before: 0.04508226919923461, loss after: 0.044403007491604654; epoch time: 0.04307150840759277 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.004707174244290868, loss after: 0.0055553935177257135]
[epoch 501/1000, batch 11/100 -> loss before: 0.047447040032486024, loss after: 0.04422976986409384]
[epoch 501/1000, batch 21/100 -> loss before: 0.04616934534038019, loss after: 0.0424656802677249]
[epoch 501/1000, batch 31/100 -> loss before: 0.017137132716992572, loss after: 0.016369393593782408]
[epoch 501/1000, batch 41/100 -> loss before: 0.048234611690973435, loss after: 0.045310568639556806]
[epoch 501/1000, batch 51/100 -> loss before: 0.039744257764215724, loss after: 0.034701268107187565]
[epoch 501/1000, batch 61/100 -> loss before: 0.01639427303932905, loss after: 0.015020851625398268]
[epoch 501/1000, batch 71/100 -> loss before: 0.04384975156968063, loss after: 0.042699392666537925]
[epoch 501/1000, batch 81/100 -> loss before: 0.022441477259035218, loss after: 0.019524597961529812]
[epoch 501/1000, batch 91/100 -> loss before: 0.053322738344939904, loss after: 0.05196826819899338]
ENDING EPOCH 501/1000 [loss before: 0.03917202102470974, loss after: 0.03412843002772149; epoch time: 0.045716047286987305 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.05575162412455106, loss after: 0.050432302958511796]
[epoch 601/1000, batch 11/100 -> loss before: 0.0316691233995678, loss after: 0.02926902675622981]
[epoch 601/1000, batch 21/100 -> loss before: 0.011652140054060355, loss after: 0.011183865121101763]
[epoch 601/1000, batch 31/100 -> loss before: 0.025627733733394848, loss after: 0.023973174093286052]
[epoch 601/1000, batch 41/100 -> loss before: 0.012194529458873902, loss after: 0.012574917330457435]
[epoch 601/1000, batch 51/100 -> loss before: 0.036871315852902424, loss after: 0.035830484808845035]
[epoch 601/1000, batch 61/100 -> loss before: 0.02125537062268797, loss after: 0.020802479656878464]
[epoch 601/1000, batch 71/100 -> loss before: 0.008689913464840297, loss after: 0.009228413528786273]
[epoch 601/1000, batch 81/100 -> loss before: 0.02932212348001594, loss after: 0.02963808628309501]
[epoch 601/1000, batch 91/100 -> loss before: 0.02345735122049421, loss after: 0.02248936812799728]
ENDING EPOCH 601/1000 [loss before: 0.032147400843404325, loss after: 0.03037178091842683; epoch time: 0.043727874755859375 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.006358823102581292, loss after: 0.005898966733150738]
[epoch 701/1000, batch 11/100 -> loss before: 0.022471348890694333, loss after: 0.021681649412341106]
[epoch 701/1000, batch 21/100 -> loss before: 0.007670844969780735, loss after: 0.0074748338624223]
[epoch 701/1000, batch 31/100 -> loss before: 0.007536263172323587, loss after: 0.008461274117296169]
[epoch 701/1000, batch 41/100 -> loss before: 0.019903725861411713, loss after: 0.019391303133595463]
[epoch 701/1000, batch 51/100 -> loss before: 0.011188005168317574, loss after: 0.01033772622233352]
[epoch 701/1000, batch 61/100 -> loss before: 0.008790931552953936, loss after: 0.007983735000883212]
[epoch 701/1000, batch 71/100 -> loss before: 0.04976402704243491, loss after: 0.04549664617180537]
[epoch 701/1000, batch 81/100 -> loss before: 0.03936072781320887, loss after: 0.038154533515098885]
[epoch 701/1000, batch 91/100 -> loss before: 0.025071995427121054, loss after: 0.025066643976031262]
ENDING EPOCH 701/1000 [loss before: 0.025683321160230598, loss after: 0.02590386290596856; epoch time: 0.04543137550354004 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.05950278919449474, loss after: 0.06038655979619524]
[epoch 801/1000, batch 11/100 -> loss before: 0.02796543054924014, loss after: 0.02571955890348913]
[epoch 801/1000, batch 21/100 -> loss before: 0.0206451812679259, loss after: 0.020322313791084384]
[epoch 801/1000, batch 31/100 -> loss before: 0.0033141398790808877, loss after: 0.0025927270374826554]
[epoch 801/1000, batch 41/100 -> loss before: 0.04964982786247651, loss after: 0.04740272395224425]
[epoch 801/1000, batch 51/100 -> loss before: 0.01031528573013243, loss after: 0.009985147508840804]
[epoch 801/1000, batch 61/100 -> loss before: 0.009619160880720786, loss after: 0.00932488403354308]
[epoch 801/1000, batch 71/100 -> loss before: 0.007771173524057849, loss after: 0.007654724569108991]
[epoch 801/1000, batch 81/100 -> loss before: 0.010999952980151094, loss after: 0.010339055480916891]
[epoch 801/1000, batch 91/100 -> loss before: 0.010577449691037945, loss after: 0.010089841985034596]
ENDING EPOCH 801/1000 [loss before: 0.025357005358335096, loss after: 0.024773152079734427; epoch time: 0.042066335678100586 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.00528936055372692, loss after: 0.005291358205991769]
[epoch 901/1000, batch 11/100 -> loss before: 0.02854544586563429, loss after: 0.026453224695888084]
[epoch 901/1000, batch 21/100 -> loss before: 0.044269706755713054, loss after: 0.044376557290502126]
[epoch 901/1000, batch 31/100 -> loss before: 0.0622029790237529, loss after: 0.06111847681250665]
[epoch 901/1000, batch 41/100 -> loss before: 0.004973118540153228, loss after: 0.004903767703093111]
[epoch 901/1000, batch 51/100 -> loss before: 0.04774518023551956, loss after: 0.045611451791063296]
[epoch 901/1000, batch 61/100 -> loss before: 0.04864075369729194, loss after: 0.04543428033211279]
[epoch 901/1000, batch 71/100 -> loss before: 0.00857181093985451, loss after: 0.008868961421674818]
[epoch 901/1000, batch 81/100 -> loss before: 0.0496269727223823, loss after: 0.048615458297209584]
[epoch 901/1000, batch 91/100 -> loss before: 0.008867925236903319, loss after: 0.009044890576790034]
ENDING EPOCH 901/1000 [loss before: 0.022912999966204266, loss after: 0.024621795879296825; epoch time: 0.04315352439880371 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.015779852114035407, loss after: 0.01363250419136518]
[epoch 1000/1000, batch 11/100 -> loss before: 0.007838594534813581, loss after: 0.007776468014617399]
[epoch 1000/1000, batch 21/100 -> loss before: 0.01790448565979858, loss after: 0.017051236613163604]
[epoch 1000/1000, batch 31/100 -> loss before: 0.018010381144387626, loss after: 0.017792204217483575]
[epoch 1000/1000, batch 41/100 -> loss before: 0.03162505061619966, loss after: 0.030821488377032906]
[epoch 1000/1000, batch 51/100 -> loss before: 0.04034606515180176, loss after: 0.037626062695705724]
[epoch 1000/1000, batch 61/100 -> loss before: 0.012119040790118541, loss after: 0.010960647965771996]
[epoch 1000/1000, batch 71/100 -> loss before: 0.01782653050556097, loss after: 0.01776929576223217]
[epoch 1000/1000, batch 81/100 -> loss before: 0.03303555115801362, loss after: 0.03213501683390896]
[epoch 1000/1000, batch 91/100 -> loss before: 0.0902321601813308, loss after: 0.08892830117158336]
ENDING EPOCH 1000/1000 [loss before: 0.023020747906313595, loss after: 0.02226747836295322; epoch time: 0.04362130165100098 s]
FIT DONE. [time: 41.43902897834778 s]
LOSS TRAIN (MSE): 0.02226747836295322
LOSS TEST (MSE): 0.026615740349792655
R^2 TRAIN: 0.9213380596868715
R^2 TEST: 0.9040979160254017
EXPERIMENT DONE
