EXPERIMENT 3122 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[128, 128, 64, 64, 32, 32], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=sgd_momentum, learning_rate=0.0001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 32481]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.26392389365226554, loss after: 0.2639227898961213]
[epoch 1/1000, batch 11/100 -> loss before: 0.27466347459588336, loss after: 0.27516816248129267]
[epoch 1/1000, batch 21/100 -> loss before: 0.2882325763827487, loss after: 0.2884250851877025]
[epoch 1/1000, batch 31/100 -> loss before: 0.3456484338415781, loss after: 0.3455528833683798]
[epoch 1/1000, batch 41/100 -> loss before: 0.3095441378750955, loss after: 0.30914219338701693]
[epoch 1/1000, batch 51/100 -> loss before: 0.17831918752832393, loss after: 0.17799416531510365]
[epoch 1/1000, batch 61/100 -> loss before: 0.2457744103017158, loss after: 0.24571618108600762]
[epoch 1/1000, batch 71/100 -> loss before: 0.3271076998016013, loss after: 0.3270183142095321]
[epoch 1/1000, batch 81/100 -> loss before: 0.37849334448910055, loss after: 0.37841594420106556]
[epoch 1/1000, batch 91/100 -> loss before: 0.3309589362856856, loss after: 0.3310305234486173]
ENDING EPOCH 1/1000 [loss before: 0.2838936199301506, loss after: 0.28341487257188225; epoch time: 0.05834627151489258 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.3088909188566551, loss after: 0.3085805211176017]
[epoch 101/1000, batch 11/100 -> loss before: 0.24259456856766298, loss after: 0.2425928794673605]
[epoch 101/1000, batch 21/100 -> loss before: 0.2781620781997762, loss after: 0.2781799360722943]
[epoch 101/1000, batch 31/100 -> loss before: 0.15232172889291157, loss after: 0.15205583662443017]
[epoch 101/1000, batch 41/100 -> loss before: 0.3683818194754349, loss after: 0.36838328953239546]
[epoch 101/1000, batch 51/100 -> loss before: 0.16065137824849302, loss after: 0.16065845631272913]
[epoch 101/1000, batch 61/100 -> loss before: 0.16286082918227845, loss after: 0.16285197625953676]
[epoch 101/1000, batch 71/100 -> loss before: 0.3669574972675492, loss after: 0.3668870129914057]
[epoch 101/1000, batch 81/100 -> loss before: 0.31631701023335623, loss after: 0.3160711463273895]
[epoch 101/1000, batch 91/100 -> loss before: 0.37791669111893367, loss after: 0.3775325905049417]
ENDING EPOCH 101/1000 [loss before: 0.2830816524173713, loss after: 0.2831310003392245; epoch time: 0.060097455978393555 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.5701843616098305, loss after: 0.5701680110380245]
[epoch 201/1000, batch 11/100 -> loss before: 0.45877582790023314, loss after: 0.4587504957991307]
[epoch 201/1000, batch 21/100 -> loss before: 0.31866125695127595, loss after: 0.31875183119851347]
[epoch 201/1000, batch 31/100 -> loss before: 0.2737191194998436, loss after: 0.273748705353675]
[epoch 201/1000, batch 41/100 -> loss before: 0.24073686591363988, loss after: 0.24077980668231028]
[epoch 201/1000, batch 51/100 -> loss before: 0.38976971380552794, loss after: 0.3896755788700343]
[epoch 201/1000, batch 61/100 -> loss before: 0.30254955351184665, loss after: 0.30265400406510856]
[epoch 201/1000, batch 71/100 -> loss before: 0.17224647840604432, loss after: 0.17234093940330664]
[epoch 201/1000, batch 81/100 -> loss before: 0.3003555609652828, loss after: 0.299819850091178]
[epoch 201/1000, batch 91/100 -> loss before: 0.47086319469710425, loss after: 0.47047029659842343]
ENDING EPOCH 201/1000 [loss before: 0.28311726563771344, loss after: 0.28307927073151956; epoch time: 0.06865692138671875 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.1503841717089263, loss after: 0.15034840521437678]
[epoch 301/1000, batch 11/100 -> loss before: 0.10675063595775525, loss after: 0.10666406303268253]
[epoch 301/1000, batch 21/100 -> loss before: 0.3954220817769101, loss after: 0.3953834910960854]
[epoch 301/1000, batch 31/100 -> loss before: 0.28124518450048575, loss after: 0.2812178935443925]
[epoch 301/1000, batch 41/100 -> loss before: 0.32654990428701136, loss after: 0.3265623403917551]
[epoch 301/1000, batch 51/100 -> loss before: 0.31432211599156, loss after: 0.3143658176657573]
[epoch 301/1000, batch 61/100 -> loss before: 0.1764031695850932, loss after: 0.1763482450368687]
[epoch 301/1000, batch 71/100 -> loss before: 0.19307487044687136, loss after: 0.19284898151478955]
[epoch 301/1000, batch 81/100 -> loss before: 0.24144755096487494, loss after: 0.24167084492209917]
[epoch 301/1000, batch 91/100 -> loss before: 0.34797785653332813, loss after: 0.34813443472039457]
ENDING EPOCH 301/1000 [loss before: 0.28310891822653433, loss after: 0.28312009525967674; epoch time: 0.05731773376464844 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.5546324250622796, loss after: 0.5539648868882692]
[epoch 401/1000, batch 11/100 -> loss before: 0.24532319244905812, loss after: 0.24520401140143755]
[epoch 401/1000, batch 21/100 -> loss before: 0.21688339452240143, loss after: 0.2163334090954303]
[epoch 401/1000, batch 31/100 -> loss before: 0.28624817386901397, loss after: 0.2862448955572591]
[epoch 401/1000, batch 41/100 -> loss before: 0.20510171311449282, loss after: 0.20507723356805113]
[epoch 401/1000, batch 51/100 -> loss before: 0.31486019742861704, loss after: 0.31463283472516573]
[epoch 401/1000, batch 61/100 -> loss before: 0.2854347523504976, loss after: 0.2854069412119954]
[epoch 401/1000, batch 71/100 -> loss before: 0.2925472128430318, loss after: 0.292185739400645]
[epoch 401/1000, batch 81/100 -> loss before: 0.4813737363239349, loss after: 0.48066857969994253]
[epoch 401/1000, batch 91/100 -> loss before: 0.15481615492699186, loss after: 0.15480255667833268]
ENDING EPOCH 401/1000 [loss before: 0.2830780390315209, loss after: 0.2830861686585185; epoch time: 0.06080126762390137 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.28678476367670513, loss after: 0.28671105232359073]
[epoch 501/1000, batch 11/100 -> loss before: 0.40756093111212915, loss after: 0.40735828802265706]
[epoch 501/1000, batch 21/100 -> loss before: 0.6352523144627558, loss after: 0.6350695063207705]
[epoch 501/1000, batch 31/100 -> loss before: 0.1190337042865777, loss after: 0.11869775318282394]
[epoch 501/1000, batch 41/100 -> loss before: 0.290687400104615, loss after: 0.29036249846925255]
[epoch 501/1000, batch 51/100 -> loss before: 0.2089596007811118, loss after: 0.20899035286211406]
[epoch 501/1000, batch 61/100 -> loss before: 0.5069058725310946, loss after: 0.5064268222419852]
[epoch 501/1000, batch 71/100 -> loss before: 0.2682934089744859, loss after: 0.26818910245020244]
[epoch 501/1000, batch 81/100 -> loss before: 0.5048411325153986, loss after: 0.5048365884033178]
[epoch 501/1000, batch 91/100 -> loss before: 0.2626794874423124, loss after: 0.2627964674221156]
ENDING EPOCH 501/1000 [loss before: 0.2830759350982906, loss after: 0.28307530915304885; epoch time: 0.06044363975524902 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.26693624982441966, loss after: 0.26694862403563613]
[epoch 601/1000, batch 11/100 -> loss before: 0.36195176739461987, loss after: 0.3621764055641029]
[epoch 601/1000, batch 21/100 -> loss before: 0.6275393324493734, loss after: 0.6271887826258971]
[epoch 601/1000, batch 31/100 -> loss before: 0.15657643300259527, loss after: 0.1562549318342285]
[epoch 601/1000, batch 41/100 -> loss before: 0.3520230592722633, loss after: 0.35197562220161915]
[epoch 601/1000, batch 51/100 -> loss before: 0.3063311920144772, loss after: 0.3063605479840955]
[epoch 601/1000, batch 61/100 -> loss before: 0.17002942122782566, loss after: 0.16998678760922573]
[epoch 601/1000, batch 71/100 -> loss before: 0.20444874613939099, loss after: 0.20451068300607575]
[epoch 601/1000, batch 81/100 -> loss before: 0.3242862276951287, loss after: 0.3240217582216975]
[epoch 601/1000, batch 91/100 -> loss before: 0.5195032736006483, loss after: 0.51957718801108]
ENDING EPOCH 601/1000 [loss before: 0.28318624944804893, loss after: 0.28307471934781336; epoch time: 0.058454275131225586 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.2303158563663569, loss after: 0.2308111801470893]
[epoch 701/1000, batch 11/100 -> loss before: 0.4033162773540912, loss after: 0.40270589995226674]
[epoch 701/1000, batch 21/100 -> loss before: 0.34268304474188105, loss after: 0.34264884582211874]
[epoch 701/1000, batch 31/100 -> loss before: 0.25369782679109865, loss after: 0.25361440670352575]
[epoch 701/1000, batch 41/100 -> loss before: 0.16892296821262404, loss after: 0.1682111067589311]
[epoch 701/1000, batch 51/100 -> loss before: 0.29194329804959995, loss after: 0.2919177195968724]
[epoch 701/1000, batch 61/100 -> loss before: 0.3147076667702154, loss after: 0.31469515295220135]
[epoch 701/1000, batch 71/100 -> loss before: 0.12329304934990595, loss after: 0.1227514399610011]
[epoch 701/1000, batch 81/100 -> loss before: 0.3108112634859249, loss after: 0.3108586656861622]
[epoch 701/1000, batch 91/100 -> loss before: 0.2951903192685848, loss after: 0.2954083486011955]
ENDING EPOCH 701/1000 [loss before: 0.2831799410001762, loss after: 0.2830814130863291; epoch time: 0.06328511238098145 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.2692160099046639, loss after: 0.26881893900048087]
[epoch 801/1000, batch 11/100 -> loss before: 0.3083310933262088, loss after: 0.30812529162820346]
[epoch 801/1000, batch 21/100 -> loss before: 0.2458224662099155, loss after: 0.24488606345641267]
[epoch 801/1000, batch 31/100 -> loss before: 0.1973865955969662, loss after: 0.19690199961923072]
[epoch 801/1000, batch 41/100 -> loss before: 0.25941321412611595, loss after: 0.25951495999564456]
[epoch 801/1000, batch 51/100 -> loss before: 0.20626600463650413, loss after: 0.2063198018169517]
[epoch 801/1000, batch 61/100 -> loss before: 0.3655585014962734, loss after: 0.36559463695297223]
[epoch 801/1000, batch 71/100 -> loss before: 0.28674181870317755, loss after: 0.2867166300867721]
[epoch 801/1000, batch 81/100 -> loss before: 0.1788874401365869, loss after: 0.17893938825859373]
[epoch 801/1000, batch 91/100 -> loss before: 0.3298643463029337, loss after: 0.329852933948643]
ENDING EPOCH 801/1000 [loss before: 0.2830749283766327, loss after: 0.28309890855805636; epoch time: 0.06693625450134277 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.2066815071470836, loss after: 0.2069012866164573]
[epoch 901/1000, batch 11/100 -> loss before: 0.2885704203431104, loss after: 0.28858597632261684]
[epoch 901/1000, batch 21/100 -> loss before: 0.10114780873353443, loss after: 0.10136089906837649]
[epoch 901/1000, batch 31/100 -> loss before: 0.22273892942682508, loss after: 0.22251360674099754]
[epoch 901/1000, batch 41/100 -> loss before: 0.30418479627110084, loss after: 0.304002049348664]
[epoch 901/1000, batch 51/100 -> loss before: 0.2932540390158129, loss after: 0.2933199733141737]
[epoch 901/1000, batch 61/100 -> loss before: 0.31581690900133547, loss after: 0.31576037403160473]
[epoch 901/1000, batch 71/100 -> loss before: 0.39087824320241793, loss after: 0.39082049791793416]
[epoch 901/1000, batch 81/100 -> loss before: 0.20054141085142668, loss after: 0.20041171858674972]
[epoch 901/1000, batch 91/100 -> loss before: 0.39007298115839617, loss after: 0.39062433433466015]
ENDING EPOCH 901/1000 [loss before: 0.28311689198868006, loss after: 0.28309133166358014; epoch time: 0.05806159973144531 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.359816173506226, loss after: 0.3596236683753474]
[epoch 1000/1000, batch 11/100 -> loss before: 0.287264541333208, loss after: 0.2865754357081583]
[epoch 1000/1000, batch 21/100 -> loss before: 0.24065034219941284, loss after: 0.24031792130242752]
[epoch 1000/1000, batch 31/100 -> loss before: 0.35148132164500173, loss after: 0.3513714858909601]
[epoch 1000/1000, batch 41/100 -> loss before: 0.29813407496050137, loss after: 0.29779889827111566]
[epoch 1000/1000, batch 51/100 -> loss before: 0.19840497006292912, loss after: 0.19839784638983837]
[epoch 1000/1000, batch 61/100 -> loss before: 0.21058867173565593, loss after: 0.2108239401104453]
[epoch 1000/1000, batch 71/100 -> loss before: 0.24079450258490936, loss after: 0.24079516744222834]
[epoch 1000/1000, batch 81/100 -> loss before: 0.3349266995562027, loss after: 0.33503626738603054]
[epoch 1000/1000, batch 91/100 -> loss before: 0.40627667866293127, loss after: 0.406434970663788]
ENDING EPOCH 1000/1000 [loss before: 0.283084555079321, loss after: 0.28311059751991524; epoch time: 0.060410261154174805 s]
FIT DONE. [time: 54.753124475479126 s]
LOSS TRAIN (MSE): 0.28311059751991524
LOSS TEST (MSE): 0.2775678798237099
R^2 TRAIN: -0.00011454198499305512
R^2 TEST: -0.00013517451195044217
EXPERIMENT DONE
