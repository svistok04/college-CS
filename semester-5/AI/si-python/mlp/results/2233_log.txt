EXPERIMENT 2233 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[64, 64, 64, 64, 64, 32, 32, 32, 32, 32, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8], activation_name=relu, targets_activation_name=linear, 
initialization_name=uniform, algo_name=rmsprop, learning_rate=0.001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 25185]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.18318931763552243, loss after: 0.21757363443147554]
[epoch 1/1000, batch 11/100 -> loss before: 0.3487293843764634, loss after: 0.34490970512612085]
[epoch 1/1000, batch 21/100 -> loss before: 0.38536069304159726, loss after: 0.3584191732746748]
[epoch 1/1000, batch 31/100 -> loss before: 0.37895780407874535, loss after: 0.37778543189544145]
[epoch 1/1000, batch 41/100 -> loss before: 0.20756307219470402, loss after: 0.20677126808202492]
[epoch 1/1000, batch 51/100 -> loss before: 0.24902952760626706, loss after: 0.2525525054146799]
[epoch 1/1000, batch 61/100 -> loss before: 0.4570676659796648, loss after: 0.47510870581939224]
[epoch 1/1000, batch 71/100 -> loss before: 0.34897643537059225, loss after: 0.3356487290980733]
[epoch 1/1000, batch 81/100 -> loss before: 0.40080023282187405, loss after: 0.4003973430477332]
[epoch 1/1000, batch 91/100 -> loss before: 0.2964421507821011, loss after: 0.2957234503946348]
ENDING EPOCH 1/1000 [loss before: 0.2899884218128069, loss after: 0.28424287849899166; epoch time: 0.15268874168395996 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.2873852074051559, loss after: 0.28705047429329356]
[epoch 101/1000, batch 11/100 -> loss before: 0.2202534241099225, loss after: 0.22022423573993172]
[epoch 101/1000, batch 21/100 -> loss before: 0.243178578176507, loss after: 0.24279861652257945]
[epoch 101/1000, batch 31/100 -> loss before: 0.38902334193430776, loss after: 0.3872822817163202]
[epoch 101/1000, batch 41/100 -> loss before: 0.6568953487976678, loss after: 0.6458648220834817]
[epoch 101/1000, batch 51/100 -> loss before: 0.2612859061495796, loss after: 0.25746401508040884]
[epoch 101/1000, batch 61/100 -> loss before: 0.5161653677957269, loss after: 0.5160461495310059]
[epoch 101/1000, batch 71/100 -> loss before: 0.14543431185358752, loss after: 0.14533164680299565]
[epoch 101/1000, batch 81/100 -> loss before: 0.2657579280636717, loss after: 0.2653718683766784]
[epoch 101/1000, batch 91/100 -> loss before: 0.24399580881980754, loss after: 0.24173236355254665]
ENDING EPOCH 101/1000 [loss before: 0.2831283423116772, loss after: 0.28307962517985313; epoch time: 0.1926279067993164 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.08809041876385786, loss after: 0.08808919231308626]
[epoch 201/1000, batch 11/100 -> loss before: 0.3579910769448049, loss after: 0.3573895700335533]
[epoch 201/1000, batch 21/100 -> loss before: 0.18947092690869588, loss after: 0.18770222191048314]
[epoch 201/1000, batch 31/100 -> loss before: 0.4615431999468802, loss after: 0.46085672226952584]
[epoch 201/1000, batch 41/100 -> loss before: 0.3192875472488534, loss after: 0.3177467301939343]
[epoch 201/1000, batch 51/100 -> loss before: 0.2845625250524706, loss after: 0.28438019754937255]
[epoch 201/1000, batch 61/100 -> loss before: 0.48535428402239633, loss after: 0.4848074123261882]
[epoch 201/1000, batch 71/100 -> loss before: 0.4137896358271343, loss after: 0.4135124255619296]
[epoch 201/1000, batch 81/100 -> loss before: 0.19909216013323117, loss after: 0.19907821662974784]
[epoch 201/1000, batch 91/100 -> loss before: 0.10912424960368745, loss after: 0.10911687401436199]
ENDING EPOCH 201/1000 [loss before: 0.283078448199752, loss after: 0.28309528138534595; epoch time: 0.11750221252441406 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.18867364323646102, loss after: 0.18860912219997417]
[epoch 301/1000, batch 11/100 -> loss before: 0.2012673221106272, loss after: 0.20109539224326817]
[epoch 301/1000, batch 21/100 -> loss before: 0.30769344535874976, loss after: 0.30617582910442565]
[epoch 301/1000, batch 31/100 -> loss before: 0.32541357303374613, loss after: 0.32344663637835314]
[epoch 301/1000, batch 41/100 -> loss before: 0.2607685919588947, loss after: 0.25988728983391574]
[epoch 301/1000, batch 51/100 -> loss before: 0.28249991569617716, loss after: 0.2824940905787001]
[epoch 301/1000, batch 61/100 -> loss before: 0.3124599533400297, loss after: 0.3108796483511251]
[epoch 301/1000, batch 71/100 -> loss before: 0.3100735950754286, loss after: 0.3095759292966454]
[epoch 301/1000, batch 81/100 -> loss before: 0.3225032778955711, loss after: 0.3224923959849898]
[epoch 301/1000, batch 91/100 -> loss before: 0.31513314016045063, loss after: 0.3112087700841891]
ENDING EPOCH 301/1000 [loss before: 0.28307827842569644, loss after: 0.28308417972247163; epoch time: 0.17125272750854492 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.2846388204939996, loss after: 0.28453300605210907]
[epoch 401/1000, batch 11/100 -> loss before: 0.27582998178176565, loss after: 0.2756888876932533]
[epoch 401/1000, batch 21/100 -> loss before: 0.3400179876771274, loss after: 0.33739952293362563]
[epoch 401/1000, batch 31/100 -> loss before: 0.3737143079183334, loss after: 0.3685451219312057]
[epoch 401/1000, batch 41/100 -> loss before: 0.23517885852447576, loss after: 0.23517716556063434]
[epoch 401/1000, batch 51/100 -> loss before: 0.32873689943491485, loss after: 0.3285897140946036]
[epoch 401/1000, batch 61/100 -> loss before: 0.2575548915889016, loss after: 0.25568813786623695]
[epoch 401/1000, batch 71/100 -> loss before: 0.26564341114341944, loss after: 0.26561513706909945]
[epoch 401/1000, batch 81/100 -> loss before: 0.18960276715946578, loss after: 0.1893465819952666]
[epoch 401/1000, batch 91/100 -> loss before: 0.19345633377738003, loss after: 0.1933529716158371]
ENDING EPOCH 401/1000 [loss before: 0.28312139611284826, loss after: 0.2831260631963982; epoch time: 0.12099146842956543 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.28425573016018607, loss after: 0.2836745538272559]
[epoch 501/1000, batch 11/100 -> loss before: 0.5790391912658297, loss after: 0.5786569260075125]
[epoch 501/1000, batch 21/100 -> loss before: 0.37988561124679554, loss after: 0.37973856300533376]
[epoch 501/1000, batch 31/100 -> loss before: 0.24555240099179346, loss after: 0.2452494666921318]
[epoch 501/1000, batch 41/100 -> loss before: 0.31527348237097036, loss after: 0.3152575160176917]
[epoch 501/1000, batch 51/100 -> loss before: 0.17710865028220782, loss after: 0.1769839781724533]
[epoch 501/1000, batch 61/100 -> loss before: 0.28969265173099024, loss after: 0.28969223187163756]
[epoch 501/1000, batch 71/100 -> loss before: 0.14749580751207597, loss after: 0.1474950221893114]
[epoch 501/1000, batch 81/100 -> loss before: 0.2389012708611859, loss after: 0.23752593316795995]
[epoch 501/1000, batch 91/100 -> loss before: 0.21389064236905053, loss after: 0.2138550644929237]
ENDING EPOCH 501/1000 [loss before: 0.28308191150992923, loss after: 0.2830897198975455; epoch time: 0.1204373836517334 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.3437333204935213, loss after: 0.3436242097576194]
[epoch 601/1000, batch 11/100 -> loss before: 0.3456090791371282, loss after: 0.34488082866779013]
[epoch 601/1000, batch 21/100 -> loss before: 0.29676384162972314, loss after: 0.29663587703240357]
[epoch 601/1000, batch 31/100 -> loss before: 0.13478564361237028, loss after: 0.13443303867925832]
[epoch 601/1000, batch 41/100 -> loss before: 0.37318634707582093, loss after: 0.37113474064268404]
[epoch 601/1000, batch 51/100 -> loss before: 0.28423516879331434, loss after: 0.2832838723465606]
[epoch 601/1000, batch 61/100 -> loss before: 0.13718070929577164, loss after: 0.13715204313219181]
[epoch 601/1000, batch 71/100 -> loss before: 0.26866464814984614, loss after: 0.2662570259549154]
[epoch 601/1000, batch 81/100 -> loss before: 0.3458989506592505, loss after: 0.3435560983732911]
[epoch 601/1000, batch 91/100 -> loss before: 0.3473321558443164, loss after: 0.3473310612081098]
ENDING EPOCH 601/1000 [loss before: 0.28308820202303425, loss after: 0.2831531976364691; epoch time: 0.1206202507019043 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.13990390869529792, loss after: 0.1396205552263305]
[epoch 701/1000, batch 11/100 -> loss before: 0.17239903571093845, loss after: 0.17055373473668906]
[epoch 701/1000, batch 21/100 -> loss before: 0.23093696264610064, loss after: 0.23064834378353086]
[epoch 701/1000, batch 31/100 -> loss before: 0.20733574665781038, loss after: 0.20717410047395232]
[epoch 701/1000, batch 41/100 -> loss before: 0.1977382694155287, loss after: 0.1976822370534897]
[epoch 701/1000, batch 51/100 -> loss before: 0.5445157405423273, loss after: 0.5415219773668141]
[epoch 701/1000, batch 61/100 -> loss before: 0.13601848708428987, loss after: 0.1359159963725835]
[epoch 701/1000, batch 71/100 -> loss before: 0.3678275167155226, loss after: 0.36780790808159397]
[epoch 701/1000, batch 81/100 -> loss before: 0.28400366591763715, loss after: 0.2839778922265867]
[epoch 701/1000, batch 91/100 -> loss before: 0.35366983975549743, loss after: 0.35365832790914326]
ENDING EPOCH 701/1000 [loss before: 0.2830781971825355, loss after: 0.2831002134614339; epoch time: 0.12229037284851074 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.24605001097313495, loss after: 0.24317165421415346]
[epoch 801/1000, batch 11/100 -> loss before: 0.11750510066986952, loss after: 0.11745730386340651]
[epoch 801/1000, batch 21/100 -> loss before: 0.24232163107450871, loss after: 0.2420236186324023]
[epoch 801/1000, batch 31/100 -> loss before: 0.2153476722211453, loss after: 0.21519483166447975]
[epoch 801/1000, batch 41/100 -> loss before: 0.4682587284834859, loss after: 0.46762974941813484]
[epoch 801/1000, batch 51/100 -> loss before: 0.26719426614885944, loss after: 0.2669721775126991]
[epoch 801/1000, batch 61/100 -> loss before: 0.27271075234660425, loss after: 0.27204052886443003]
[epoch 801/1000, batch 71/100 -> loss before: 0.32372017308930506, loss after: 0.32343098536429243]
[epoch 801/1000, batch 81/100 -> loss before: 0.2929641679872136, loss after: 0.29203130810280065]
[epoch 801/1000, batch 91/100 -> loss before: 0.24551333409657783, loss after: 0.2453115008532981]
ENDING EPOCH 801/1000 [loss before: 0.28307939297980045, loss after: 0.2830782706347679; epoch time: 0.15685153007507324 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.13670455925052136, loss after: 0.13608381314709803]
[epoch 901/1000, batch 11/100 -> loss before: 0.32656619635051376, loss after: 0.32531462425466695]
[epoch 901/1000, batch 21/100 -> loss before: 0.2552549417188725, loss after: 0.2540678186672239]
[epoch 901/1000, batch 31/100 -> loss before: 0.29003387249939133, loss after: 0.2891163483605846]
[epoch 901/1000, batch 41/100 -> loss before: 0.34471048807939997, loss after: 0.3436141892227037]
[epoch 901/1000, batch 51/100 -> loss before: 0.2425192359923991, loss after: 0.2399451341932198]
[epoch 901/1000, batch 61/100 -> loss before: 0.4075660865895651, loss after: 0.40756583402395685]
[epoch 901/1000, batch 71/100 -> loss before: 0.1756190560082433, loss after: 0.17489053169410582]
[epoch 901/1000, batch 81/100 -> loss before: 0.39003238893313186, loss after: 0.38949120925006]
[epoch 901/1000, batch 91/100 -> loss before: 0.21172074325521623, loss after: 0.211465256932522]
ENDING EPOCH 901/1000 [loss before: 0.28308075819651707, loss after: 0.2830852540685383; epoch time: 0.12976813316345215 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.25623281276121035, loss after: 0.25568258015907047]
[epoch 1000/1000, batch 11/100 -> loss before: 0.23209789446739276, loss after: 0.23188712900796346]
[epoch 1000/1000, batch 21/100 -> loss before: 0.3129250205348816, loss after: 0.3121395303055057]
[epoch 1000/1000, batch 31/100 -> loss before: 0.2362134114827268, loss after: 0.23616987796869088]
[epoch 1000/1000, batch 41/100 -> loss before: 0.29707448731183533, loss after: 0.2952171686500468]
[epoch 1000/1000, batch 51/100 -> loss before: 0.2980566555221773, loss after: 0.29704331010169605]
[epoch 1000/1000, batch 61/100 -> loss before: 0.2115788171885443, loss after: 0.21093357213364355]
[epoch 1000/1000, batch 71/100 -> loss before: 0.34930861031167526, loss after: 0.3491890409270665]
[epoch 1000/1000, batch 81/100 -> loss before: 0.10212853451044983, loss after: 0.1021279038366556]
[epoch 1000/1000, batch 91/100 -> loss before: 0.35723271910958715, loss after: 0.35676426977830433]
ENDING EPOCH 1000/1000 [loss before: 0.28308220700938425, loss after: 0.2830798744419525; epoch time: 0.15412402153015137 s]
FIT DONE. [time: 129.51286935806274 s]
LOSS TRAIN (MSE): 0.2830798744419525
LOSS TEST (MSE): 0.2776584473885073
R^2 TRAIN: -6.009851917676556e-06
R^2 TEST: -0.00046150840649672453
EXPERIMENT DONE
