EXPERIMENT 2142 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[128, 128, 64, 64, 32, 32], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=adam, learning_rate=0.001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 32481]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.26392389365226554, loss after: 0.26413469721510857]
[epoch 1/1000, batch 11/100 -> loss before: 0.37209907600433906, loss after: 0.37126668006733454]
[epoch 1/1000, batch 21/100 -> loss before: 0.3247581443818507, loss after: 0.31784613560309566]
[epoch 1/1000, batch 31/100 -> loss before: 0.34671448502043906, loss after: 0.34686509214960504]
[epoch 1/1000, batch 41/100 -> loss before: 0.29246079695138844, loss after: 0.2901850416173831]
[epoch 1/1000, batch 51/100 -> loss before: 0.19323623031328543, loss after: 0.1813905023683618]
[epoch 1/1000, batch 61/100 -> loss before: 0.23256519822413715, loss after: 0.23247224770953392]
[epoch 1/1000, batch 71/100 -> loss before: 0.32813631779675356, loss after: 0.3260310960634257]
[epoch 1/1000, batch 81/100 -> loss before: 0.38572129700220686, loss after: 0.3866621410362071]
[epoch 1/1000, batch 91/100 -> loss before: 0.33643552732664767, loss after: 0.3328757726926857]
ENDING EPOCH 1/1000 [loss before: 0.2838936199301506, loss after: 0.2868739134173932; epoch time: 0.09294009208679199 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.2645154358622689, loss after: 0.2639415390515219]
[epoch 101/1000, batch 11/100 -> loss before: 0.24980863482925014, loss after: 0.249649690288888]
[epoch 101/1000, batch 21/100 -> loss before: 0.19339101966963465, loss after: 0.19389631098812293]
[epoch 101/1000, batch 31/100 -> loss before: 0.14072717830921425, loss after: 0.14039169890965159]
[epoch 101/1000, batch 41/100 -> loss before: 0.25974012248814227, loss after: 0.25952594613366686]
[epoch 101/1000, batch 51/100 -> loss before: 0.16186223914220765, loss after: 0.1618916771632809]
[epoch 101/1000, batch 61/100 -> loss before: 0.12531463703896537, loss after: 0.1251461562497648]
[epoch 101/1000, batch 71/100 -> loss before: 0.34159564796228054, loss after: 0.3417352879359423]
[epoch 101/1000, batch 81/100 -> loss before: 0.2203276980357915, loss after: 0.22038970779847014]
[epoch 101/1000, batch 91/100 -> loss before: 0.24528345486852485, loss after: 0.24494897736649285]
ENDING EPOCH 101/1000 [loss before: 0.23151749560046483, loss after: 0.232518739866444; epoch time: 0.09973406791687012 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.5186101414479158, loss after: 0.518383404581902]
[epoch 201/1000, batch 11/100 -> loss before: 0.37234089842934703, loss after: 0.3723952307501778]
[epoch 201/1000, batch 21/100 -> loss before: 0.20484172974790454, loss after: 0.20538547495596077]
[epoch 201/1000, batch 31/100 -> loss before: 0.2134954296124456, loss after: 0.21365845485373494]
[epoch 201/1000, batch 41/100 -> loss before: 0.21686773632346684, loss after: 0.21654460378026252]
[epoch 201/1000, batch 51/100 -> loss before: 0.3892603445086128, loss after: 0.3894892767532561]
[epoch 201/1000, batch 61/100 -> loss before: 0.24875266278876582, loss after: 0.24872120229889108]
[epoch 201/1000, batch 71/100 -> loss before: 0.0934967524298975, loss after: 0.0937170261686258]
[epoch 201/1000, batch 81/100 -> loss before: 0.29223158783039227, loss after: 0.2895156898039758]
[epoch 201/1000, batch 91/100 -> loss before: 0.3272325728622859, loss after: 0.3266231214625258]
ENDING EPOCH 201/1000 [loss before: 0.2311646117092742, loss after: 0.2330677515534256; epoch time: 0.09817218780517578 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.11528636476961478, loss after: 0.1154579729381214]
[epoch 301/1000, batch 11/100 -> loss before: 0.0426568637863359, loss after: 0.04094522089416239]
[epoch 301/1000, batch 21/100 -> loss before: 0.31584031319589306, loss after: 0.3151800562624561]
[epoch 301/1000, batch 31/100 -> loss before: 0.17417538499130442, loss after: 0.17386584829956442]
[epoch 301/1000, batch 41/100 -> loss before: 0.2823298344917262, loss after: 0.2824614127359641]
[epoch 301/1000, batch 51/100 -> loss before: 0.23130560424442598, loss after: 0.2314634642459185]
[epoch 301/1000, batch 61/100 -> loss before: 0.14977439970299508, loss after: 0.14972612134892307]
[epoch 301/1000, batch 71/100 -> loss before: 0.04136178309463937, loss after: 0.04115880671003809]
[epoch 301/1000, batch 81/100 -> loss before: 0.09393818034752462, loss after: 0.09467404595344306]
[epoch 301/1000, batch 91/100 -> loss before: 0.3253110538446017, loss after: 0.324925853500782]
ENDING EPOCH 301/1000 [loss before: 0.231560433896278, loss after: 0.23136880150250222; epoch time: 0.09208297729492188 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.35961006455341227, loss after: 0.35787897600961377]
[epoch 401/1000, batch 11/100 -> loss before: 0.11085210485125432, loss after: 0.10722361214149537]
[epoch 401/1000, batch 21/100 -> loss before: 0.05661945911805988, loss after: 0.056412455305079524]
[epoch 401/1000, batch 31/100 -> loss before: 0.06085251268190249, loss after: 0.05935512651335476]
[epoch 401/1000, batch 41/100 -> loss before: 0.12567806446358962, loss after: 0.1263350236046695]
[epoch 401/1000, batch 51/100 -> loss before: 0.21689426981284834, loss after: 0.2176472229217258]
[epoch 401/1000, batch 61/100 -> loss before: 0.12690259434580495, loss after: 0.12617039920305773]
[epoch 401/1000, batch 71/100 -> loss before: 0.1492898227176604, loss after: 0.15033087710562967]
[epoch 401/1000, batch 81/100 -> loss before: 0.2534873157273808, loss after: 0.23750015996117932]
[epoch 401/1000, batch 91/100 -> loss before: 0.06280343931431887, loss after: 0.06146891470043188]
ENDING EPOCH 401/1000 [loss before: 0.1434771690869334, loss after: 0.13633418436499048; epoch time: 0.0910642147064209 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.058429449846775505, loss after: 0.058669658130086275]
[epoch 501/1000, batch 11/100 -> loss before: 0.1354679963220819, loss after: 0.1414247975087286]
[epoch 501/1000, batch 21/100 -> loss before: 0.20298094385497553, loss after: 0.20589081760043065]
[epoch 501/1000, batch 31/100 -> loss before: 0.03522169328241947, loss after: 0.03536470360917475]
[epoch 501/1000, batch 41/100 -> loss before: 0.1061920850090885, loss after: 0.10401010185343726]
[epoch 501/1000, batch 51/100 -> loss before: 0.20379750727819376, loss after: 0.20394704457717086]
[epoch 501/1000, batch 61/100 -> loss before: 0.047072058547901655, loss after: 0.0462171292791333]
[epoch 501/1000, batch 71/100 -> loss before: 0.037175474998182544, loss after: 0.036297389525545246]
[epoch 501/1000, batch 81/100 -> loss before: 0.20114077153627186, loss after: 0.19928408820785545]
[epoch 501/1000, batch 91/100 -> loss before: 0.13564351150650045, loss after: 0.13447953653690745]
ENDING EPOCH 501/1000 [loss before: 0.11105848844243837, loss after: 0.0990158096083131; epoch time: 0.1012272834777832 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.024400355782363416, loss after: 0.0218877923278004]
[epoch 601/1000, batch 11/100 -> loss before: 0.037171744377076654, loss after: 0.03394692219521979]
[epoch 601/1000, batch 21/100 -> loss before: 0.2554823758772205, loss after: 0.24290323058220636]
[epoch 601/1000, batch 31/100 -> loss before: 0.04240292154619409, loss after: 0.04240787523490126]
[epoch 601/1000, batch 41/100 -> loss before: 0.07620811602093214, loss after: 0.07461936230350517]
[epoch 601/1000, batch 51/100 -> loss before: 0.02732738216622647, loss after: 0.028067930318583244]
[epoch 601/1000, batch 61/100 -> loss before: 0.14253597774515783, loss after: 0.12900263930038214]
[epoch 601/1000, batch 71/100 -> loss before: 0.07201982045157355, loss after: 0.06903094868388444]
[epoch 601/1000, batch 81/100 -> loss before: 0.030497129274722656, loss after: 0.02853477580139756]
[epoch 601/1000, batch 91/100 -> loss before: 0.09670360106480413, loss after: 0.0947139776019009]
ENDING EPOCH 601/1000 [loss before: 0.07026589438077825, loss after: 0.0709556415339421; epoch time: 0.0914912223815918 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.028279648993749507, loss after: 0.024363623351061195]
[epoch 701/1000, batch 11/100 -> loss before: 0.023667128098359795, loss after: 0.020563984411942553]
[epoch 701/1000, batch 21/100 -> loss before: 0.05004953755690724, loss after: 0.04702409231337111]
[epoch 701/1000, batch 31/100 -> loss before: 0.005730229313177497, loss after: 0.005421686974996903]
[epoch 701/1000, batch 41/100 -> loss before: 0.014615860377413326, loss after: 0.013031897040374291]
[epoch 701/1000, batch 51/100 -> loss before: 0.01775537046586232, loss after: 0.016336929376931996]
[epoch 701/1000, batch 61/100 -> loss before: 0.08164347438365412, loss after: 0.07609449608004554]
[epoch 701/1000, batch 71/100 -> loss before: 0.01978538495448314, loss after: 0.017224215478679413]
[epoch 701/1000, batch 81/100 -> loss before: 0.03357191724303362, loss after: 0.028638897581430723]
[epoch 701/1000, batch 91/100 -> loss before: 0.03695093995871927, loss after: 0.027481357600136935]
ENDING EPOCH 701/1000 [loss before: 0.05012720928118053, loss after: 0.047759570358528534; epoch time: 0.09504151344299316 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.024701274562840422, loss after: 0.020883508661334836]
[epoch 801/1000, batch 11/100 -> loss before: 0.04504109304790094, loss after: 0.03754646342411616]
[epoch 801/1000, batch 21/100 -> loss before: 0.03405997417419444, loss after: 0.031494090520957224]
[epoch 801/1000, batch 31/100 -> loss before: 0.017152190010895414, loss after: 0.016675970929442462]
[epoch 801/1000, batch 41/100 -> loss before: 0.008079221926379469, loss after: 0.007812320465338997]
[epoch 801/1000, batch 51/100 -> loss before: 0.014180151736257344, loss after: 0.010932284093396761]
[epoch 801/1000, batch 61/100 -> loss before: 0.0190584560996779, loss after: 0.017824382690771724]
[epoch 801/1000, batch 71/100 -> loss before: 0.04724131215061635, loss after: 0.048858476653997104]
[epoch 801/1000, batch 81/100 -> loss before: 0.031784890059027356, loss after: 0.027228827950715424]
[epoch 801/1000, batch 91/100 -> loss before: 0.129746070496258, loss after: 0.12828899008532554]
ENDING EPOCH 801/1000 [loss before: 0.04034790513808424, loss after: 0.03496169144901401; epoch time: 0.09407377243041992 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.02123607227825631, loss after: 0.017228860358675515]
[epoch 901/1000, batch 11/100 -> loss before: 0.019408956438993166, loss after: 0.017887067508428674]
[epoch 901/1000, batch 21/100 -> loss before: 0.0039019967549878166, loss after: 0.003249501428229499]
[epoch 901/1000, batch 31/100 -> loss before: 0.02338031179288865, loss after: 0.020596140077126866]
[epoch 901/1000, batch 41/100 -> loss before: 0.014525498716669125, loss after: 0.013409236092102452]
[epoch 901/1000, batch 51/100 -> loss before: 0.1182990592449368, loss after: 0.11221502159353139]
[epoch 901/1000, batch 61/100 -> loss before: 0.035064738391134824, loss after: 0.026532564683089278]
[epoch 901/1000, batch 71/100 -> loss before: 0.02185949431783046, loss after: 0.019470985274516996]
[epoch 901/1000, batch 81/100 -> loss before: 0.014832184262588552, loss after: 0.013983100276812205]
[epoch 901/1000, batch 91/100 -> loss before: 0.0389495617014855, loss after: 0.034664008201021064]
ENDING EPOCH 901/1000 [loss before: 0.028845606115595408, loss after: 0.030198757236461666; epoch time: 0.08985638618469238 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.0396117435707213, loss after: 0.038027880299165144]
[epoch 1000/1000, batch 11/100 -> loss before: 0.030798914949386868, loss after: 0.028674460204755513]
[epoch 1000/1000, batch 21/100 -> loss before: 0.005052629571954419, loss after: 0.005021714300351884]
[epoch 1000/1000, batch 31/100 -> loss before: 0.048843212668733905, loss after: 0.04780493682452166]
[epoch 1000/1000, batch 41/100 -> loss before: 0.020503041752844238, loss after: 0.020091407403147153]
[epoch 1000/1000, batch 51/100 -> loss before: 0.02932327812974469, loss after: 0.022944269446897106]
[epoch 1000/1000, batch 61/100 -> loss before: 0.00838825737651231, loss after: 0.008795262541354019]
[epoch 1000/1000, batch 71/100 -> loss before: 0.010831607669003544, loss after: 0.012152176946572104]
[epoch 1000/1000, batch 81/100 -> loss before: 0.0079260477157306, loss after: 0.007997224228684187]
[epoch 1000/1000, batch 91/100 -> loss before: 0.13608114203617644, loss after: 0.12263043951207275]
ENDING EPOCH 1000/1000 [loss before: 0.023833974361649957, loss after: 0.022554076933234014; epoch time: 0.09364938735961914 s]
FIT DONE. [time: 87.37786960601807 s]
LOSS TRAIN (MSE): 0.022554076933234014
LOSS TEST (MSE): 0.02619705915281852
R^2 TRAIN: 0.9203256235563951
R^2 TEST: 0.9056065120209706
EXPERIMENT DONE
