EXPERIMENT 2223 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[64, 64, 64, 64, 64, 32, 32, 32, 32, 32, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8], activation_name=relu, targets_activation_name=linear, 
initialization_name=uniform, algo_name=sgd_momentum, learning_rate=0.001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 25185]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.18318931763552243, loss after: 0.18237280906184747]
[epoch 1/1000, batch 11/100 -> loss before: 0.3298917569360116, loss after: 0.32750469723729136]
[epoch 1/1000, batch 21/100 -> loss before: 0.4203255108290621, loss after: 0.4141139797750674]
[epoch 1/1000, batch 31/100 -> loss before: 0.3475051824347922, loss after: 0.34385149502440776]
[epoch 1/1000, batch 41/100 -> loss before: 0.20585025281480057, loss after: 0.20629254943435943]
[epoch 1/1000, batch 51/100 -> loss before: 0.2504390361730561, loss after: 0.25106811790096895]
[epoch 1/1000, batch 61/100 -> loss before: 0.44819394180672684, loss after: 0.448375687949636]
[epoch 1/1000, batch 71/100 -> loss before: 0.37207278212274186, loss after: 0.34324299137417735]
[epoch 1/1000, batch 81/100 -> loss before: 0.3658090466227612, loss after: 0.3658166077080902]
[epoch 1/1000, batch 91/100 -> loss before: 0.3172907429072219, loss after: 0.30432601123709996]
ENDING EPOCH 1/1000 [loss before: 0.2899884218128069, loss after: 0.2705737345443601; epoch time: 0.09049248695373535 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.20010929294540752, loss after: 0.19999073615869817]
[epoch 101/1000, batch 11/100 -> loss before: 0.16678342380078934, loss after: 0.16669573125070264]
[epoch 101/1000, batch 21/100 -> loss before: 0.15967852070879923, loss after: 0.15947304098471624]
[epoch 101/1000, batch 31/100 -> loss before: 0.2777346217201367, loss after: 0.2769900363577275]
[epoch 101/1000, batch 41/100 -> loss before: 0.5292107271537241, loss after: 0.5285926046104883]
[epoch 101/1000, batch 51/100 -> loss before: 0.23261585296713827, loss after: 0.23221218684936235]
[epoch 101/1000, batch 61/100 -> loss before: 0.5269802478195883, loss after: 0.5269069216532366]
[epoch 101/1000, batch 71/100 -> loss before: 0.1604724301344893, loss after: 0.15968686662937068]
[epoch 101/1000, batch 81/100 -> loss before: 0.28360964657179927, loss after: 0.28357066563359323]
[epoch 101/1000, batch 91/100 -> loss before: 0.21828803660960855, loss after: 0.21819524031261323]
ENDING EPOCH 101/1000 [loss before: 0.23133841741715627, loss after: 0.2307757302342008; epoch time: 0.09641361236572266 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.08411068976220837, loss after: 0.08391011486378523]
[epoch 201/1000, batch 11/100 -> loss before: 0.2050957145380643, loss after: 0.2048932496422291]
[epoch 201/1000, batch 21/100 -> loss before: 0.2317454775067843, loss after: 0.23094141689830705]
[epoch 201/1000, batch 31/100 -> loss before: 0.3807144388328323, loss after: 0.38076785485714987]
[epoch 201/1000, batch 41/100 -> loss before: 0.26083860865412467, loss after: 0.2609740777361416]
[epoch 201/1000, batch 51/100 -> loss before: 0.20543467645261035, loss after: 0.2054506391845028]
[epoch 201/1000, batch 61/100 -> loss before: 0.5038509596598666, loss after: 0.5040488586588968]
[epoch 201/1000, batch 71/100 -> loss before: 0.4076068698920182, loss after: 0.4075687067486683]
[epoch 201/1000, batch 81/100 -> loss before: 0.13281750688043342, loss after: 0.13240270187955025]
[epoch 201/1000, batch 91/100 -> loss before: 0.06126521172450631, loss after: 0.061252485059288174]
ENDING EPOCH 201/1000 [loss before: 0.23338506274645032, loss after: 0.23128305232483576; epoch time: 0.12241220474243164 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.1879418554021425, loss after: 0.18791029128311493]
[epoch 301/1000, batch 11/100 -> loss before: 0.03979436320617664, loss after: 0.039825596855056725]
[epoch 301/1000, batch 21/100 -> loss before: 0.26176486759364737, loss after: 0.26106575578121377]
[epoch 301/1000, batch 31/100 -> loss before: 0.2009154821277721, loss after: 0.20096505478114146]
[epoch 301/1000, batch 41/100 -> loss before: 0.1907865452183491, loss after: 0.1899447985867485]
[epoch 301/1000, batch 51/100 -> loss before: 0.28632962272482493, loss after: 0.2863911772610461]
[epoch 301/1000, batch 61/100 -> loss before: 0.28580994159793016, loss after: 0.285799361104221]
[epoch 301/1000, batch 71/100 -> loss before: 0.25095981811791124, loss after: 0.2505772170495115]
[epoch 301/1000, batch 81/100 -> loss before: 0.33096850773772934, loss after: 0.33100107327806183]
[epoch 301/1000, batch 91/100 -> loss before: 0.2576389203039306, loss after: 0.2572211526420112]
ENDING EPOCH 301/1000 [loss before: 0.23064111278963895, loss after: 0.23071107436658667; epoch time: 0.08770298957824707 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.2818578019809556, loss after: 0.28180176504688037]
[epoch 401/1000, batch 11/100 -> loss before: 0.16228262538247606, loss after: 0.1622435217299838]
[epoch 401/1000, batch 21/100 -> loss before: 0.22326153550357483, loss after: 0.2232074776504498]
[epoch 401/1000, batch 31/100 -> loss before: 0.16340263071576283, loss after: 0.1636545241445743]
[epoch 401/1000, batch 41/100 -> loss before: 0.15616840873743293, loss after: 0.15600333085943263]
[epoch 401/1000, batch 51/100 -> loss before: 0.13757073506854997, loss after: 0.13689319533999864]
[epoch 401/1000, batch 61/100 -> loss before: 0.16167125262184462, loss after: 0.1616074562967745]
[epoch 401/1000, batch 71/100 -> loss before: 0.17892629668781102, loss after: 0.17873210689585703]
[epoch 401/1000, batch 81/100 -> loss before: 0.16499988421673525, loss after: 0.1652030446533314]
[epoch 401/1000, batch 91/100 -> loss before: 0.20699005945683124, loss after: 0.20686877240895435]
ENDING EPOCH 401/1000 [loss before: 0.23059347289080132, loss after: 0.23090890773274472; epoch time: 0.09992384910583496 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.16999481657643517, loss after: 0.17008946060756208]
[epoch 501/1000, batch 11/100 -> loss before: 0.4267757484413889, loss after: 0.42677784728508694]
[epoch 501/1000, batch 21/100 -> loss before: 0.30051253296427916, loss after: 0.3005616696355078]
[epoch 501/1000, batch 31/100 -> loss before: 0.23000796423701392, loss after: 0.22994522322182398]
[epoch 501/1000, batch 41/100 -> loss before: 0.31969097366069193, loss after: 0.3196129361828522]
[epoch 501/1000, batch 51/100 -> loss before: 0.11420020200927652, loss after: 0.11339928696587247]
[epoch 501/1000, batch 61/100 -> loss before: 0.10620833145087889, loss after: 0.1062324180952994]
[epoch 501/1000, batch 71/100 -> loss before: 0.13674139917458328, loss after: 0.1367446686962717]
[epoch 501/1000, batch 81/100 -> loss before: 0.20893715050401548, loss after: 0.2090192856286499]
[epoch 501/1000, batch 91/100 -> loss before: 0.1711244254398553, loss after: 0.17113714883983627]
ENDING EPOCH 501/1000 [loss before: 0.23058861009661677, loss after: 0.23084050298807454; epoch time: 0.09594368934631348 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.3577308627854879, loss after: 0.3578015694013332]
[epoch 601/1000, batch 11/100 -> loss before: 0.15023302161400548, loss after: 0.15003870894633778]
[epoch 601/1000, batch 21/100 -> loss before: 0.1904172348326485, loss after: 0.19049055422072825]
[epoch 601/1000, batch 31/100 -> loss before: 0.12207465764155574, loss after: 0.11974090451116166]
[epoch 601/1000, batch 41/100 -> loss before: 0.3335374895451933, loss after: 0.33313036064318624]
[epoch 601/1000, batch 51/100 -> loss before: 0.3197657710530114, loss after: 0.31979969368467204]
[epoch 601/1000, batch 61/100 -> loss before: 0.10265101320790415, loss after: 0.10262697934849585]
[epoch 601/1000, batch 71/100 -> loss before: 0.3278118385209753, loss after: 0.3269726008539375]
[epoch 601/1000, batch 81/100 -> loss before: 0.29886186604694615, loss after: 0.29863790952114994]
[epoch 601/1000, batch 91/100 -> loss before: 0.25169551846289906, loss after: 0.25171374123004886]
ENDING EPOCH 601/1000 [loss before: 0.23068025799156044, loss after: 0.23054321511128373; epoch time: 0.09069609642028809 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.11084548188600271, loss after: 0.11050652359875461]
[epoch 701/1000, batch 11/100 -> loss before: 0.23096926802698925, loss after: 0.22931397466820988]
[epoch 701/1000, batch 21/100 -> loss before: 0.19852528584651377, loss after: 0.19816997447781198]
[epoch 701/1000, batch 31/100 -> loss before: 0.2199808350406852, loss after: 0.22012172243681252]
[epoch 701/1000, batch 41/100 -> loss before: 0.1355184483669007, loss after: 0.13530635242809283]
[epoch 701/1000, batch 51/100 -> loss before: 0.31343390758249884, loss after: 0.31269181586974487]
[epoch 701/1000, batch 61/100 -> loss before: 0.15156862504146626, loss after: 0.15161548743975423]
[epoch 701/1000, batch 71/100 -> loss before: 0.19404493019751046, loss after: 0.19385079681247358]
[epoch 701/1000, batch 81/100 -> loss before: 0.2168983710488596, loss after: 0.2168133532346288]
[epoch 701/1000, batch 91/100 -> loss before: 0.3466550713965095, loss after: 0.34642981133014705]
ENDING EPOCH 701/1000 [loss before: 0.2306962783348899, loss after: 0.23080301773068237; epoch time: 0.09493255615234375 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.2001430556229386, loss after: 0.19957408512475736]
[epoch 801/1000, batch 11/100 -> loss before: 0.11686718260197015, loss after: 0.11685092256067942]
[epoch 801/1000, batch 21/100 -> loss before: 0.24157249335587921, loss after: 0.24139890214550935]
[epoch 801/1000, batch 31/100 -> loss before: 0.15809117294850075, loss after: 0.15807356183282667]
[epoch 801/1000, batch 41/100 -> loss before: 0.295216377619772, loss after: 0.29497642268574054]
[epoch 801/1000, batch 51/100 -> loss before: 0.14066325437876553, loss after: 0.14036726500075813]
[epoch 801/1000, batch 61/100 -> loss before: 0.2538864446034933, loss after: 0.2538050806337232]
[epoch 801/1000, batch 71/100 -> loss before: 0.28496904595640105, loss after: 0.284926893568917]
[epoch 801/1000, batch 81/100 -> loss before: 0.17163532581101534, loss after: 0.1715764033443939]
[epoch 801/1000, batch 91/100 -> loss before: 0.23704413791893994, loss after: 0.23694621596738555]
ENDING EPOCH 801/1000 [loss before: 0.23049072506104407, loss after: 0.23045566044152316; epoch time: 0.09366536140441895 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.08165058197109426, loss after: 0.0814977636963289]
[epoch 901/1000, batch 11/100 -> loss before: 0.2973654069672639, loss after: 0.29727235971555516]
[epoch 901/1000, batch 21/100 -> loss before: 0.15153154743102457, loss after: 0.15111830638877533]
[epoch 901/1000, batch 31/100 -> loss before: 0.14054288188590694, loss after: 0.14042498582041674]
[epoch 901/1000, batch 41/100 -> loss before: 0.2636262330598775, loss after: 0.26328840904381423]
[epoch 901/1000, batch 51/100 -> loss before: 0.2361149010497055, loss after: 0.23596268890488306]
[epoch 901/1000, batch 61/100 -> loss before: 0.41277081467918386, loss after: 0.4129357960357556]
[epoch 901/1000, batch 71/100 -> loss before: 0.15829671040470772, loss after: 0.15818753881725856]
[epoch 901/1000, batch 81/100 -> loss before: 0.42127007221842483, loss after: 0.4210040745546101]
[epoch 901/1000, batch 91/100 -> loss before: 0.19987566213467187, loss after: 0.19987229644637083]
ENDING EPOCH 901/1000 [loss before: 0.23070442040109485, loss after: 0.23057359189167184; epoch time: 0.09307098388671875 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.2322720235871846, loss after: 0.23246185649463072]
[epoch 1000/1000, batch 11/100 -> loss before: 0.08504375139931669, loss after: 0.08502840546074306]
[epoch 1000/1000, batch 21/100 -> loss before: 0.19627092215931277, loss after: 0.1962723234124412]
[epoch 1000/1000, batch 31/100 -> loss before: 0.24522125239781967, loss after: 0.2452221322547666]
[epoch 1000/1000, batch 41/100 -> loss before: 0.34404298642421327, loss after: 0.3434748017547341]
[epoch 1000/1000, batch 51/100 -> loss before: 0.1819602883487777, loss after: 0.18146076986832146]
[epoch 1000/1000, batch 61/100 -> loss before: 0.18292835142132385, loss after: 0.18290566131135494]
[epoch 1000/1000, batch 71/100 -> loss before: 0.36688993353869176, loss after: 0.3666242508825531]
[epoch 1000/1000, batch 81/100 -> loss before: 0.09811969873834263, loss after: 0.09805980964500852]
[epoch 1000/1000, batch 91/100 -> loss before: 0.3404828730357266, loss after: 0.34036930763152673]
ENDING EPOCH 1000/1000 [loss before: 0.23042277338587278, loss after: 0.2304387155846798; epoch time: 0.09678363800048828 s]
FIT DONE. [time: 87.10552406311035 s]
LOSS TRAIN (MSE): 0.2304387155846798
LOSS TEST (MSE): 0.2192412537789247
R^2 TRAIN: 0.18595378445213384
R^2 TEST: 0.21002786868686696
EXPERIMENT DONE
