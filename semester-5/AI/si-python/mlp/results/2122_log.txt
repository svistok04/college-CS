EXPERIMENT 2122 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[128, 128, 64, 64, 32, 32], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=sgd_momentum, learning_rate=0.001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 32481]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.26392389365226554, loss after: 0.2639129510856675]
[epoch 1/1000, batch 11/100 -> loss before: 0.33322341323422466, loss after: 0.3347558871655194]
[epoch 1/1000, batch 21/100 -> loss before: 0.324062626487395, loss after: 0.3205251470488127]
[epoch 1/1000, batch 31/100 -> loss before: 0.34307585836170434, loss after: 0.3433288731923677]
[epoch 1/1000, batch 41/100 -> loss before: 0.2959976143623535, loss after: 0.2932701355290549]
[epoch 1/1000, batch 51/100 -> loss before: 0.20672965033936147, loss after: 0.1979500017635373]
[epoch 1/1000, batch 61/100 -> loss before: 0.23929533651318305, loss after: 0.23814293152521743]
[epoch 1/1000, batch 71/100 -> loss before: 0.33070827723522, loss after: 0.32892526259970145]
[epoch 1/1000, batch 81/100 -> loss before: 0.3796653009628964, loss after: 0.38050252647762794]
[epoch 1/1000, batch 91/100 -> loss before: 0.34540031256633996, loss after: 0.3428773744222714]
ENDING EPOCH 1/1000 [loss before: 0.2838936199301506, loss after: 0.28345406405496076; epoch time: 0.06190896034240723 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.2951221221766445, loss after: 0.29417019531258815]
[epoch 101/1000, batch 11/100 -> loss before: 0.2427474941724756, loss after: 0.24264133332980045]
[epoch 101/1000, batch 21/100 -> loss before: 0.2806551098276448, loss after: 0.28087550610751527]
[epoch 101/1000, batch 31/100 -> loss before: 0.1432264198106784, loss after: 0.14227399357987283]
[epoch 101/1000, batch 41/100 -> loss before: 0.369350812970353, loss after: 0.36886301525193704]
[epoch 101/1000, batch 51/100 -> loss before: 0.16068583145139936, loss after: 0.1606417174688613]
[epoch 101/1000, batch 61/100 -> loss before: 0.16276179232824822, loss after: 0.16279596567598953]
[epoch 101/1000, batch 71/100 -> loss before: 0.36841487960590474, loss after: 0.3672019011322176]
[epoch 101/1000, batch 81/100 -> loss before: 0.31472506150283375, loss after: 0.3132323932813626]
[epoch 101/1000, batch 91/100 -> loss before: 0.3667754171441683, loss after: 0.3649618265819895]
ENDING EPOCH 101/1000 [loss before: 0.2835595299524417, loss after: 0.2914301506106911; epoch time: 0.061135053634643555 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.5681441240329683, loss after: 0.5683077033246694]
[epoch 201/1000, batch 11/100 -> loss before: 0.457672609861694, loss after: 0.457670116375459]
[epoch 201/1000, batch 21/100 -> loss before: 0.3533182990268964, loss after: 0.3545235966106832]
[epoch 201/1000, batch 31/100 -> loss before: 0.2721703304080111, loss after: 0.27259084841613845]
[epoch 201/1000, batch 41/100 -> loss before: 0.25806125426966453, loss after: 0.258242965446855]
[epoch 201/1000, batch 51/100 -> loss before: 0.3868606587963011, loss after: 0.3867482914396327]
[epoch 201/1000, batch 61/100 -> loss before: 0.31524611295010513, loss after: 0.3145764738515544]
[epoch 201/1000, batch 71/100 -> loss before: 0.17654809352949025, loss after: 0.17684785064266503]
[epoch 201/1000, batch 81/100 -> loss before: 0.2956450242797198, loss after: 0.28984996683600606]
[epoch 201/1000, batch 91/100 -> loss before: 0.4534767415242695, loss after: 0.45195197232701484]
ENDING EPOCH 201/1000 [loss before: 0.2830700314752315, loss after: 0.2915981928277081; epoch time: 0.06835412979125977 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.15886269500948075, loss after: 0.15830907022669094]
[epoch 301/1000, batch 11/100 -> loss before: 0.11222729301012305, loss after: 0.11047965014956203]
[epoch 301/1000, batch 21/100 -> loss before: 0.4131778146927866, loss after: 0.4129707772327473]
[epoch 301/1000, batch 31/100 -> loss before: 0.27885458071433894, loss after: 0.2789724997021034]
[epoch 301/1000, batch 41/100 -> loss before: 0.3281727504608286, loss after: 0.32834882745220595]
[epoch 301/1000, batch 51/100 -> loss before: 0.3225194183844132, loss after: 0.322155807690932]
[epoch 301/1000, batch 61/100 -> loss before: 0.1751765657870745, loss after: 0.17512286318438616]
[epoch 301/1000, batch 71/100 -> loss before: 0.19732767027835835, loss after: 0.19404095983568698]
[epoch 301/1000, batch 81/100 -> loss before: 0.2632981721972195, loss after: 0.26519998439982334]
[epoch 301/1000, batch 91/100 -> loss before: 0.39307628031531827, loss after: 0.38893902403481095]
ENDING EPOCH 301/1000 [loss before: 0.2846373395521503, loss after: 0.2847248275735125; epoch time: 0.06696152687072754 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.5477610713877727, loss after: 0.5454921273985527]
[epoch 401/1000, batch 11/100 -> loss before: 0.2414367529872344, loss after: 0.24127464107484795]
[epoch 401/1000, batch 21/100 -> loss before: 0.21414838925630852, loss after: 0.20842494643811338]
[epoch 401/1000, batch 31/100 -> loss before: 0.28824235455050384, loss after: 0.2877792727991806]
[epoch 401/1000, batch 41/100 -> loss before: 0.204376394776473, loss after: 0.2039079114314565]
[epoch 401/1000, batch 51/100 -> loss before: 0.3145252175963222, loss after: 0.31282407359559344]
[epoch 401/1000, batch 61/100 -> loss before: 0.28895031465839793, loss after: 0.2883552842944404]
[epoch 401/1000, batch 71/100 -> loss before: 0.2937161150244102, loss after: 0.289719246649799]
[epoch 401/1000, batch 81/100 -> loss before: 0.4782922367408804, loss after: 0.4728855353896641]
[epoch 401/1000, batch 91/100 -> loss before: 0.1547398745177472, loss after: 0.1545852447869842]
ENDING EPOCH 401/1000 [loss before: 0.283233981517659, loss after: 0.2840544295479403; epoch time: 0.05556344985961914 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.2910665717932729, loss after: 0.2898948312048647]
[epoch 501/1000, batch 11/100 -> loss before: 0.4023381723453333, loss after: 0.401213290687339]
[epoch 501/1000, batch 21/100 -> loss before: 0.6230653472346825, loss after: 0.6231347383309359]
[epoch 501/1000, batch 31/100 -> loss before: 0.11325374590856178, loss after: 0.11235695114640842]
[epoch 501/1000, batch 41/100 -> loss before: 0.3502322560748764, loss after: 0.34154567327312424]
[epoch 501/1000, batch 51/100 -> loss before: 0.21039707245395114, loss after: 0.20992233295456053]
[epoch 501/1000, batch 61/100 -> loss before: 0.5071969213801711, loss after: 0.5046956725610324]
[epoch 501/1000, batch 71/100 -> loss before: 0.26745058394212357, loss after: 0.2669482689449145]
[epoch 501/1000, batch 81/100 -> loss before: 0.5061370895001641, loss after: 0.5059502792605193]
[epoch 501/1000, batch 91/100 -> loss before: 0.26899966198878034, loss after: 0.27034984741018314]
ENDING EPOCH 501/1000 [loss before: 0.2831994959765134, loss after: 0.2873597626353759; epoch time: 0.0546269416809082 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.27002798346084267, loss after: 0.2706370265444936]
[epoch 601/1000, batch 11/100 -> loss before: 0.43344410649791654, loss after: 0.4299173686980223]
[epoch 601/1000, batch 21/100 -> loss before: 0.6316880078840112, loss after: 0.627030504770492]
[epoch 601/1000, batch 31/100 -> loss before: 0.1368410982947804, loss after: 0.13596177323437417]
[epoch 601/1000, batch 41/100 -> loss before: 0.3485322881570086, loss after: 0.3487226590977822]
[epoch 601/1000, batch 51/100 -> loss before: 0.3072826510179399, loss after: 0.3072160818817756]
[epoch 601/1000, batch 61/100 -> loss before: 0.16894046040591604, loss after: 0.16847428282909224]
[epoch 601/1000, batch 71/100 -> loss before: 0.20365125544182333, loss after: 0.20410292419349788]
[epoch 601/1000, batch 81/100 -> loss before: 0.318610806003349, loss after: 0.31732356789838645]
[epoch 601/1000, batch 91/100 -> loss before: 0.5252115277189392, loss after: 0.5252423523194383]
ENDING EPOCH 601/1000 [loss before: 0.28437650638815337, loss after: 0.2846114137033797; epoch time: 0.05701136589050293 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.2515534185523923, loss after: 0.25345596105628465]
[epoch 701/1000, batch 11/100 -> loss before: 0.3838984316224934, loss after: 0.38280426996459427]
[epoch 701/1000, batch 21/100 -> loss before: 0.3788566262242938, loss after: 0.3735184346477456]
[epoch 701/1000, batch 31/100 -> loss before: 0.25310460694935405, loss after: 0.25360128394027637]
[epoch 701/1000, batch 41/100 -> loss before: 0.13261602678315745, loss after: 0.13163331542041773]
[epoch 701/1000, batch 51/100 -> loss before: 0.2844356692671381, loss after: 0.28496989671408296]
[epoch 701/1000, batch 61/100 -> loss before: 0.31442197856146625, loss after: 0.31450726516926825]
[epoch 701/1000, batch 71/100 -> loss before: 0.1322746853420082, loss after: 0.12787478553707088]
[epoch 701/1000, batch 81/100 -> loss before: 0.32214559978111523, loss after: 0.32208422610490356]
[epoch 701/1000, batch 91/100 -> loss before: 0.2962052947286472, loss after: 0.2982139694775833]
ENDING EPOCH 701/1000 [loss before: 0.2864766819430036, loss after: 0.2841112767021149; epoch time: 0.055917978286743164 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.2544338672721345, loss after: 0.25373409391339674]
[epoch 801/1000, batch 11/100 -> loss before: 0.2995708232024218, loss after: 0.2995885659579274]
[epoch 801/1000, batch 21/100 -> loss before: 0.21616322694107248, loss after: 0.2148729429111754]
[epoch 801/1000, batch 31/100 -> loss before: 0.19476294998057445, loss after: 0.19076368760386755]
[epoch 801/1000, batch 41/100 -> loss before: 0.2744091939329103, loss after: 0.2748491571807542]
[epoch 801/1000, batch 51/100 -> loss before: 0.21612272605200994, loss after: 0.21517117790310772]
[epoch 801/1000, batch 61/100 -> loss before: 0.36700511418317455, loss after: 0.36708964564385005]
[epoch 801/1000, batch 71/100 -> loss before: 0.2854924404491627, loss after: 0.2848872272241364]
[epoch 801/1000, batch 81/100 -> loss before: 0.18124022148145766, loss after: 0.1813883244488716]
[epoch 801/1000, batch 91/100 -> loss before: 0.3294437893485589, loss after: 0.3294311917848873]
ENDING EPOCH 801/1000 [loss before: 0.2865896615225186, loss after: 0.28335920741862586; epoch time: 0.054111480712890625 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.23897026730181065, loss after: 0.2379174877285415]
[epoch 901/1000, batch 11/100 -> loss before: 0.2931881575499168, loss after: 0.29242121476081684]
[epoch 901/1000, batch 21/100 -> loss before: 0.10537098666716023, loss after: 0.10573984415656792]
[epoch 901/1000, batch 31/100 -> loss before: 0.22824070912493194, loss after: 0.22576293012778006]
[epoch 901/1000, batch 41/100 -> loss before: 0.29869867111370946, loss after: 0.2980493106841735]
[epoch 901/1000, batch 51/100 -> loss before: 0.3063070237773916, loss after: 0.3059514167197622]
[epoch 901/1000, batch 61/100 -> loss before: 0.32131901111535216, loss after: 0.3199214939956413]
[epoch 901/1000, batch 71/100 -> loss before: 0.3969369411924944, loss after: 0.3975197527020382]
[epoch 901/1000, batch 81/100 -> loss before: 0.21895262525031045, loss after: 0.2173863289565266]
[epoch 901/1000, batch 91/100 -> loss before: 0.39433678086832313, loss after: 0.3983777707307101]
ENDING EPOCH 901/1000 [loss before: 0.2918023583695453, loss after: 0.2845727392960696; epoch time: 0.05685925483703613 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.3568626354876022, loss after: 0.3568680911703635]
[epoch 1000/1000, batch 11/100 -> loss before: 0.25467076212649753, loss after: 0.25410550348727245]
[epoch 1000/1000, batch 21/100 -> loss before: 0.2605105530500252, loss after: 0.25545621379083305]
[epoch 1000/1000, batch 31/100 -> loss before: 0.3519134203166899, loss after: 0.35173857354802196]
[epoch 1000/1000, batch 41/100 -> loss before: 0.289374837097997, loss after: 0.28782227413787237]
[epoch 1000/1000, batch 51/100 -> loss before: 0.19824363788285754, loss after: 0.19824378990190455]
[epoch 1000/1000, batch 61/100 -> loss before: 0.22412302850535853, loss after: 0.22489595047725913]
[epoch 1000/1000, batch 71/100 -> loss before: 0.24004749364576333, loss after: 0.24040829067106864]
[epoch 1000/1000, batch 81/100 -> loss before: 0.34047749891667434, loss after: 0.34141293526264255]
[epoch 1000/1000, batch 91/100 -> loss before: 0.42180584411354644, loss after: 0.42211110304508714]
ENDING EPOCH 1000/1000 [loss before: 0.28536146021274916, loss after: 0.2832535178618224; epoch time: 0.05510377883911133 s]
FIT DONE. [time: 53.33101773262024 s]
LOSS TRAIN (MSE): 0.2832535178618224
LOSS TEST (MSE): 0.2775077033610641
R^2 TRAIN: -0.0006194213979819896
R^2 TEST: 8.165387974945482e-05
EXPERIMENT DONE
