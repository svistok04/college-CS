EXPERIMENT 3123 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[64, 64, 64, 64, 64, 32, 32, 32, 32, 32, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=sgd_momentum, learning_rate=0.0001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 25185]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.6815884323265685, loss after: 0.6808824144431755]
[epoch 1/1000, batch 11/100 -> loss before: 0.4385016026451578, loss after: 0.4362022430685877]
[epoch 1/1000, batch 21/100 -> loss before: 0.7207861759606903, loss after: 0.7167930684346688]
[epoch 1/1000, batch 31/100 -> loss before: 0.5533143676878497, loss after: 0.5503480940554846]
[epoch 1/1000, batch 41/100 -> loss before: 0.5215966168500162, loss after: 0.5175789403848908]
[epoch 1/1000, batch 51/100 -> loss before: 0.5600517330328268, loss after: 0.5564954696303646]
[epoch 1/1000, batch 61/100 -> loss before: 0.6112754382027699, loss after: 0.6092040031512103]
[epoch 1/1000, batch 71/100 -> loss before: 0.5205801652244343, loss after: 0.5182303997084597]
[epoch 1/1000, batch 81/100 -> loss before: 0.551041628825352, loss after: 0.5490780690365094]
[epoch 1/1000, batch 91/100 -> loss before: 0.3371168036711144, loss after: 0.33621451429390825]
ENDING EPOCH 1/1000 [loss before: 0.599648552700766, loss after: 0.36237823135420055; epoch time: 0.11000442504882812 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.28598894966564414, loss after: 0.2860190328924321]
[epoch 101/1000, batch 11/100 -> loss before: 0.22130382526197723, loss after: 0.22127839268747937]
[epoch 101/1000, batch 21/100 -> loss before: 0.24335567554429294, loss after: 0.2433250549866573]
[epoch 101/1000, batch 31/100 -> loss before: 0.38731323766453524, loss after: 0.3873179881307284]
[epoch 101/1000, batch 41/100 -> loss before: 0.6523161010084884, loss after: 0.6521187427098611]
[epoch 101/1000, batch 51/100 -> loss before: 0.268303006203896, loss after: 0.2680525128650014]
[epoch 101/1000, batch 61/100 -> loss before: 0.5155188045127213, loss after: 0.5155159407076477]
[epoch 101/1000, batch 71/100 -> loss before: 0.1462706487848351, loss after: 0.14624199333393942]
[epoch 101/1000, batch 81/100 -> loss before: 0.26682147828964, loss after: 0.2667923634886836]
[epoch 101/1000, batch 91/100 -> loss before: 0.2459142375574443, loss after: 0.24581230864164735]
ENDING EPOCH 101/1000 [loss before: 0.28309577734280356, loss after: 0.2830782184984447; epoch time: 0.11317610740661621 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.08808887612266109, loss after: 0.08809073574035628]
[epoch 201/1000, batch 11/100 -> loss before: 0.35792041610622694, loss after: 0.3578791995303865]
[epoch 201/1000, batch 21/100 -> loss before: 0.1945881928182061, loss after: 0.19436942156073864]
[epoch 201/1000, batch 31/100 -> loss before: 0.4600963006222525, loss after: 0.46010097495141683]
[epoch 201/1000, batch 41/100 -> loss before: 0.3146306617149811, loss after: 0.3147131959416171]
[epoch 201/1000, batch 51/100 -> loss before: 0.2842180740580666, loss after: 0.28421703473528104]
[epoch 201/1000, batch 61/100 -> loss before: 0.4828748278358163, loss after: 0.48292740472111156]
[epoch 201/1000, batch 71/100 -> loss before: 0.41683014295884896, loss after: 0.41673114539529754]
[epoch 201/1000, batch 81/100 -> loss before: 0.19914136323995896, loss after: 0.19912985844627645]
[epoch 201/1000, batch 91/100 -> loss before: 0.1090604305470613, loss after: 0.10906014698143154]
ENDING EPOCH 201/1000 [loss before: 0.28307863341397344, loss after: 0.2830791185499829; epoch time: 0.10442066192626953 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.1883678350139579, loss after: 0.18838329974788626]
[epoch 301/1000, batch 11/100 -> loss before: 0.20128745966117795, loss after: 0.20127910867704157]
[epoch 301/1000, batch 21/100 -> loss before: 0.3054040076841969, loss after: 0.3054153775496409]
[epoch 301/1000, batch 31/100 -> loss before: 0.3231837031541033, loss after: 0.3231887663090095]
[epoch 301/1000, batch 41/100 -> loss before: 0.26335733903426983, loss after: 0.2632437138470408]
[epoch 301/1000, batch 51/100 -> loss before: 0.28250680227288144, loss after: 0.28250385809460743]
[epoch 301/1000, batch 61/100 -> loss before: 0.3113707734334769, loss after: 0.3113151380657023]
[epoch 301/1000, batch 71/100 -> loss before: 0.3111984056189573, loss after: 0.31117163055437136]
[epoch 301/1000, batch 81/100 -> loss before: 0.3224497937029507, loss after: 0.3224547415038983]
[epoch 301/1000, batch 91/100 -> loss before: 0.3144027840868796, loss after: 0.31430621500195144]
ENDING EPOCH 301/1000 [loss before: 0.28308824131093935, loss after: 0.28307991874653315; epoch time: 0.10528445243835449 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.28357403503502004, loss after: 0.28361384373061066]
[epoch 401/1000, batch 11/100 -> loss before: 0.27631908437593944, loss after: 0.27630232354705075]
[epoch 401/1000, batch 21/100 -> loss before: 0.3440726953698556, loss after: 0.3438771680049958]
[epoch 401/1000, batch 31/100 -> loss before: 0.37373278288609557, loss after: 0.37359945685033624]
[epoch 401/1000, batch 41/100 -> loss before: 0.23559946206607768, loss after: 0.23557433014104262]
[epoch 401/1000, batch 51/100 -> loss before: 0.3304276585547926, loss after: 0.3303717578190136]
[epoch 401/1000, batch 61/100 -> loss before: 0.25903461629359553, loss after: 0.25902079578333226]
[epoch 401/1000, batch 71/100 -> loss before: 0.265410216098752, loss after: 0.26542728145427097]
[epoch 401/1000, batch 81/100 -> loss before: 0.18781537693954176, loss after: 0.18787447024756007]
[epoch 401/1000, batch 91/100 -> loss before: 0.19324702128092464, loss after: 0.19324588412287674]
ENDING EPOCH 401/1000 [loss before: 0.28308812763755964, loss after: 0.2830782143407172; epoch time: 0.11651015281677246 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.28470037701463646, loss after: 0.28465489767531604]
[epoch 501/1000, batch 11/100 -> loss before: 0.5798554212449741, loss after: 0.5797997697526552]
[epoch 501/1000, batch 21/100 -> loss before: 0.3797895080887117, loss after: 0.37979135503263617]
[epoch 501/1000, batch 31/100 -> loss before: 0.24534458345344135, loss after: 0.2453381053647039]
[epoch 501/1000, batch 41/100 -> loss before: 0.31520410267777377, loss after: 0.31520692591412275]
[epoch 501/1000, batch 51/100 -> loss before: 0.1771744720225201, loss after: 0.17717148652223907]
[epoch 501/1000, batch 61/100 -> loss before: 0.2897109442946604, loss after: 0.2897098897005367]
[epoch 501/1000, batch 71/100 -> loss before: 0.14762466383400713, loss after: 0.14761756422082078]
[epoch 501/1000, batch 81/100 -> loss before: 0.2375093600635369, loss after: 0.23748393755012898]
[epoch 501/1000, batch 91/100 -> loss before: 0.21382658348906175, loss after: 0.21383025000331415]
ENDING EPOCH 501/1000 [loss before: 0.2830782508979605, loss after: 0.28308088302007844; epoch time: 0.1132957935333252 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.34352366248353977, loss after: 0.34356129791722145]
[epoch 601/1000, batch 11/100 -> loss before: 0.34276720225573065, loss after: 0.34288784098148006]
[epoch 601/1000, batch 21/100 -> loss before: 0.29630759965113074, loss after: 0.29631167042555345]
[epoch 601/1000, batch 31/100 -> loss before: 0.13326639603946044, loss after: 0.133338210377382]
[epoch 601/1000, batch 41/100 -> loss before: 0.37643911442339384, loss after: 0.37610714020172803]
[epoch 601/1000, batch 51/100 -> loss before: 0.28270831694231335, loss after: 0.282727388092625]
[epoch 601/1000, batch 61/100 -> loss before: 0.13771727537475945, loss after: 0.1376892753166783]
[epoch 601/1000, batch 71/100 -> loss before: 0.2693946112931568, loss after: 0.26918089796023625]
[epoch 601/1000, batch 81/100 -> loss before: 0.3460120543782891, loss after: 0.34584948584867375]
[epoch 601/1000, batch 91/100 -> loss before: 0.3472953195089815, loss after: 0.3472958457622862]
ENDING EPOCH 601/1000 [loss before: 0.2831015193877196, loss after: 0.2830783352865687; epoch time: 0.13577890396118164 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.1395452441476834, loss after: 0.1395354898904964]
[epoch 701/1000, batch 11/100 -> loss before: 0.17877136563199067, loss after: 0.17833756378096716]
[epoch 701/1000, batch 21/100 -> loss before: 0.23315415835136646, loss after: 0.23305353212373414]
[epoch 701/1000, batch 31/100 -> loss before: 0.2083957098529286, loss after: 0.20836609978967893]
[epoch 701/1000, batch 41/100 -> loss before: 0.19850473714221167, loss after: 0.1984431180413676]
[epoch 701/1000, batch 51/100 -> loss before: 0.5425502596103652, loss after: 0.5424348197370378]
[epoch 701/1000, batch 61/100 -> loss before: 0.13578733711575147, loss after: 0.13579820107470036]
[epoch 701/1000, batch 71/100 -> loss before: 0.3678125739834927, loss after: 0.3678170280259979]
[epoch 701/1000, batch 81/100 -> loss before: 0.28468086124608893, loss after: 0.2846469020428768]
[epoch 701/1000, batch 91/100 -> loss before: 0.3539538096699327, loss after: 0.3539462057487158]
ENDING EPOCH 701/1000 [loss before: 0.28308110070008174, loss after: 0.2830904429534703; epoch time: 0.12331175804138184 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.2471938393173962, loss after: 0.24693584652171788]
[epoch 801/1000, batch 11/100 -> loss before: 0.11746735552976813, loss after: 0.11747052364726356]
[epoch 801/1000, batch 21/100 -> loss before: 0.24340897134752054, loss after: 0.24334114114767863]
[epoch 801/1000, batch 31/100 -> loss before: 0.2160209648601034, loss after: 0.21599407804947016]
[epoch 801/1000, batch 41/100 -> loss before: 0.4666302302597324, loss after: 0.4667175989309008]
[epoch 801/1000, batch 51/100 -> loss before: 0.2673543311997951, loss after: 0.2673262523745346]
[epoch 801/1000, batch 61/100 -> loss before: 0.27395205608571416, loss after: 0.273851978135966]
[epoch 801/1000, batch 71/100 -> loss before: 0.32346672737712157, loss after: 0.32346956745727906]
[epoch 801/1000, batch 81/100 -> loss before: 0.2919866484805641, loss after: 0.29198920152838115]
[epoch 801/1000, batch 91/100 -> loss before: 0.2457910432009199, loss after: 0.24575653803989778]
ENDING EPOCH 801/1000 [loss before: 0.2830784536301895, loss after: 0.28307904790691313; epoch time: 0.10235166549682617 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.13738330690867095, loss after: 0.1373012262338395]
[epoch 901/1000, batch 11/100 -> loss before: 0.3244319395294673, loss after: 0.32444413382413284]
[epoch 901/1000, batch 21/100 -> loss before: 0.25750276648265846, loss after: 0.2573644506106558]
[epoch 901/1000, batch 31/100 -> loss before: 0.29375667252610793, loss after: 0.2935935954582601]
[epoch 901/1000, batch 41/100 -> loss before: 0.3473921409943589, loss after: 0.34729584169909583]
[epoch 901/1000, batch 51/100 -> loss before: 0.24239820669567566, loss after: 0.24239074473747885]
[epoch 901/1000, batch 61/100 -> loss before: 0.4075632611809949, loss after: 0.40756133695401286]
[epoch 901/1000, batch 71/100 -> loss before: 0.17730053337228463, loss after: 0.17720101041285313]
[epoch 901/1000, batch 81/100 -> loss before: 0.39183239156016775, loss after: 0.39177018512351747]
[epoch 901/1000, batch 91/100 -> loss before: 0.21182152064135845, loss after: 0.21174350714228854]
ENDING EPOCH 901/1000 [loss before: 0.28307872927570304, loss after: 0.2830830945546591; epoch time: 0.10396289825439453 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.2552942630285123, loss after: 0.2553468752562899]
[epoch 1000/1000, batch 11/100 -> loss before: 0.23231068459606882, loss after: 0.23227635484329526]
[epoch 1000/1000, batch 21/100 -> loss before: 0.3113061969383603, loss after: 0.3113512467995642]
[epoch 1000/1000, batch 31/100 -> loss before: 0.23607172403276847, loss after: 0.2360879659256138]
[epoch 1000/1000, batch 41/100 -> loss before: 0.29828375315689837, loss after: 0.29816913871900125]
[epoch 1000/1000, batch 51/100 -> loss before: 0.2955697911813728, loss after: 0.2956085778743232]
[epoch 1000/1000, batch 61/100 -> loss before: 0.21148301435497308, loss after: 0.21149512296062575]
[epoch 1000/1000, batch 71/100 -> loss before: 0.3501194681028029, loss after: 0.35007863214709933]
[epoch 1000/1000, batch 81/100 -> loss before: 0.10215922588958118, loss after: 0.10215543472153188]
[epoch 1000/1000, batch 91/100 -> loss before: 0.3583943476345781, loss after: 0.3583077491175618]
ENDING EPOCH 1000/1000 [loss before: 0.28307926831845615, loss after: 0.283078192811892; epoch time: 0.13418316841125488 s]
FIT DONE. [time: 102.59755492210388 s]
LOSS TRAIN (MSE): 0.283078192811892
LOSS TEST (MSE): 0.2776861546681845
R^2 TRAIN: -6.933717733303979e-08
R^2 TEST: -0.0005613435351605744
EXPERIMENT DONE
