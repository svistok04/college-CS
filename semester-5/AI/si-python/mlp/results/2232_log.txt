EXPERIMENT 2232 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[128, 128, 64, 64, 32, 32], activation_name=relu, targets_activation_name=linear, 
initialization_name=uniform, algo_name=rmsprop, learning_rate=0.001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 32481]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.87553713426397, loss after: 27.47624143667798]
[epoch 1/1000, batch 11/100 -> loss before: 0.9493737112891731, loss after: 0.7985320723895449]
[epoch 1/1000, batch 21/100 -> loss before: 0.2553880084470546, loss after: 0.17124242896395392]
[epoch 1/1000, batch 31/100 -> loss before: 0.6015799012596033, loss after: 0.3070480560349579]
[epoch 1/1000, batch 41/100 -> loss before: 0.7920877588149053, loss after: 0.2325772531137865]
[epoch 1/1000, batch 51/100 -> loss before: 0.1682230695167899, loss after: 0.11331211127304248]
[epoch 1/1000, batch 61/100 -> loss before: 0.20443368473235424, loss after: 0.10612426833499497]
[epoch 1/1000, batch 71/100 -> loss before: 0.6145176469024971, loss after: 0.3491446071712972]
[epoch 1/1000, batch 81/100 -> loss before: 0.38512923318714715, loss after: 0.3513594169053852]
[epoch 1/1000, batch 91/100 -> loss before: 0.43678355884433084, loss after: 0.3439982686695515]
ENDING EPOCH 1/1000 [loss before: 1.102092538201909, loss after: 0.30547131149749956; epoch time: 0.09784245491027832 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.11823492029796887, loss after: 0.09203010613555457]
[epoch 101/1000, batch 11/100 -> loss before: 0.11537459657868268, loss after: 0.08395277560235309]
[epoch 101/1000, batch 21/100 -> loss before: 0.08567095199877699, loss after: 0.029053114190012923]
[epoch 101/1000, batch 31/100 -> loss before: 0.061286802471307175, loss after: 0.02149996415651493]
[epoch 101/1000, batch 41/100 -> loss before: 0.01911416623725342, loss after: 0.0033244470332905395]
[epoch 101/1000, batch 51/100 -> loss before: 0.072959195823632, loss after: 0.01600452392792872]
[epoch 101/1000, batch 61/100 -> loss before: 0.03401624415416107, loss after: 0.029328175521777224]
[epoch 101/1000, batch 71/100 -> loss before: 0.15870862382432177, loss after: 0.08766565936443454]
[epoch 101/1000, batch 81/100 -> loss before: 0.10522179299743775, loss after: 0.10585073162205745]
[epoch 101/1000, batch 91/100 -> loss before: 0.02094132755518527, loss after: 0.011539932206082567]
ENDING EPOCH 101/1000 [loss before: 0.09679886342848089, loss after: 0.11579191921619157; epoch time: 0.07502102851867676 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.05981005897029986, loss after: 0.16431174177000868]
[epoch 201/1000, batch 11/100 -> loss before: 0.10378278070970644, loss after: 0.05808303171436522]
[epoch 201/1000, batch 21/100 -> loss before: 0.0464463911160856, loss after: 0.13053763643381627]
[epoch 201/1000, batch 31/100 -> loss before: 0.020228661160460553, loss after: 0.0022240842201591135]
[epoch 201/1000, batch 41/100 -> loss before: 0.026154314974085446, loss after: 0.008912928980411332]
[epoch 201/1000, batch 51/100 -> loss before: 0.06489690507008074, loss after: 0.02782684467720365]
[epoch 201/1000, batch 61/100 -> loss before: 0.028557187131853408, loss after: 0.010367959479382327]
[epoch 201/1000, batch 71/100 -> loss before: 0.026428435156408987, loss after: 0.01254600644158269]
[epoch 201/1000, batch 81/100 -> loss before: 0.0353840233717027, loss after: 0.01832022457824491]
[epoch 201/1000, batch 91/100 -> loss before: 0.012750632800736234, loss after: 0.005342113804503388]
ENDING EPOCH 201/1000 [loss before: 0.03874549573995178, loss after: 0.06675855346340259; epoch time: 0.09516167640686035 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.015962097227298393, loss after: 0.0031384243633566754]
[epoch 301/1000, batch 11/100 -> loss before: 0.008394100942737938, loss after: 0.002008715369994025]
[epoch 301/1000, batch 21/100 -> loss before: 0.01295813003651548, loss after: 0.008378623020149262]
[epoch 301/1000, batch 31/100 -> loss before: 0.021491194424950982, loss after: 0.005733391560537411]
[epoch 301/1000, batch 41/100 -> loss before: 0.034043358411125255, loss after: 0.013217881061498881]
[epoch 301/1000, batch 51/100 -> loss before: 0.04066961449157512, loss after: 0.02499790363920823]
[epoch 301/1000, batch 61/100 -> loss before: 0.03378627280433268, loss after: 0.010853030303257362]
[epoch 301/1000, batch 71/100 -> loss before: 0.07620863300553968, loss after: 0.026180442086378725]
[epoch 301/1000, batch 81/100 -> loss before: 0.015701265271930047, loss after: 0.005019041654569713]
[epoch 301/1000, batch 91/100 -> loss before: 0.058173368146881885, loss after: 0.009045633557829696]
ENDING EPOCH 301/1000 [loss before: 0.02653676646459969, loss after: 0.0397122716861293; epoch time: 0.08270621299743652 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.03502674134452828, loss after: 0.16238536283904334]
[epoch 401/1000, batch 11/100 -> loss before: 0.042989098895805766, loss after: 0.024162332103746853]
[epoch 401/1000, batch 21/100 -> loss before: 0.013414499645129635, loss after: 0.005810316862697782]
[epoch 401/1000, batch 31/100 -> loss before: 0.025132490448744205, loss after: 0.0021121230339810663]
[epoch 401/1000, batch 41/100 -> loss before: 0.0033770046894582447, loss after: 0.004916122453140655]
[epoch 401/1000, batch 51/100 -> loss before: 0.027398162465036634, loss after: 0.011965285273315415]
[epoch 401/1000, batch 61/100 -> loss before: 0.00893020964967699, loss after: 0.0036787782315352673]
[epoch 401/1000, batch 71/100 -> loss before: 0.011783339190072491, loss after: 0.003874124587893495]
[epoch 401/1000, batch 81/100 -> loss before: 0.023975683436414492, loss after: 0.009800202379680923]
[epoch 401/1000, batch 91/100 -> loss before: 0.011547590764969242, loss after: 0.013144987781694295]
ENDING EPOCH 401/1000 [loss before: 0.021265064695379338, loss after: 0.032547312959170745; epoch time: 0.09528303146362305 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.015175461243454114, loss after: 0.008654284223725951]
[epoch 501/1000, batch 11/100 -> loss before: 0.030159493254909103, loss after: 0.0019527624963014787]
[epoch 501/1000, batch 21/100 -> loss before: 0.05335595093465086, loss after: 0.007175473675642979]
[epoch 501/1000, batch 31/100 -> loss before: 0.019117623068953697, loss after: 0.011585602433093766]
[epoch 501/1000, batch 41/100 -> loss before: 0.012359440071176517, loss after: 0.005105496642647691]
[epoch 501/1000, batch 51/100 -> loss before: 0.013160730922623196, loss after: 0.003749722947989531]
[epoch 501/1000, batch 61/100 -> loss before: 0.005730910451411062, loss after: 0.0027600429806221476]
[epoch 501/1000, batch 71/100 -> loss before: 0.01274495520424868, loss after: 0.0034276018002060843]
[epoch 501/1000, batch 81/100 -> loss before: 0.009617731893811553, loss after: 0.0019429013911539874]
[epoch 501/1000, batch 91/100 -> loss before: 0.061733164434545365, loss after: 0.018636596751405023]
ENDING EPOCH 501/1000 [loss before: 0.0261969486180578, loss after: 0.022001772848453293; epoch time: 0.09159374237060547 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.02730835622134698, loss after: 0.006744865064080953]
[epoch 601/1000, batch 11/100 -> loss before: 0.024643138220188005, loss after: 0.0027036403432420548]
[epoch 601/1000, batch 21/100 -> loss before: 0.07999946241164188, loss after: 0.042893259613297444]
[epoch 601/1000, batch 31/100 -> loss before: 0.01552422788933148, loss after: 0.01027665936386903]
[epoch 601/1000, batch 41/100 -> loss before: 0.034770128844823306, loss after: 0.007817213175776586]
[epoch 601/1000, batch 51/100 -> loss before: 0.014147452537914367, loss after: 0.005571939021495039]
[epoch 601/1000, batch 61/100 -> loss before: 0.009872452629318785, loss after: 0.0020722612581781054]
[epoch 601/1000, batch 71/100 -> loss before: 0.010855521871892516, loss after: 0.004967412069673097]
[epoch 601/1000, batch 81/100 -> loss before: 0.007374014365546175, loss after: 0.0022963293378655317]
[epoch 601/1000, batch 91/100 -> loss before: 0.013816160131688618, loss after: 0.0020543874715133023]
ENDING EPOCH 601/1000 [loss before: 0.016388034423298014, loss after: 0.02150568235048655; epoch time: 0.04255175590515137 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.009970654088101518, loss after: 0.0022551123965355276]
[epoch 701/1000, batch 11/100 -> loss before: 0.03616816305313749, loss after: 0.01409511482383893]
[epoch 701/1000, batch 21/100 -> loss before: 0.01685447900720007, loss after: 0.0037162030719982827]
[epoch 701/1000, batch 31/100 -> loss before: 0.004255577741571483, loss after: 0.0021984356771027106]
[epoch 701/1000, batch 41/100 -> loss before: 0.01072248131662083, loss after: 0.004968868643613948]
[epoch 701/1000, batch 51/100 -> loss before: 0.015109586064144292, loss after: 0.0067200805935458945]
[epoch 701/1000, batch 61/100 -> loss before: 0.04180693771934602, loss after: 0.10375934111469969]
[epoch 701/1000, batch 71/100 -> loss before: 0.009543147357594064, loss after: 0.0032746854378259277]
[epoch 701/1000, batch 81/100 -> loss before: 0.01277593731149566, loss after: 0.0040922281437452915]
[epoch 701/1000, batch 91/100 -> loss before: 0.026261289370269454, loss after: 0.005927907219289891]
ENDING EPOCH 701/1000 [loss before: 0.014694122687343207, loss after: 0.014949351367717101; epoch time: 0.07212543487548828 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.003745063917196688, loss after: 0.0015244735904606937]
[epoch 801/1000, batch 11/100 -> loss before: 0.00925370981201597, loss after: 0.003373897697170749]
[epoch 801/1000, batch 21/100 -> loss before: 0.008479304394627936, loss after: 0.004206072619416221]
[epoch 801/1000, batch 31/100 -> loss before: 0.009483568203116584, loss after: 0.0028895707384866726]
[epoch 801/1000, batch 41/100 -> loss before: 0.009136659434731131, loss after: 0.0003889836336580022]
[epoch 801/1000, batch 51/100 -> loss before: 0.007121595815334764, loss after: 0.002342873742252246]
[epoch 801/1000, batch 61/100 -> loss before: 0.013634184234915244, loss after: 0.004169931695744296]
[epoch 801/1000, batch 71/100 -> loss before: 0.07548946118577184, loss after: 0.04730083603527892]
[epoch 801/1000, batch 81/100 -> loss before: 0.025810077234677153, loss after: 0.007360241483364773]
[epoch 801/1000, batch 91/100 -> loss before: 0.01884615125207979, loss after: 0.011125460659358275]
ENDING EPOCH 801/1000 [loss before: 0.01090305988292219, loss after: 0.010607241318316455; epoch time: 0.07156634330749512 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.012325470285443455, loss after: 0.0025306131223280736]
[epoch 901/1000, batch 11/100 -> loss before: 0.02052454889076765, loss after: 0.00968447683566627]
[epoch 901/1000, batch 21/100 -> loss before: 0.007773358468398052, loss after: 0.0035517117331176603]
[epoch 901/1000, batch 31/100 -> loss before: 0.0050427764919160085, loss after: 0.0011893778715043146]
[epoch 901/1000, batch 41/100 -> loss before: 0.008795690125796465, loss after: 0.0047735731913305015]
[epoch 901/1000, batch 51/100 -> loss before: 0.1700240269349313, loss after: 0.0153598634150894]
[epoch 901/1000, batch 61/100 -> loss before: 0.0070743595798212905, loss after: 0.0020314515515635736]
[epoch 901/1000, batch 71/100 -> loss before: 0.00919529483954949, loss after: 0.005590304292850779]
[epoch 901/1000, batch 81/100 -> loss before: 0.015100854175600139, loss after: 0.0366761485047379]
[epoch 901/1000, batch 91/100 -> loss before: 0.022653240798780547, loss after: 0.007664635229938421]
ENDING EPOCH 901/1000 [loss before: 0.016116452760980005, loss after: 0.01733287553429572; epoch time: 0.0977485179901123 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.018206426028775705, loss after: 0.004655383796171328]
[epoch 1000/1000, batch 11/100 -> loss before: 0.027930970597793807, loss after: 0.009901573340057613]
[epoch 1000/1000, batch 21/100 -> loss before: 0.005338580709945121, loss after: 0.0008146782410943943]
[epoch 1000/1000, batch 31/100 -> loss before: 0.0134159491576306, loss after: 0.004382857829489767]
[epoch 1000/1000, batch 41/100 -> loss before: 0.00851289001051122, loss after: 0.0027676625028527834]
[epoch 1000/1000, batch 51/100 -> loss before: 0.013507873641486073, loss after: 0.01253463617734837]
[epoch 1000/1000, batch 61/100 -> loss before: 0.016877892388348404, loss after: 0.013290431962224375]
[epoch 1000/1000, batch 71/100 -> loss before: 0.011827140756381267, loss after: 0.0028861625466430137]
[epoch 1000/1000, batch 81/100 -> loss before: 0.024323140098117903, loss after: 0.001782016495404988]
[epoch 1000/1000, batch 91/100 -> loss before: 0.016496084080332936, loss after: 0.008114286162476718]
ENDING EPOCH 1000/1000 [loss before: 0.008777702888867419, loss after: 0.010520369566898804; epoch time: 0.1003718376159668 s]
FIT DONE. [time: 78.89358615875244 s]
LOSS TRAIN (MSE): 0.010520369566898804
LOSS TEST (MSE): 0.02548421292673111
R^2 TRAIN: 0.9628358151087165
R^2 TEST: 0.9081750461942363
EXPERIMENT DONE
