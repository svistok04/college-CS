EXPERIMENT 3143 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[64, 64, 64, 64, 64, 32, 32, 32, 32, 32, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8], activation_name=sigmoid, targets_activation_name=linear, 
initialization_name=uniform, algo_name=adam, learning_rate=0.0001, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 25185]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 0.6815884323265685, loss after: 0.6806214309525831]
[epoch 1/1000, batch 11/100 -> loss before: 0.44797418185524246, loss after: 0.447282582322653]
[epoch 1/1000, batch 21/100 -> loss before: 0.7647798514141033, loss after: 0.7637251050731168]
[epoch 1/1000, batch 31/100 -> loss before: 0.6094554226935438, loss after: 0.60861225917974]
[epoch 1/1000, batch 41/100 -> loss before: 0.6305816429463049, loss after: 0.629418321984402]
[epoch 1/1000, batch 51/100 -> loss before: 0.7023566054365605, loss after: 0.7011993963626628]
[epoch 1/1000, batch 61/100 -> loss before: 0.7315576129868859, loss after: 0.7307232802399877]
[epoch 1/1000, batch 71/100 -> loss before: 0.6763197734014659, loss after: 0.6753274231256124]
[epoch 1/1000, batch 81/100 -> loss before: 0.7148473479281604, loss after: 0.7139046913357484]
[epoch 1/1000, batch 91/100 -> loss before: 0.4516742310619984, loss after: 0.4510436101096199]
ENDING EPOCH 1/1000 [loss before: 0.599648552700766, loss after: 0.5125655325678082; epoch time: 0.1832904815673828 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.2861277894957622, loss after: 0.286150427797926]
[epoch 101/1000, batch 11/100 -> loss before: 0.2212823617683159, loss after: 0.22126344301364606]
[epoch 101/1000, batch 21/100 -> loss before: 0.24342030355354355, loss after: 0.24339640424695402]
[epoch 101/1000, batch 31/100 -> loss before: 0.387072878005954, loss after: 0.38707752432876896]
[epoch 101/1000, batch 41/100 -> loss before: 0.6516022144367704, loss after: 0.6514556321735622]
[epoch 101/1000, batch 51/100 -> loss before: 0.2682992817500084, loss after: 0.26810992211806617]
[epoch 101/1000, batch 61/100 -> loss before: 0.5154775921295327, loss after: 0.5154755101959577]
[epoch 101/1000, batch 71/100 -> loss before: 0.14626720704535615, loss after: 0.14624550432596856]
[epoch 101/1000, batch 81/100 -> loss before: 0.2668724796710052, loss after: 0.2668500860478507]
[epoch 101/1000, batch 91/100 -> loss before: 0.24596983795346644, loss after: 0.24589240047939484]
ENDING EPOCH 101/1000 [loss before: 0.2830870239637715, loss after: 0.2830782121529106; epoch time: 0.18421196937561035 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.08808962927600536, loss after: 0.08809104614509217]
[epoch 201/1000, batch 11/100 -> loss before: 0.3579527684263942, loss after: 0.357921512664151]
[epoch 201/1000, batch 21/100 -> loss before: 0.1950447744470633, loss after: 0.19487754223707376]
[epoch 201/1000, batch 31/100 -> loss before: 0.4598134168350116, loss after: 0.4598181209289246]
[epoch 201/1000, batch 41/100 -> loss before: 0.3148731926873107, loss after: 0.31493416976096966]
[epoch 201/1000, batch 51/100 -> loss before: 0.2842402198939014, loss after: 0.2842391131003867]
[epoch 201/1000, batch 61/100 -> loss before: 0.4828760189826851, loss after: 0.48291561828207136]
[epoch 201/1000, batch 71/100 -> loss before: 0.41695622059607196, loss after: 0.41688043222469673]
[epoch 201/1000, batch 81/100 -> loss before: 0.19910381814969486, loss after: 0.19909563489824816]
[epoch 201/1000, batch 91/100 -> loss before: 0.10905139992668182, loss after: 0.10905125602970538]
ENDING EPOCH 201/1000 [loss before: 0.28307853635031816, loss after: 0.28307875440865093; epoch time: 0.1694326400756836 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.18844898498000265, loss after: 0.18846017421441216]
[epoch 301/1000, batch 11/100 -> loss before: 0.20122463080500017, loss after: 0.2012190244161499]
[epoch 301/1000, batch 21/100 -> loss before: 0.30545098693645295, loss after: 0.30545861280395936]
[epoch 301/1000, batch 31/100 -> loss before: 0.3230049044168484, loss after: 0.3230093701137372]
[epoch 301/1000, batch 41/100 -> loss before: 0.2633477545772011, loss after: 0.26326362332259995]
[epoch 301/1000, batch 51/100 -> loss before: 0.2824936370986096, loss after: 0.28249156492057936]
[epoch 301/1000, batch 61/100 -> loss before: 0.3111713668833582, loss after: 0.3111308810607375]
[epoch 301/1000, batch 71/100 -> loss before: 0.3113986070864581, loss after: 0.31137777211901585]
[epoch 301/1000, batch 81/100 -> loss before: 0.3224587661069134, loss after: 0.3224624114287313]
[epoch 301/1000, batch 91/100 -> loss before: 0.31447068326324057, loss after: 0.31439770990637605]
ENDING EPOCH 301/1000 [loss before: 0.2830830889725966, loss after: 0.28307940593976516; epoch time: 0.16628313064575195 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.28361152450678784, loss after: 0.2836412268222362]
[epoch 401/1000, batch 11/100 -> loss before: 0.2763569354971106, loss after: 0.2763443802656738]
[epoch 401/1000, batch 21/100 -> loss before: 0.3445135448932346, loss after: 0.3443651190460274]
[epoch 401/1000, batch 31/100 -> loss before: 0.3740807740280783, loss after: 0.37397854860280116]
[epoch 401/1000, batch 41/100 -> loss before: 0.2356568104622097, loss after: 0.23563672748416759]
[epoch 401/1000, batch 51/100 -> loss before: 0.33086402305259915, loss after: 0.33081956308765154]
[epoch 401/1000, batch 61/100 -> loss before: 0.2606140809072733, loss after: 0.26059649943537716]
[epoch 401/1000, batch 71/100 -> loss before: 0.2655314997346992, loss after: 0.26554462274639834]
[epoch 401/1000, batch 81/100 -> loss before: 0.18793947479648845, loss after: 0.18798242738327947]
[epoch 401/1000, batch 91/100 -> loss before: 0.19326779477813907, loss after: 0.19326650360828596]
ENDING EPOCH 401/1000 [loss before: 0.2830859042999705, loss after: 0.28307817344376124; epoch time: 0.20667171478271484 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.28466565089242385, loss after: 0.28463195847170664]
[epoch 501/1000, batch 11/100 -> loss before: 0.5798538737082857, loss after: 0.579812579340292]
[epoch 501/1000, batch 21/100 -> loss before: 0.37981897550621424, loss after: 0.37982028425603004]
[epoch 501/1000, batch 31/100 -> loss before: 0.24536753891813495, loss after: 0.24536275036666916]
[epoch 501/1000, batch 41/100 -> loss before: 0.3152141827105449, loss after: 0.3152162723670858]
[epoch 501/1000, batch 51/100 -> loss before: 0.1771742036615525, loss after: 0.17717202254449224]
[epoch 501/1000, batch 61/100 -> loss before: 0.2897089098942044, loss after: 0.289708156449689]
[epoch 501/1000, batch 71/100 -> loss before: 0.1476374086335883, loss after: 0.14763188172152816]
[epoch 501/1000, batch 81/100 -> loss before: 0.23719671851473226, loss after: 0.2371789683207322]
[epoch 501/1000, batch 91/100 -> loss before: 0.21387051435686955, loss after: 0.21387309029093463]
ENDING EPOCH 501/1000 [loss before: 0.28307833905424545, loss after: 0.2830803894353501; epoch time: 0.23593854904174805 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.3436434343588294, loss after: 0.3436719148795407]
[epoch 601/1000, batch 11/100 -> loss before: 0.34271271886679316, loss after: 0.342804247085447]
[epoch 601/1000, batch 21/100 -> loss before: 0.29619546479843634, loss after: 0.2961987359929431]
[epoch 601/1000, batch 31/100 -> loss before: 0.13332547750414717, loss after: 0.13337999510809537]
[epoch 601/1000, batch 41/100 -> loss before: 0.3766211554065681, loss after: 0.37636675981156803]
[epoch 601/1000, batch 51/100 -> loss before: 0.28239941274174013, loss after: 0.2824150539391126]
[epoch 601/1000, batch 61/100 -> loss before: 0.1378008414842547, loss after: 0.13777848824892627]
[epoch 601/1000, batch 71/100 -> loss before: 0.26878756707213347, loss after: 0.2686281440881511]
[epoch 601/1000, batch 81/100 -> loss before: 0.34665906206282593, loss after: 0.34653175951732884]
[epoch 601/1000, batch 91/100 -> loss before: 0.34729766629770553, loss after: 0.34729839970517384]
ENDING EPOCH 601/1000 [loss before: 0.28309319086681967, loss after: 0.28307833001606164; epoch time: 0.19963335990905762 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.1396229090318753, loss after: 0.13961511159773465]
[epoch 701/1000, batch 11/100 -> loss before: 0.1786926451828056, loss after: 0.17836484786226237]
[epoch 701/1000, batch 21/100 -> loss before: 0.23340616129597871, loss after: 0.23332844127803415]
[epoch 701/1000, batch 31/100 -> loss before: 0.20867779681461904, loss after: 0.20865365703377142]
[epoch 701/1000, batch 41/100 -> loss before: 0.1983923356530543, loss after: 0.19834713825850478]
[epoch 701/1000, batch 51/100 -> loss before: 0.5420569542396749, loss after: 0.541973093575979]
[epoch 701/1000, batch 61/100 -> loss before: 0.13580887136819883, loss after: 0.13581683027741115]
[epoch 701/1000, batch 71/100 -> loss before: 0.3678281281249644, loss after: 0.36783146560697666]
[epoch 701/1000, batch 81/100 -> loss before: 0.28471551084716384, loss after: 0.2846892535951458]
[epoch 701/1000, batch 91/100 -> loss before: 0.3540045582432512, loss after: 0.353998341784706]
ENDING EPOCH 701/1000 [loss before: 0.2830798732498245, loss after: 0.283084448140448; epoch time: 0.17101526260375977 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.2473289402046432, loss after: 0.24713618083015546]
[epoch 801/1000, batch 11/100 -> loss before: 0.11749910506419033, loss after: 0.11750132184479231]
[epoch 801/1000, batch 21/100 -> loss before: 0.24348799460117995, loss after: 0.24343740083614454]
[epoch 801/1000, batch 31/100 -> loss before: 0.2161045187882702, loss after: 0.21608415495729813]
[epoch 801/1000, batch 41/100 -> loss before: 0.4666479052133493, loss after: 0.4667122498642513]
[epoch 801/1000, batch 51/100 -> loss before: 0.267366764195475, loss after: 0.2673460046498317]
[epoch 801/1000, batch 61/100 -> loss before: 0.274117206237919, loss after: 0.27404231373072474]
[epoch 801/1000, batch 71/100 -> loss before: 0.3235921225943087, loss after: 0.32359371758494315]
[epoch 801/1000, batch 81/100 -> loss before: 0.2921814756208553, loss after: 0.2921823205944624]
[epoch 801/1000, batch 91/100 -> loss before: 0.24587182029005547, loss after: 0.24584536932592643]
ENDING EPOCH 801/1000 [loss before: 0.2830786945657836, loss after: 0.2830783875829844; epoch time: 0.16993451118469238 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.1373349272885632, loss after: 0.1372738782825547]
[epoch 901/1000, batch 11/100 -> loss before: 0.3241326337300575, loss after: 0.32414207110332177]
[epoch 901/1000, batch 21/100 -> loss before: 0.25786245449729395, loss after: 0.25775661623848317]
[epoch 901/1000, batch 31/100 -> loss before: 0.2943448758049395, loss after: 0.29421861413543826]
[epoch 901/1000, batch 41/100 -> loss before: 0.3482171568655953, loss after: 0.348140060327863]
[epoch 901/1000, batch 51/100 -> loss before: 0.24355295097880364, loss after: 0.2435408326330172]
[epoch 901/1000, batch 61/100 -> loss before: 0.40755941999421913, loss after: 0.4075585600028071]
[epoch 901/1000, batch 71/100 -> loss before: 0.1774734322459055, loss after: 0.1773962197995304]
[epoch 901/1000, batch 81/100 -> loss before: 0.3921913838472298, loss after: 0.39214226353312975]
[epoch 901/1000, batch 91/100 -> loss before: 0.2116448727415643, loss after: 0.2115875297871194]
ENDING EPOCH 901/1000 [loss before: 0.2830785086855606, loss after: 0.2830828683501645; epoch time: 0.17153716087341309 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.25530353374300163, loss after: 0.2553421254415092]
[epoch 1000/1000, batch 11/100 -> loss before: 0.2323587617068728, loss after: 0.23233316584373612]
[epoch 1000/1000, batch 21/100 -> loss before: 0.3110716789549561, loss after: 0.31110559325770515]
[epoch 1000/1000, batch 31/100 -> loss before: 0.23614206197791376, loss after: 0.23615405715978147]
[epoch 1000/1000, batch 41/100 -> loss before: 0.2986804814687901, loss after: 0.29859285651532674]
[epoch 1000/1000, batch 51/100 -> loss before: 0.29513927062997725, loss after: 0.2951697979901888]
[epoch 1000/1000, batch 61/100 -> loss before: 0.2118988838894435, loss after: 0.2119060480203859]
[epoch 1000/1000, batch 71/100 -> loss before: 0.3503154759389035, loss after: 0.350283380927044]
[epoch 1000/1000, batch 81/100 -> loss before: 0.10214269942678918, loss after: 0.10214039444636436]
[epoch 1000/1000, batch 91/100 -> loss before: 0.35835811127305733, loss after: 0.35829471512547423]
ENDING EPOCH 1000/1000 [loss before: 0.28307920546248067, loss after: 0.28307817381014316; epoch time: 0.16981720924377441 s]
FIT DONE. [time: 167.5251500606537 s]
LOSS TRAIN (MSE): 0.28307817381014316
LOSS TEST (MSE): 0.27769030389883365
R^2 TRAIN: -2.2117305764624007e-09
R^2 TEST: -0.0005762940817513051
EXPERIMENT DONE
