EXPERIMENT 1241 START
DATA SETTINGS: domain=4.71238898038469, noise_std=0.1, m_train=1000, m_test=10000
APPROXIMATOR: MLPApproximator(structure=[128, 64, 32], activation_name=relu, targets_activation_name=linear, 
initialization_name=uniform, algo_name=adam, learning_rate=0.01, n_epochs=1000, batch_size=10)
FIT [total of weights (params): 10753]
---
EPOCH 1/1000:
[epoch 1/1000, batch 1/100 -> loss before: 20.679362556386955, loss after: 0.31793761097595297]
[epoch 1/1000, batch 11/100 -> loss before: 0.5110494493341249, loss after: 0.44174910962295605]
[epoch 1/1000, batch 21/100 -> loss before: 0.22473180393899456, loss after: 0.12059817558831969]
[epoch 1/1000, batch 31/100 -> loss before: 0.5304985917689666, loss after: 0.5768351579021662]
[epoch 1/1000, batch 41/100 -> loss before: 0.20057652151475072, loss after: 0.2031109226004819]
[epoch 1/1000, batch 51/100 -> loss before: 0.29621687688779336, loss after: 0.2085379200289418]
[epoch 1/1000, batch 61/100 -> loss before: 0.5485890846765704, loss after: 0.5839914710139742]
[epoch 1/1000, batch 71/100 -> loss before: 0.4044265044932861, loss after: 0.3832270684529565]
[epoch 1/1000, batch 81/100 -> loss before: 0.13203025808198512, loss after: 0.1290296681611094]
[epoch 1/1000, batch 91/100 -> loss before: 0.39291441117216735, loss after: 0.39466559796246076]
ENDING EPOCH 1/1000 [loss before: 18.773492598911375, loss after: 0.2765734169397304; epoch time: 0.044637203216552734 s]
---
EPOCH 101/1000:
[epoch 101/1000, batch 1/100 -> loss before: 0.1930901229079434, loss after: 0.1900192268295788]
[epoch 101/1000, batch 11/100 -> loss before: 0.04856321491159289, loss after: 0.04921867080847299]
[epoch 101/1000, batch 21/100 -> loss before: 0.17856570438302807, loss after: 0.15233144982384292]
[epoch 101/1000, batch 31/100 -> loss before: 0.07442820511269306, loss after: 0.07247735399823775]
[epoch 101/1000, batch 41/100 -> loss before: 0.39807274618180927, loss after: 0.3949198631433496]
[epoch 101/1000, batch 51/100 -> loss before: 0.045608087706867875, loss after: 0.04584884927505859]
[epoch 101/1000, batch 61/100 -> loss before: 0.11419783120863432, loss after: 0.11439613215059911]
[epoch 101/1000, batch 71/100 -> loss before: 0.2099638048514086, loss after: 0.20703730295479517]
[epoch 101/1000, batch 81/100 -> loss before: 0.2991461377100867, loss after: 0.2921976157963405]
[epoch 101/1000, batch 91/100 -> loss before: 0.04019982545019409, loss after: 0.04126273174306012]
ENDING EPOCH 101/1000 [loss before: 0.16743646870845397, loss after: 0.17180011727605238; epoch time: 0.04210472106933594 s]
---
EPOCH 201/1000:
[epoch 201/1000, batch 1/100 -> loss before: 0.06915806269954469, loss after: 0.056063646580541845]
[epoch 201/1000, batch 11/100 -> loss before: 0.13723666722874067, loss after: 0.12801905340629388]
[epoch 201/1000, batch 21/100 -> loss before: 0.21752642499262292, loss after: 0.21221175541178008]
[epoch 201/1000, batch 31/100 -> loss before: 0.05406106985555239, loss after: 0.04739617866457918]
[epoch 201/1000, batch 41/100 -> loss before: 0.23010783739663127, loss after: 0.22542841964372823]
[epoch 201/1000, batch 51/100 -> loss before: 0.13377181548938938, loss after: 0.12809914686613702]
[epoch 201/1000, batch 61/100 -> loss before: 0.05768699960075067, loss after: 0.052545184563010626]
[epoch 201/1000, batch 71/100 -> loss before: 0.16212398246366413, loss after: 0.1440834128437871]
[epoch 201/1000, batch 81/100 -> loss before: 0.10423974921079268, loss after: 0.09770300571146423]
[epoch 201/1000, batch 91/100 -> loss before: 0.1020876241505149, loss after: 0.08426924846065846]
ENDING EPOCH 201/1000 [loss before: 0.10309094874274433, loss after: 0.11372495778210939; epoch time: 0.03992104530334473 s]
---
EPOCH 301/1000:
[epoch 301/1000, batch 1/100 -> loss before: 0.0065087054321428415, loss after: 0.004226397537105049]
[epoch 301/1000, batch 11/100 -> loss before: 0.13404844211984326, loss after: 0.12537244518542573]
[epoch 301/1000, batch 21/100 -> loss before: 0.08545677864467657, loss after: 0.07964465762294866]
[epoch 301/1000, batch 31/100 -> loss before: 0.08751116411331339, loss after: 0.08815043219432298]
[epoch 301/1000, batch 41/100 -> loss before: 0.05014749632844724, loss after: 0.04900354639886968]
[epoch 301/1000, batch 51/100 -> loss before: 0.09459642903110936, loss after: 0.09162880514427449]
[epoch 301/1000, batch 61/100 -> loss before: 0.16957425285182115, loss after: 0.17026649344276198]
[epoch 301/1000, batch 71/100 -> loss before: 0.23190774767763195, loss after: 0.21006001444731054]
[epoch 301/1000, batch 81/100 -> loss before: 0.0917106795485785, loss after: 0.08725400443201464]
[epoch 301/1000, batch 91/100 -> loss before: 0.07050585324186205, loss after: 0.06781331073372346]
ENDING EPOCH 301/1000 [loss before: 0.09161089797133974, loss after: 0.10755814680736231; epoch time: 0.04117107391357422 s]
---
EPOCH 401/1000:
[epoch 401/1000, batch 1/100 -> loss before: 0.07755736207977264, loss after: 0.07593020347941162]
[epoch 401/1000, batch 11/100 -> loss before: 0.18801105110941146, loss after: 0.1735494426198944]
[epoch 401/1000, batch 21/100 -> loss before: 0.05770619024055975, loss after: 0.057569574962208755]
[epoch 401/1000, batch 31/100 -> loss before: 0.022395789235866718, loss after: 0.020472600731025564]
[epoch 401/1000, batch 41/100 -> loss before: 0.08246431782941671, loss after: 0.08355167988751429]
[epoch 401/1000, batch 51/100 -> loss before: 0.17317883040497772, loss after: 0.16879131348897708]
[epoch 401/1000, batch 61/100 -> loss before: 0.033041081769109706, loss after: 0.031239498227474898]
[epoch 401/1000, batch 71/100 -> loss before: 0.0833870883296717, loss after: 0.07897308956525398]
[epoch 401/1000, batch 81/100 -> loss before: 0.08014378247210174, loss after: 0.0805515023729583]
[epoch 401/1000, batch 91/100 -> loss before: 0.07580989403567082, loss after: 0.07671049887754454]
ENDING EPOCH 401/1000 [loss before: 0.08455753855949641, loss after: 0.0766880257935577; epoch time: 0.04062676429748535 s]
---
EPOCH 501/1000:
[epoch 501/1000, batch 1/100 -> loss before: 0.0794944308183139, loss after: 0.06885918450071203]
[epoch 501/1000, batch 11/100 -> loss before: 0.09551818074184906, loss after: 0.09476300898533564]
[epoch 501/1000, batch 21/100 -> loss before: 0.08272404721083933, loss after: 0.08154582010620551]
[epoch 501/1000, batch 31/100 -> loss before: 0.0398076875396863, loss after: 0.0387523688755833]
[epoch 501/1000, batch 41/100 -> loss before: 0.07067511132940292, loss after: 0.06419003747034943]
[epoch 501/1000, batch 51/100 -> loss before: 0.03171695209837108, loss after: 0.030684396961147616]
[epoch 501/1000, batch 61/100 -> loss before: 0.025593355179900475, loss after: 0.023249033624531144]
[epoch 501/1000, batch 71/100 -> loss before: 0.19260787731644666, loss after: 0.18595240307079786]
[epoch 501/1000, batch 81/100 -> loss before: 0.13344919718598888, loss after: 0.11937239624900906]
[epoch 501/1000, batch 91/100 -> loss before: 0.27537627658545255, loss after: 0.2671241884347681]
ENDING EPOCH 501/1000 [loss before: 0.09563897486511283, loss after: 0.09762449029135463; epoch time: 0.03926515579223633 s]
---
EPOCH 601/1000:
[epoch 601/1000, batch 1/100 -> loss before: 0.04996949902315245, loss after: 0.04773560249533753]
[epoch 601/1000, batch 11/100 -> loss before: 0.08292344128832836, loss after: 0.0807095155776716]
[epoch 601/1000, batch 21/100 -> loss before: 0.06345910966630952, loss after: 0.058977148891370235]
[epoch 601/1000, batch 31/100 -> loss before: 0.08986274990315654, loss after: 0.08346487112417636]
[epoch 601/1000, batch 41/100 -> loss before: 0.06055618256634284, loss after: 0.06035414212475428]
[epoch 601/1000, batch 51/100 -> loss before: 0.0609405964419263, loss after: 0.05560219016438195]
[epoch 601/1000, batch 61/100 -> loss before: 0.20959347649584514, loss after: 0.184033805699381]
[epoch 601/1000, batch 71/100 -> loss before: 0.07401023951604675, loss after: 0.04771072298441237]
[epoch 601/1000, batch 81/100 -> loss before: 0.06264664492195246, loss after: 0.05536261839063831]
[epoch 601/1000, batch 91/100 -> loss before: 0.12101322006295864, loss after: 0.11894178740644135]
ENDING EPOCH 601/1000 [loss before: 0.08639609968981918, loss after: 0.101984567513152; epoch time: 0.03888440132141113 s]
---
EPOCH 701/1000:
[epoch 701/1000, batch 1/100 -> loss before: 0.01802810993827069, loss after: 0.01803406850974784]
[epoch 701/1000, batch 11/100 -> loss before: 0.02883410166477914, loss after: 0.023211612395754398]
[epoch 701/1000, batch 21/100 -> loss before: 0.022369252190805433, loss after: 0.02187856880914089]
[epoch 701/1000, batch 31/100 -> loss before: 0.013694166343385026, loss after: 0.012425639849604203]
[epoch 701/1000, batch 41/100 -> loss before: 0.09258229139132564, loss after: 0.07700010748095415]
[epoch 701/1000, batch 51/100 -> loss before: 0.12916777664524307, loss after: 0.13159848016986897]
[epoch 701/1000, batch 61/100 -> loss before: 0.05686420219655343, loss after: 0.04990365388805083]
[epoch 701/1000, batch 71/100 -> loss before: 0.05737706959004392, loss after: 0.04545445677096301]
[epoch 701/1000, batch 81/100 -> loss before: 0.10656187874231411, loss after: 0.10382216608551864]
[epoch 701/1000, batch 91/100 -> loss before: 0.28799596183522824, loss after: 0.28606706647691726]
ENDING EPOCH 701/1000 [loss before: 0.08757523404565722, loss after: 0.09572032987081115; epoch time: 0.04217648506164551 s]
---
EPOCH 801/1000:
[epoch 801/1000, batch 1/100 -> loss before: 0.24267862909550325, loss after: 0.2429066397642592]
[epoch 801/1000, batch 11/100 -> loss before: 0.2741563146759399, loss after: 0.25869666248007744]
[epoch 801/1000, batch 21/100 -> loss before: 0.11400247482183179, loss after: 0.10435106859245358]
[epoch 801/1000, batch 31/100 -> loss before: 0.057704446235630244, loss after: 0.048957617307787374]
[epoch 801/1000, batch 41/100 -> loss before: 0.05775694042435211, loss after: 0.05538481156569863]
[epoch 801/1000, batch 51/100 -> loss before: 0.136310289934332, loss after: 0.12863165071644833]
[epoch 801/1000, batch 61/100 -> loss before: 0.08003822892866577, loss after: 0.07335714172872847]
[epoch 801/1000, batch 71/100 -> loss before: 0.02545838413788623, loss after: 0.018140360380452584]
[epoch 801/1000, batch 81/100 -> loss before: 0.05745340897317407, loss after: 0.049673364878460774]
[epoch 801/1000, batch 91/100 -> loss before: 0.015108509669971315, loss after: 0.013063282937488124]
ENDING EPOCH 801/1000 [loss before: 0.08730318095835454, loss after: 0.0932707894007589; epoch time: 0.04329395294189453 s]
---
EPOCH 901/1000:
[epoch 901/1000, batch 1/100 -> loss before: 0.011177707430461755, loss after: 0.01080644286201607]
[epoch 901/1000, batch 11/100 -> loss before: 0.08376558377662706, loss after: 0.08038708377261752]
[epoch 901/1000, batch 21/100 -> loss before: 0.12152235572734013, loss after: 0.11151822058209801]
[epoch 901/1000, batch 31/100 -> loss before: 0.11916013443243542, loss after: 0.1164107457844223]
[epoch 901/1000, batch 41/100 -> loss before: 0.02584246009351579, loss after: 0.023139889215574458]
[epoch 901/1000, batch 51/100 -> loss before: 0.1438445063782971, loss after: 0.14196325994008097]
[epoch 901/1000, batch 61/100 -> loss before: 0.14998101211471576, loss after: 0.1372859483063944]
[epoch 901/1000, batch 71/100 -> loss before: 0.019655183362175907, loss after: 0.02360081449185675]
[epoch 901/1000, batch 81/100 -> loss before: 0.15672558143614906, loss after: 0.14113342394530334]
[epoch 901/1000, batch 91/100 -> loss before: 0.02954752902919717, loss after: 0.02632055489012074]
ENDING EPOCH 901/1000 [loss before: 0.09018006612789901, loss after: 0.0927061390286398; epoch time: 0.04008936882019043 s]
---
EPOCH 1000/1000:
[epoch 1000/1000, batch 1/100 -> loss before: 0.06958289475925701, loss after: 0.06664252690114544]
[epoch 1000/1000, batch 11/100 -> loss before: 0.02302196220500462, loss after: 0.020801538923382823]
[epoch 1000/1000, batch 21/100 -> loss before: 0.12417750866108779, loss after: 0.11743676917283455]
[epoch 1000/1000, batch 31/100 -> loss before: 0.07180483049532492, loss after: 0.07150838752602058]
[epoch 1000/1000, batch 41/100 -> loss before: 0.11583904492483539, loss after: 0.1126566850709835]
[epoch 1000/1000, batch 51/100 -> loss before: 0.11500562996404037, loss after: 0.11338560899551031]
[epoch 1000/1000, batch 61/100 -> loss before: 0.08358628336376114, loss after: 0.08380042833473415]
[epoch 1000/1000, batch 71/100 -> loss before: 0.02466611062979792, loss after: 0.022161715567847497]
[epoch 1000/1000, batch 81/100 -> loss before: 0.13604872450561584, loss after: 0.13798673046999432]
[epoch 1000/1000, batch 91/100 -> loss before: 0.22953073140326202, loss after: 0.21948039539604225]
ENDING EPOCH 1000/1000 [loss before: 0.08015228636064109, loss after: 0.10445619231584613; epoch time: 0.04032778739929199 s]
FIT DONE. [time: 40.09603142738342 s]
LOSS TRAIN (MSE): 0.10445619231584613
LOSS TEST (MSE): 0.10435651317569636
R^2 TRAIN: 0.6309987762711353
R^2 TEST: 0.6239816379952823
EXPERIMENT DONE
